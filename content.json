{"posts":[{"title":"BERT 实战","text":"BERT 的任务有两个： Masked LM (MLM): 将句子转化为 tokens，并以一定概率随机选取部分 token 替换为 [MASK] 掩码。这里的 [MASK] 是广义的掩码，包括替换为 “[MASK]” token、替换为随机 token 和保持原 token。 Next Sentence Prediction (NSP): 将两个句子拼接起来，判断这两个句子是不是属于前后关系。 Google 官方提供了数据预处理的代码：create_pretraining_data.py，其中输入文件包括原始文本和词汇表，输出文件为处理后的 tfrecord 格式文件（Tensorflow 可读取文件），用于后续对 BERT 的训练或微调。 1.1 参数一些输入参数如下： 输入参数 含义 默认值 input_file 输入的原始文本，可以是多个文件，用 “,” 隔开 None vocab_file 词汇表，可以使用官方提供的用于微调，或者自己生成 None do_lower_case 将输入文本转为小写（针对英文） True do_whole_word_mask 2019 新增的 wwm 模式，将整个词掩码 Fasle max_seq_length 序列最大长度 128 max_predictions_per_seq 每个序列最多掩码的数量 20 random_seed 随机种子，便于复现实验 12345 dupe_factor 对输入文本重复进行不同掩码操作的次数 10 short_seq_prob 创建小于最大长度的序列的概率 0.1 masked_lm_prob 掩码概率 0.15 1.2 流程首先，对于每一个输入文本，进入到 create_training_instances: 官方提示的输入文本要求： 每行一句话。理想情况下，这些应该是实际的句子，而不是整个段落或任意跨度的文本。（因为我们将句子边界用于“下一句预测”任务）。 不同的文档段落之间用空行隔开。 create_training_instances 一、逐行读取文本，若该行不为空，则对该行使用 tokenization.FullTokenizer().tokenize() 转为 tokens。 处理流程： 调用 BasicTokenizer().tokenize() 处理文本行 ​ 这里先将文本行转为 Unicode 来统一处理。 ​ 1.1 清理文本行：_clean_text() ​ 去除 control 字符和不合法字符，并将 \\t \\n \\r \\s 等字符转为一个空格字符，返回清理后的字符串。 ​ 1.2 对于汉字，每个汉字前后添加空格字符：_tokenize_chinese_chars() ​ 1.3 使用 text.split() 将不相连的字符分开：whitespace_tokenize() ​ 至此，返回处理后的 token 列表。英文是按照单词分开，一个 token 是一个单词；中文实际上是按照字分开了，一个 token 是一个字。 ​ 1.4 若 do_lower_case 为 True，则将字符串转为小写，并去除音调进行规范化。 2. 调用 `WordpieceTokenizer().tokenize()` 处理后的每一个 token ​ 使用最长匹配优先的贪心算法按照给定的词汇表将文本转为 word pieces。例如，将 “unaffable” 转为 [“un”, “##aff”, “## able”]。输入为一个 token 或者是空格分开的 token 序列，输出为 word pieces 列表。该方法对中文是无效的，因为前面已经把中文文本按字分开了。 ​ 至此，完成了 tokenize，返回分隔后的 token 列表。 二、将每行得到的 token 列表添加到文档列表 all_documents = [[]] 的最后一个文档中，一个文档包含多行的 token 列表。去除空列表（多余的空行导致）之后随机打乱文档顺序（shuffle）。 三、从所有文档（all_documents）中的每篇文档，创建实例：create_instances_from_document(): ​ 官方提示：由于我们总是要将序列扩充（padding）到 max_seq_length ，因此我们通常想用句子填满整个序列，所以短句子会造成计算的浪费。然而，有时候（short_seq_prob == 0.1 代表 10% 的概率）也需要短句子来降低预训练和微调之间的不匹配问题。 Next Sentence Prediction (NSP) 任务 ​ 1.1 确定目标序列长度（target_seq_length） ​ target_seq_length = max_seq_length - 3 [CLS] [SEP] [SEP] 占了三个 ​ 并且以 10% 的概率，target_seq_length = rng.randint(2, max_num_tokens) ​ 1.2 选取句子 a 和句子 b ​ 以 50% 的概率选择一个文档中相邻的两个序列，另外 50% 的概率从不同的文档选择不相邻的两个序列。 ​ 然后以列表的形式表示为 [CLS] 句子 a [SEP] 句子 b [SEP]，同时句子 b 之前的所有 token 的 id 记为 0，之后的所有 token 的id 记为 1。 Masked LM (MLM) 任务：create_masked_lm_predictions() ​ 输入：上述得到的 token 序列；输出：Mask 后的 token 序列。 ​ 2.1 对 token 序列分组，如果 do_whole_word_mask 为 True，则将同一个 word pieces 的 token 分到一组。例如，[“un”, “##aff”, “## able”] 分到一组；否则每个 token 为 1 组。并打乱顺序。 ​ 2.2 选择 min(max_predictions_per_seq, max(1, int(round(len(tokens) * masked_lm_prob)))) 个 token 进行 Mask，其中，被选中的组中的所有 token 都要 Mask（wwm）。 ​ 2.3 Mask 操作 ​ 被选中的 token，80 % 的概率替换为 “[MASK]” token，10% 的概率替换为词汇表中的随机词，10% 的概率不变。 ​ 然后，由 Mask 之后的 token 序列的位置和标签得到 TrainingInstance 类型的对象。 四、重复 dupe_factor 次步骤三，得到所有创建的实例列表。 到这里，预处理基本完成了，剩下的就是将得到的数据写入文件。 write_instance_to_example_files 将上述得到的 instance 列表写入文件，创建 TF example files，用于后续训练或微调。","link":"2022/03/01/BERT%E5%AE%9E%E6%88%98/"},{"title":"Centos7 Mysql 使用","text":"1. 清理环境 查看是否已经安装MySQL数据库 1rpm -qa | grep mysql 若存在，依次卸载旧数据库文件 123sudo systemctl status mysqld # 查看mysql运行状态sudo systemctl stop mysqld # 停止mysqlsudo yum remove mysql-xxx-xxx # 卸载已安装的程序 删除MySQL配置文件 123find / -name mysql# /var/lib/mysqlrm -rf [查找到的配置文件] 卸载MariaDB 12rpm -qa | grep mariadb # 检查mariadbsudo yum remove [已安装的mariadb程序] 2. 安装 MySQL 根据系统版本下载yum源 12cat /etc/redhat-release # 查看系统版本wget http：//dev.mysql.com/get/Downloads/mysql80-community-release-el7-1.noarch.rpm # 选择对应版本的rpm 安装刚才下载的MySQL源 1sudo rpm -Uvh mysql80-community-release-el7-3.noarch.rpm 选择要安装的MySQL版本 1234yum repolist all | grep mysql # 查看所有版本，默认下载最新版本# 切换版本，例如切换到5.7版本，若下载最新版本，忽略这一步sudo yum-config-manager --disable mysql80-communitysudo yum-config-manager --enable mysql57-community 安装MySQL 1sudo yum install mysql-community-server 启动MySQL 1sudo systemctl start mysqld 查看、停止MySQL 123sudo systemctl status mysqld # 查看运行状态sudo systemctl stop mysqld # 停止运行sudo systemctl restart mysqld # 重启服务 设置开机启动 12systemctl enable mysqldsystemctl daemon-reload 3. 配置MySQL 打开数据库端口 12sudo firewall-cmd --zone=public --add-port=3306/tcp --permanent # 永久打开3306端口sudo firewall-cmd --reload 查看默认密码 1sudo grep 'temporary password' /var/log/mysqld.log 修改密码 123456789101112# 首先进入MySQLmysql -u root -p # 输入初始密码use mysql;# 修改密码# mysql 7.0mysql&gt; UPDATE user SET password=password(&quot;密码&quot;) WHERE user='root';# mysql 8.0mysql&gt; alter user 'root'@'localhost' identified with mysql_native_password by '密码';mysql&gt; flush privileges; # 刷新生效 开启本地、远程服务 12345678910111213# 开启本地访问mysql&gt; grant all privileges on *.* to root@&quot;localhost&quot; identified by &quot;密码&quot;;# 开启远程访问# mysql 7.0mysql&gt; grant all privileges on *.* to root@&quot;%&quot; identified by &quot;密码&quot;;# mysql 8.0mysql&gt; UPDATE user SET host = '%' WHERE user ='root;mysql&gt; flush privileges; # 刷新MySQL的系统权限相关表# 退出mysql&gt; exit; 忘记密码，重新设置密码 密码要求同时包含大小写字母数字标点，所以先确定你输入的密码是否符合要求。 1234567891011121314151617181920212223242526# 首先找到配置文件 my.cnf，一般是在 /etc/my.cnfvim /etc/my.cnf# 在配置文件中添加 skip-grant-tables，设置为免密码登录，保存退出# 重启服务以生效sudo systemctl restart mysqld# 先将密码设置为空mysql -u root -p # 此时没有密码，直接敲回车进入mysql &gt; use mysql;mysql &gt; update user set authentication_string = '' where user = 'root';mysql &gt; quit # 退出# 然后去除免密码登录# 注释掉配置文件中的 skip-grant-tables 一行# 重启服务 sudo systemctl restart mysqld# 重新登录MySQL，此时密码为空mysql -u root -p # 直接敲回车登录# 修改密码mysql &gt; ALTER USER 'root'@'localhost' IDENTIFIED BY '你的新密码'; # 注意密码需要同时包含大小写字母数字和标点flush privileges; # 刷新mysql &gt; quit # 退出# 重启服务 sudo systemctl restart mysqld 4. 数据库操作数据库操作首先要登录数据库（废话） 1mysql -u root -p 用户操作 1234567891011# 创建用户mysql &gt; CREATE USER username IDENTIFIED BY 'password';# 查看所有用户mysql&gt; select host, user from mysql.user;# 查看当前用户mysql&gt; select current_user;# 查看当前用户权限mysql&gt; show grants; 数据库操作 1234567891011121314151617181920# 显示数据库mysql &gt; show databases;# 选择数据库mysql &gt; use mysql;# 创建数据库mysql &gt; CREATE DATABASE `databasename` DEFAULT CHARACTER SET utf8 COLLATE utf8_general_ci;# 显示数据库的所有表mysql&gt; show tables;# 创建表mysql&gt; create table student(id int auto_increment primary key,name varchar(16) not null, age int, sex char(1));# 查看表结构mysql&gt; describe student;# 查看索引mysql&gt; show index from students; 增删改查 ​ ……","link":"2021/12/18/Centos7-Mysql-%E4%BD%BF%E7%94%A8/"},{"title":"GLUE","text":"一、简介自然语言处理（NLP）主要自然语言理解（NLU）和自然语言生成（NLG）。为了让NLU任务发挥最大的作用，来自纽约大学、华盛顿大学等机构创建了一个多任务的自然语言理解基准和分析平台，也就是GLUE（General Language Understanding Evaluation）。 GLUE的论文为：GLUE: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding[1] GLUE的官网为：https://gluebenchmark.com/ 二、任务介绍GLUE共有九个任务，分别是CoLA、SST-2、MRPC、STS-B、QQP、MNLI、QNLI、RTE、WNLI。如下图图2所示，可以分为三类，分别是单句任务，相似性和释义任务， 2.1 CoLA CoLA(The Corpus of Linguistic Acceptability，语言可接受性语料库)，单句子分类任务，语料来自语言理论的书籍和期刊，每个句子被标注为是否合乎语法的单词序列。本任务是一个二分类任务，标签共两个，分别是0和1，其中0表示不合乎语法，1表示合乎语法。 样本个数：训练集8, 551个，开发集1, 043个，测试集1, 063个。 任务：可接受程度，合乎语法与不合乎语法二分类。 评价准则：Matthews correlation coefficient。 2.2 SST-2 SST-2(The Stanford Sentiment Treebank，斯坦福情感树库)，单句子分类任务，包含电影评论中的句子和它们情感的人类注释。这项任务是给定句子的情感，类别分为两类正面情感（positive，样本标签对应为1）和负面情感（negative，样本标签对应为0），并且只用句子级别的标签。也就是，本任务也是一个二分类任务，针对句子级别，分为正面和负面情感。 样本个数：训练集67, 350个，开发集873个，测试集1, 821个。 任务：情感分类，正面情感和负面情感二分类。 评价准则：accuracy。 2.3 MRPC MRPC(The Microsoft Research Paraphrase Corpus，微软研究院释义语料库)，相似性和释义任务，是从在线新闻源中自动抽取句子对语料库，并人工注释句子对中的句子是否在语义上等效。类别并不平衡，其中68%的正样本，所以遵循常规的做法，报告准确率（accuracy）和F1值。 样本个数：训练集3, 668个，开发集408个，测试集1, 725个。 任务：是否释义二分类，是释义，不是释义两类。 评价准则：准确率（accuracy）和F1值。 2.4 STSB STSB(The Semantic Textual Similarity Benchmark，语义文本相似性基准测试)，相似性和释义任务，是从新闻标题、视频标题、图像标题以及自然语言推断数据中提取的句子对的集合，每对都是由人类注释的，其相似性评分为0-5(大于等于0且小于等于5的浮点数，原始paper里写的是1-5，可能是作者失误）。任务就是预测这些相似性得分，本质上是一个回归问题，但是依然可以用分类的方法，可以归类为句子对的文本五分类任务。 样本个数：训练集5, 749个，开发集1, 379个，测试集1, 377个。 任务：回归任务，预测为1-5之间的相似性得分的浮点数。但是依然可以使用分类的方法，作为五分类。 评价准则：Pearson and Spearman correlation coefficients。 2.5 QQP QQP(The Quora Question Pairs, Quora问题对数集)，相似性和释义任务，是社区问答网站Quora中问题对的集合。任务是确定一对问题在语义上是否等效。与MRPC一样，QQP也是正负样本不均衡的，不同是的QQP负样本占63%，正样本是37%，所以我们也是报告准确率和F1值。我们使用标准测试集，为此我们从作者那里获得了专用标签。我们观察到测试集与训练集分布不同。 样本个数：训练集363, 870个，开发集40, 431个，测试集390, 965个。 任务：判定句子对是否等效，等效、不等效两种情况，二分类任务。 评价准则：准确率（accuracy）和F1值。 2.6 MNLI MNLI(The Multi-Genre Natural Language Inference Corpus, 多类型自然语言推理数据库)，自然语言推断任务，是通过众包方式对句子对进行文本蕴含标注的集合。给定前提（premise）语句和假设（hypothesis）语句，任务是预测前提语句是否包含假设（蕴含, entailment），与假设矛盾（矛盾，contradiction）或者两者都不（中立，neutral）。前提语句是从数十种不同来源收集的，包括转录的语音，小说和政府报告。 样本个数：训练集392, 702个，开发集dev-matched 9, 815个，开发集dev-mismatched9, 832个，测试集test-matched 9, 796个，测试集test-dismatched9, 847个。因为MNLI是集合了许多不同领域风格的文本，所以又分为了matched和mismatched两个版本的数据集，matched指的是训练集和测试集的数据来源一致，mismached指的是训练集和测试集来源不一致。 任务：句子对，一个前提，一个是假设。前提和假设的关系有三种情况：蕴含（entailment），矛盾（contradiction），中立（neutral）。句子对三分类问题。 评价准则：matched accuracy/mismatched accuracy。 2.7 QNLI QNLI(Qusetion-answering NLI，问答自然语言推断)，自然语言推断任务。QNLI是从另一个数据集The Stanford Question Answering Dataset(斯坦福问答数据集, SQuAD 1.0)[3]转换而来的。SQuAD 1.0是有一个问题-段落对组成的问答数据集，其中段落来自维基百科，段落中的一个句子包含问题的答案。这里可以看到有个要素，来自维基百科的段落，问题，段落中的一个句子包含问题的答案。通过将问题和上下文（即维基百科段落）中的每一句话进行组合，并过滤掉词汇重叠比较低的句子对就得到了QNLI中的句子对。相比原始SQuAD任务，消除了模型选择准确答案的要求；也消除了简化的假设，即答案适中在输入中并且词汇重叠是可靠的提示。 样本个数：训练集104, 743个，开发集5, 463个，测试集5, 461个。 任务：判断问题（question）和句子（sentence，维基百科段落中的一句）是否蕴含，蕴含和不蕴含，二分类。 评价准则：准确率（accuracy）。 2.8 RTE RTE(The Recognizing Textual Entailment datasets，识别文本蕴含数据集)，自然语言推断任务，它是将一系列的年度文本蕴含挑战赛的数据集进行整合合并而来的，包含RTE1[4]，RTE2，RTE3[5]，RTE5等，这些数据样本都从新闻和维基百科构建而来。将这些所有数据转换为二分类，对于三分类的数据，为了保持一致性，将中立（neutral）和矛盾（contradiction）转换为不蕴含（not entailment）。 样本个数：训练集2, 491个，开发集277个，测试集3, 000个。 任务：判断句子对是否蕴含，句子1和句子2是否互为蕴含，二分类任务。 评价准则：准确率（accuracy）。 2.9 WNLI WNLI(Winograd NLI，Winograd自然语言推断)，自然语言推断任务，数据集来自于竞赛数据的转换。Winograd Schema Challenge[6]，该竞赛是一项阅读理解任务，其中系统必须读一个带有代词的句子，并从列表中找到代词的指代对象。这些样本都是都是手动创建的，以挫败简单的统计方法：每个样本都取决于句子中单个单词或短语提供的上下文信息。为了将问题转换成句子对分类，方法是通过用每个可能的列表中的每个可能的指代去替换原始句子中的代词。任务是预测两个句子对是否有关（蕴含、不蕴含）。训练集两个类别是均衡的，测试集是不均衡的，65%是不蕴含。 样本个数：训练集635个，开发集71个，测试集146个。 任务：判断句子对是否相关，蕴含和不蕴含，二分类任务。 评价准则：准确率（accuracy）。 数据集下载使用以下几个官方的下载方法（需要科学上网）： 官方的下载链接：https://gluebenchmark.com/tasks 官方下载脚本：https://github.com/nyu-mll/jiant/blob/master/scripts/download_glue_data.py 下载数据的脚本：https://gist.github.com/W4ngatang/60c2bdb54d156a41194446737ce03e2e","link":"2020/08/31/GLUE/"},{"title":"E2E-ToD","text":"ApproachesMemN2N[NIPS 2015] Sukhbaatar et al., New York University, End-To-End Memory Networks. Given an input set $x_1, .., x_i$ to be stored in memory, the entire set of ${x_i}$ are converted into memory vectors ${m_i}$ of dimension d computed by embedding each $x_i$ in a continuous space with embedding matrices A and C (both of size d × |V |). The query q is also embedded to obtain an internal state $u$ using another embedding matrix B. In the embedding space, we compute the match between $u$ and each memory $m_i$ by taking the inner product followed by a softmax: $p_i = Softmax(u^Tm_i)$, $p$ is a probability vector over the inputs. Each $x_i$ has a corresponding output vector $c_i$, the response vector from the memory $o$ is then a sum over the transformed inputs $c_i$ , weighted by the probability vector from the input: $o = \\sum\\limits_i p_ic_i$ . The predicted label: $\\hat{a} = Softmax(W(o+u))$ . During training, all three embedding matrices A, B and C, as well as W are jointly learned by minimizing a standard cross-entropy loss between $\\hat{a}$ and the true label $a$. GMemN2N[ACL 2017] Fei Liu et al., The University of Melbourne Victoria, Australia, Gated End-to-End Memory Networks. Based on MemN2N, introduced a novel end-to-end memory access regulation mechanism inspired by the current progress on the connection short-cutting principle in the field of computer vision. Highway and Residual Networks Highway Networks (Srivastava et al., NIPS 2015) include a transform gate T and a carry gate C, allowing the network to learn how much information it should transform or carry to form the input to the next layer. $y = H(x) \\odot T(x) + x \\odot C(x)$ or $y = H(x) \\odot T(x) + x \\odot (1-T(x))$ where $H(x), T(x), C(x)$ is a non-linear transformation of its input $x$, $\\odot$ is the Hadamard product. Residual Networks (He et al., CVPR 2016) can be viewed as a special case of Highway Networks where both the transform and carry gates are substituted by the identity mapping function: $y = H(x) + x$ However, Highway Networks is the adaptive gating mechanism, capable of learning to dynamically control the information flow based on the current input. Therefore, we adopt the idea and integrate it into MemN2N, dynamically conditioning the memory reading operation on the controller state $u^k$ at each hop. $T^k(u^k) = \\sigma(W^k_Tu^k + b^k_T)$ $u^{k+1} = \\sigma^k \\odot T^k(u^k) + u^k \\odot (1-T^k(u^k))$ Mem2Seq[ACL 2018] Madotto et al., The Hong Kong University of Science and Technology, Mem2Seq: Effectively Incorporating Knowledge Bases into End-to-End Task-Oriented Dialog Systems. Mem2Seq using global multi-hop attention mechanisms to copy words directly from dialog history or KBs. Main contributions: (1) combines multi-hop attention mechanisms with the idea of pointer networks, which allows us to effectively incorporate KB information. (2) learns how to generate dynamic queries to control the memory access. Memory Encoder: We define all the words in the dialog history as a sequence of tokens $X = {x_1, . . . , x_n, $}$, the KB tuples as $B = {b_1, . . . , b_l}$, $U = [B; X]$ as the concatenation of the two sets $X$ and $B$. $Y = {y_1, . . . , y_m}$ as the set of words in the expected system response. $PTR = {ptr_1 , . . . , ptr_m}$ as the pointer index set. Mem2Seq uses a standard MemN2N with adjacent weighted tying (Sukhbaatar et al., 2015) as an encoder. The input of the encoder is word-level information in $U$. The memories of MemNN are represented by a set of trainable embedding matrices $C = {C1 , . . . , C{K+1}}$, where each $C_k$ maps tokens to vectors, and a query vector $q_k$ is used as a reading head. The attention weights at hop $k$ for each memory i using: $p^k_i = Softmax((q^k)^TC^k_i)$, where $C^k_i = C^k(x_i)$ is the memory content in position $i$. Then, the model reads out the memory $o^k$ by the weighted sum over $C^{k+1}$: $o^k = \\sum\\limits_i p^k_iC^{k+1}_i$. Then, the query vector is updated for the next hop by using $q^{k+1} = q^k + o^k$. Memory Decoder: The decoder uses RNN and MemN2N. GRU is used as a dynamic query generator for the MemNN. At each decoding step $t$, the GRU gets the previously generated word and the previous query as input, and it generates the new query vector: $ht = GRU(C^1(\\hat{y}{t-1}), h_{t-1})$, where $h_0$ is the encoder vector $o^K$. At each time step, two distribution are generated: one over all the words in the vocabulary ($P{vocab})$, and one over the memory contents ($P{ptr})$. $P_{vocab}$ is generated by concatenating the first hop attention read out and the current query vector. $P_{vocab}(\\hat{y}_t) = Softmax(W_1[h_t;o_1])$ $P{ptr}$ is generated using the attention weights at the last MemNN hop of the decoder: $P{ptr} = p^K_t$. If the expected word is not appearing in the memories, then the $P{ptr}$ is trained to produce the sentinel token , then the model generates the token from $P{vocab}$; otherwise, it using the $P_{ptr}$ distribution. Basically, the sentinel token is used as a hard gate to control which distribution to use at each time step. GLMP[ICLR 2019] Wu et al., The Hong Kong University of Science and Technology, Global-to-local Memory Pointer Networks for Task-oriented Dialogue. The dialogue history $X = (x_1, . . . , x_n)$ and the KB information $B = (b_1, . . . , b_l)$ are the input, and the system response $Y = (y_1, . . . , y_m)$ is the expected output. First, the global memory encoder uses a context RNN to encode dialogue history and writes its hidden states into the external knowledge. Then the last hidden state is used to read the external knowledge and generate the global memory pointer at the same time. During the decoding stage, the local memory decoder first generates sketch responses by a sketch RNN. Then the global memory pointer and the sketch RNN hidden state are passed to the external knowledge as a filter and a query. The local memory pointer returns from the external knowledge can copy text from the external knowledge to replace the sketch tags and obtain the final system response. Global contextual representation In the KB memory module, each element bi ∈ B is represented in the triplet format as (Subject, Relation, Object) structure, for example, (Tom’s house, distance, 3 miles). The dialogue context $X$ is stored in the dialogue memory module like a triplet format, ($user, turn1, I). Knowledge read and write Our external knowledge is composed of a set of trainable embedding matrices $C = (C^1 , . . . , C^{K+1})$, where $C^k ∈ R^{|V |×d{emb}}$ , $K$ is the maximum memory hop. We denote memory in the external knowledge as $M = [B; X] = (m_1, . . . , m{n+l})$, where $m_i$ is one of the triplet components mentioned. To read the memory, the external knowledge needs a initial query vector $q$, and computes the attention weights at each hop $k$ using: $p^ki = Softmax((q^k ) ^T c^k_i )$, where $c^k_i = B(C^k (mi)) ∈ R^{d{emb}}$ is the embedding in $i_{th}$ memory position using the embedding matrix $C^k$ , $q^k$ is the query vector for hop $k$, and $B(.)$ is the bag-of-word function. Then, the model reads out the memory $o^k$ by the weighted sum over $c^{k+1}$ and update the query vector $q^{k+1}$: $o^k = \\sum\\limits_i p^k_i c^{k+1}_i, q^{k+1} = q^k + o^k$. GLOBAL MEMORY ENCODER A context RNN is used to model the sequential dependency and encode the context X. Then the hidden states are written into the external knowledge. The last encoder hidden state serves as the query to read the external knowledge and get two outputs, the global memory pointer and the memory readout. A bi-directional gated recurrent unit (GRU) (Chung et al., 2014) is used to encode dialogue history into the hidden states $H = (h^e_1 , . . . , h^e_l )$, and the last hidden state $h^e_n$ is used to query the external knowledge as the encoded dialogue history. LOCAL MEMORY DECODER The local memory decoder first initializes its sketch RNN using the concatenation of $h^e_n$ and $q^{K+1}$, and generates a sketch response that excludes slot values but includes the sketch tags. For example, sketch RNN will generate “@poi is @distance away”. At each decoding time step, if a sketch tag is generated, the global memory pointer is passed to the external knowledge, and the expected output word will be picked up from the local memory pointer. Otherwise, the output word is the word that generated by the sketch RNN. For example, a @poi tag is generated at the first time step, therefore, the word Starbucks is picked up from the local memory pointer as the system output word. GraphDialog[EMNLP 2020] Yang et al., The University of Melbourne, GraphDialog: Integrating Graph Knowledge into End-to-End Task-Oriented Dialogue Systems There are two challenges for such systems: one is how to effectively incorporate external knowledge bases (KBs) into the learning framework; the other is how to accurately capture the semantics of dialogue history. In this paper, we address these two challenges by exploiting the graph structural information in the knowledge base and in the dependency parsing tree of the dialogue. Dialogue Graph We first use the off-the-shelf tool spacy to extract the dependency relations among the words in the dialogue history X. The intuition is that the representation learning of the head words should be allowed being influenced by the dependent words and vice versa. Multi-hop Reasoning Mechanism over Knowledge Graph We extend the graph with multi-hop reasoning mechanism, which aimed to strengthen the reasoning ability over graph as well as to capture the graph structural information between entities via self-attention. Decoder We use a standard Gated Recurrent Unit (GRU) as the decoder to generate the system response word-by-word. DF-Net [Multi-Domain Feature Fusion][ACL 2020] Qin et al., Harbin Institute of Technology, Dynamic Fusion Network for Multi-Domain End-to-end Task-Oriented Dialog. Most neural models rely on large training data, which are only available for a certain number of task domains, such as navigation and scheduling. This makes it difficult to scalable for a new domain with limited labeled data. To this end, we investigate methods that can make explicit use of domain knowledge and introduce a shared-private network to learn shared and specific knowledge. Methods for multi-domain dialogue: (1) mixed multi-domain mixed datasets (2) each domain separately (3) basic shared-private fusion mechanism (4) dynamic fusion mechanism Based on GLMP, we propose to use a shared-private framework including a shared encoder-decoder for capturing domain-shared feature and a private model for each domain to consider the domain-specific features explicitly. Each instance X goes through both the shared and its corresponding private encoder-decoder. MCL [Meta Learning][AAAI 2021] Qin et al., Shenzhen Institutes of Advanced Technology, Chinese Academy of Sciences, Exploring Auxiliary Reasoning Tasks for Task-oriented Dialog Systems with Meta Cooperative Learning. Contributions Solved Problems This model consists of three networks: (i) an auxiliary KB reasoning task for learning meta KB knowledge; (ii) an auxiliary dialogue reasoning task for learning dialogue patterns; (iii) a TDS task (primary task) for not only retrieving accurate entities from KB but also generating natural responses. Given the input: (1) dialog history that includes a sequence of historical user utterances ${u1, . . . , u_c}$ and system response utterances ${s_1, . . . , s{c−1}}$, and (2) KB tuples ${b_1, . . . , b_l}$, the goal of the task-oriented dialog generation (primary task) is to generate the next system response $s_c = {y_1, y_2, . . . , y_T }$ word by word, where $c$ and $l$ represent the numbers of utterances and KB tuples, $T$ is the length of the generated response. Our proposed task-oriented dialog generation model is composed of four primary components: a dialog encoder, a dialog memory, a KB memory, and a response decoder. Dialog Encoder For the $i$-th ($i$ &gt; 1) turn, the input is ${s{i−1}, u_i}$ consisting of the last system response $s{i−1}$ and current user request $ui$ .We define the input ${s{i−1}, ui}$ at each turn as dialog context, which is denoted as a sequence of tokens $X = (x_1, x_2, . . . , x_m)$. First, each token is converted into a word embedding through an embedding layer. Then, we apply a BiGRU (Chung et al. 2014) to encode the dialog context into hidden states: $h_t = BiGRU(e(x_t), h{t−1})$. where $e(x_t)$ denotes the embedding of word $x_t$. Dialog Memory We propose a dialog memory to reason over dialog context, which is implemented with a dynamic key-value memory network (Wang et al. 2020). The dialog memory network contains a dialog key memory and a dialog value memory. The dialog key memory keeps updated at each turn for tracking the dialog history, while the dialog value memory keeps fixed for storing the representation of dialog context. KB Memory We employ a separate KB memory to encode the KB information. We access the KB memory by K-hop reading mechanism. Specifically, we use an initial query vector (decoder hidden state) as the reading head, and it loops over K hops and computes the soft attention weights at each hop. The soft memory attention decides the relevance between each memory vector and the query vector. Response Decoder Specifically, the t-th word in the target response is either copied from dialog value memory/KB value memory or generated from the overall vocabulary. Similarly, we use $Pd(y_t)$ and $P{kb}(yt)$ to represent the probabilities for copying the $t$-th word from the dialog memory and KB memory, respectively. A soft gate $g_1$ determines whether a word is copied from memories or generated from the vocabulary, and another gate $g_2$ determines which of the two memories is used to copy values. The final output distribution $Pθ(yt)$ for the t-th target word is computed as: $Pθ(yt) = g_1P_v(y_t) + (1 − g_1) [g_2P_d(y_t) + (1 − g_2) P{kb}(y_t)]$. We compute the loss function $L{primary}$ as the cross-entropy between the predicted word distribution $Pθ(yt)$ and the ground-truth target word distribution $y_t$: $L{primary} (θ) = − \\sum\\limits^T{t=1} y_t log (Pθ(y_t))$, where T indicates the length of the output response. TPEM [Continual Learning][ACL 2021] Geng et al., University of Science and Technology of China, Continual Learning for Task-oriented Dialogue System with Iterative Network Pruning, Expanding and Masking. Continual learning is hardly a new idea for machine learning, but remains as a non-trivial step for building empirically successful AI systems. It is essentially the case for creating a high-quality TDS. On the one hand, a dialogue system is expected to reuse previously acquired knowledge, but focusing too much on stability may hinder a TDS from quickly adapting to a new task. On the other hand, when a TDS pays too much attention to plasticity, it may quickly forget previously-acquired abilities. In this paper, we propose a continual learning method for task-oriented dialogue system with iterative network pruning, expanding and masking (TPEM), which preserves performance on previously encountered tasks while accelerating learning progress on the future tasks. TPEM adopts the global-to-local memory pointer networks (GLMP) as the base model. Network Pruning To avoid “catastrophic forgetting” of GLMP, a feasible way is to add weights for learning new tasks. However, as the number of tasks grows, the complexity of model architecture increases rapidly, making the deep model difficult to train. To avoid constructing a huge network, we compress the model for the current task by releasing a certain fraction of neglectable weights of old tasks by setting them to zero. To avoid changing the network connectivity, we re-train the preserved weights for a small number of epochs. When inferring task k, the released weights are masked in a binary on/off fashion. Network Expanding The amount of preserved weights for old tasks becomes larger with the growth of new tasks, and there will be fewer free weights for learning new tasks, resulting in slowing down the learning process and making the found solution non-optimal. An intuitive solution is to expand the model while learning new tasks. To effectively perform network expansion while keeping the compactness of network architecture, we define the following strategy to expand the hidden size $Hk$ for the $k$-th task from $H{k−1}$: $Hk = H{k−1} + α ∗ (P_{k−1} − F_k) ∗ log(1 + N_k/β)$, where α and β are two hyperparameters. Network Masking However, not all preserved weights are beneficial to learn new tasks, especially when there is a large gap between old and new tasks. To resolve this issue, we apply a learnable binary mask $M^k$ for each task k to filter some old weights that may hinder the learning of new tasks. $M^k_ij = \\begin{cases} 1, &amp; if \\tilde{M}^k_ij &gt; τ \\ 0, &amp;ortherwise \\end{cases}$ Note that old weights WP k are “picked” only and keep unchanged during training. Thus, old tasks can be recalled without forgetting. CDNET [KB Distillation][AAAI 2021] Raghu et al., IBM Research, Constraint based Knowledge Base Distillation in End-to-End Task Oriented Dialogs. Inferring those KB entities that are most relevant for an utterance is crucial for response generation. However, distilling the KB by softly filtering irrelevant KB information based on the dialog history revealed that embeddings learnt for entities of the same type are quite close to each other. Such embeddings hurt the overall performance as they reduce the gap between relevant and irrelevant KB records. Moreover, GLMP performs KB distillation but fails to capture the relationship across attributes in KB records. This paper proposed two methods to distill irrelevant KB records: (1) a pairwise similarity score and (2) an embedding constraint loss. Content Encoder The dialog history H is encoded using a hierarchical encoder with two GRU. KB Encoder We encode the KB using the multi-level memory. In the first level, each KB record is represented as sum of its attributes. In the second level, each attribute is a key-value pair. KB Distillation The KB distillation module softly filters irrelevant KB records based on the dialog history by computing a distillation distribution ($P_d$) over the KB records. $Pd = [d_1, d_2, …, d_M]$, $d_m = Softmax(s_m)$, $s_m = \\sum\\limits{w∈H} \\sum\\limits_{v^n_m∈r_m}CosSim(Φ^e(w), Φ^e(v^n_m))$ where CosSim is the cosine similarity between two vectors. Sketch RNN We use a GRU to generate the sketch response. At each time t, a generate distribution $Pg$ is computed using the decoder hidden state $h_t$ and an attended summary of the dialog context $g_t = \\sum_i\\sum_ja{ij}w^ji$, where $a{ij}$ is the Luong attention weights over the context word representations $w^j_i$. Context Memory Pointer Same as GLMP. At each time t, generate the copy distribution over the context $P{con}$ by performing a multi-hop Luong attention over the context memory. The initial query $q^0_t$ is set to $h_t$, $q^0_t$ is then attended over the context to generate an attention distribution $a^1$ and a summarized context $g^1_t$. In the next hop the same process is repeated by updating the query $q^1_t = q^0_t + g^1_t$. The attention weights after H hops is used for computing the context pointer $P{con}$ as follows: $P{con}(y_t = w) = \\sum{ij:w^ji = w} a^H{ij}$ KB Memory Pointer At each time t, we generate the copy distribution over the KB $P_{kb}$ using: (1) Luong attention weight $β^t_m$ over the KB record $r_m$ (2) Luong attention weight $γ^t_n$ over attribute keys in a record $k_n$ (3) the distillation weight $d_m$ over the KB record $r_m$ The KB pointer $P_{kb}$ is computed as follows: The two copy pointers are combined using a soft gate α to get the final copy distribution $P_c$ as follows: $Pc(y_t) = αP{kb}(yt) + (1 − α)P{con}(y_t)$ DatasetsbAbI[Multi-Task] The bAbI dialog includes five end-to-end dialog learning tasks in the restaurant domain, which are simulated dialog data. Task 1 to 4 are about API calls, refining API calls, recommending options, and providing additional information, respectively. Task 5 is the union of tasks 1-4. There are two test sets for each task: one follows the same distribution as the training set and the other has out-of-vocabulary (OOV) entity values that does not exist in the training set. Along with the train, dev and test sets, we also include a knowledge base file (dialog-babi-kb-all.txt) that contain all entities appearing in dialogs for tasks 1-5. We also include a file containing the candidates to select the answer from (dialog-babi-candidates.txt) for tasks 1-5, that is simply made of all the bot utterances in train, dev, test for these tasks. The goal of the tasks is to predict the bot utterances, that can be sentences or API calls. In-Car Assistant (SMD, KVRET)The In-Car Assistant dataset is composed of 3031 multi-turn dialogs from calendar scheduling, weather information retrieval, and point-of-interest navigation domains. The average number of turns per dialog is about 2.6. CamRestThis corpus contains 676 multi-turn dialogs belonging to restaurant reservation domain. There are 5 turns on average per dialog. Multi-WOZ[Multi-domain] Multi-Domain Wizard-of-Oz dataset (MultiWOZ), a fully-labeled collection of human-human written conversations spanning over multiple domains and topics. There are 3,406 single-domain dialogues that include booking if the domain allows for that and 7,032 multi-domain dialogues consisting of at least 2 up to 5 domains. The test and development sets contain 1k examples each. DSTC-2The Dialog State Tracking Challenges 2 &amp; 3 (DSTC2&amp;3) were research challenge focused on improving the state of the art in tracking the state of spoken dialog systems. DSTC-2 released a large number of training dialogs related to restaurant search. Evaluation MetricsBLEUBLEU measures the n-gram (i.e., 4-gram) overlap between the produced responses and the gold responses. BLEU is a popular metric to measure the TDS ’s ability to accurately generate the dialog from the language model perspective. Entity-F1Entity F1 measures the system’s capability of generating relevant entities to accomplish certain tasks by retrieving accurate entities from the provided KB. There are a set of reference entities for each utterance. The entity F1 score is computed by micro-averaging the precision and recall over KB entities of the generated responses.","link":"2021/11/29/E2E-ToD/"},{"title":"Git 使用指南","text":"1. 工作原理 Workspace：工作区Index / Stage：暂存区Repository：本地仓库Remote：远程仓库 基本的 Git 工作流程如下： 在工作区中修改文件。 将你想要下次提交的更改选择性地暂存，这样只会将更改的部分添加到暂存区。 提交更新，找到暂存区的文件，将快照永久性存储到 Git 目录。 2. 配置用户信息 设置你的用户名和邮件地址： 12$ git config --global user.name &quot;John Doe&quot;$ git config --global user.email johndoe@example.com 检查配置信息 1$ git config --list 设置代理 12$ git config --global http.proxy 'http://127.0.0.1:1080'$ git config --global https.proxy 'https://127.0.0.1:1080' 3. 获取 Git 仓库新建仓库 通过 Github 创建个人仓库。 在本地的仓库路径下，初始化 git 项目 123456$ git init$ git add &lt;your_files&gt;$ git commit -m &quot;commit message&quot;$ git branch -M main$ git remote add origin &lt;your_git&gt;$ git push -u origin main 克隆现有的仓库 1$ git clone https://github.com/libgit2/libgit2 4. 记录更新工作目录下的每一个文件都不外乎这两种状态：已跟踪 或 未跟踪。 已跟踪的文件是指那些被纳入了版本控制的文件，在上一次快照中有它们的记录，在工作一段时间后， 它们的状态可能是未修改，已修改或已放入暂存区。 编辑过某些文件之后，由于自上次提交后你对它们做了修改，Git 将它们标记为已修改文件。 在工作时，你可以选择性地将这些修改过的文件放入暂存区，然后提交所有已暂存的修改，如此反复。 检查当前文件状态 可以用 git status 命令查看哪些文件处于什么状态。 在项目下创建一个新的 README 文件，新建的 README 文件出现在 Untracked files 下面。 未跟踪的文件意味着 Git 在之前的快照（提交）中没有这些文件；Git 不会自动将之纳入跟踪范围。 跟踪新文件 使用命令 git add 开始跟踪一个文件。 1$ git add README 此时再运行 git status 命令，会看到 README 文件已被跟踪，并处于暂存状态。在 Changes to be committed 这行下面的，就说明是已暂存状态。 暂存已修改的文件 修改一个已被跟踪的文件，出现在 Changes not staged for commit 这行下面，说明已跟踪文件的内容发生了变化，但还没有放到暂存区。要暂存这次更新，需要运行 git add 命令。 git add 是个多功能命令：可以用它开始跟踪新文件，或者把已跟踪的文件放到暂存区，还能用于合并时把有冲突的文件标记为已解决状态等。将这个命令理解为“精确地将内容添加到下一次提交中”而不是“将一个文件添加到项目中”要更加合适。 运行了 git add 之后又作了修订的文件，需要重新运行 git add 把最新版本重新暂存起来。 忽略文件 一般我们总会有些文件无需纳入 Git 的管理，也不希望它们总出现在未跟踪文件列表。 在这种情况下，我们可以创建一个名为 .gitignore 的文件，列出要忽略的文件的模式。GitHub 有一个十分详细的针对数十种项目及语言的 .gitignore 文件列表， 你可以在 https://github.com/github/gitignore 找到它。 查看已暂存和未暂存的修改 想知道具体修改了什么地方，可以用 git diff 命令。 要查看尚未暂存的文件更新了哪些部分，不加参数直接输入 git diff，此命令比较的是工作目录中当前文件和暂存区域快照之间的差异，也就是修改之后还没有暂存起来的变化内容。 若要查看已暂存的将要添加到下次提交里的内容，可以用 git diff --staged 命令。这条命令将比对已暂存文件与最后一次提交的文件差异。 请注意，git diff 本身只显示尚未暂存的改动，而不是自上次提交以来所做的所有改动。 提交更新 现在的暂存区已经准备就绪，可以提交了。在此之前，请务必确认还有什么已修改或新建的文件还没有 git add 过， 否则提交的时候不会记录这些尚未暂存的变化。所以，每次准备提交前，先用 git status 看下，你所需要的文件是不是都已暂存起来了， 然后再运行提交命令 git commit。 每一次运行提交操作，都是对你项目作一次快照，以后可以回到这个状态，或者进行比较。 跳过暂存 尽管使用暂存区域的方式可以精心准备要提交的细节，但有时候这么做略显繁琐。只要在提交的时候，给 git commit 加上 -a 选项，Git 就会自动把所有已经跟踪过的文件暂存起来一并提交，从而跳过 git add 步骤。这很方便，但是要小心，有时这个选项会将不需要的文件添加到提交中。 移除文件 要从 Git 中移除某个文件，就必须要从已跟踪文件清单中移除（确切地说，是从暂存区域移除），然后提交。可以用 git rm 命令完成此项工作，并连带从工作目录中删除指定的文件，这样以后就不会出现在未跟踪文件清单中了。 如果只是简单地从工作目录中手工删除文件，删除的文件会记录在“Changes not staged for commit” 中，需要再运行 git rm 记录此次移除文件的操作，下一次提交时，该文件就不再纳入版本管理了。 如果要删除之前修改过或已经放到暂存区的文件，则必须使用强制删除选项 -f。 另外一种情况是，我们想把文件从 Git 仓库中删除（亦即从暂存区域移除），但仍然希望保留在当前工作目录中。 换句话说，你想让文件保留在磁盘，但是并不想让 Git 继续跟踪。当你忘记添加 .gitignore 文件，不小心把一个很大的日志文件或一堆 .a 这样的编译生成文件添加到暂存区时，这一做法尤其有用。 为达到这一目的，使用 --cached 选项： 1$ git rm --cached README 移动文件 Git 并不显式跟踪文件移动操作。 如果在 Git 中重命名了某个文件，仓库中存储的元数据并不会体现出这是一次改名操作。 不过 Git 非常聪明，它会推断出究竟发生了什么。想要移动文件或重命名，你可以进行以下操作： 1$ git mv README.md README 其实，运行 git mv 就相当于运行了下面三条命令： 123$ mv README.md README$ git rm README.md$ git add README 两者唯一的区别在于，git mv 是一条命令而非三条命令，直接使用 git mv 方便得多。 不过在使用其他工具重命名文件时，记得在提交前 git rm 删除旧文件名，再 git add 添加新文件名。 5. 查看提交历史运行 git log 命令时，会按时间先后顺序列出所有的提交，最近的更新排在最上面。这个命令会列出每个提交的 SHA-1 校验和、作者的名字和电子邮件地址、提交时间以及提交说明。 一个比较有用的选项是 -p 或 --patch ，它会显示每次提交所引入的差异（按 补丁 的格式输出）。 你也可以限制显示的日志条目数量，例如使用 -2 选项来只显示最近的两次提交： 1git log -p -2 该选项除了显示基本信息之外，还附带了每次提交的变化。 想看到每次提交的简略统计信息，可以使用 --stat 选项： 1$ git log --stat 另一个非常有用的选项是 --pretty。 这个选项可以使用不同于默认格式的方式展示提交历史。比如 oneline 会将每个提交放在一行显示，在浏览大量的提交时非常有用。 另外还有 short，full 和 fuller 选项，它们展示信息的格式基本一致，但是详尽程度不一： 1$ git log --pretty=oneline format ，可以定制记录的显示格式： 1$ git log --pretty=format:&quot;%h - %an, %ar : %s&quot; %H 提交的完整哈希值 %h 提交的简写哈希值 %an 作者名字 %ae 作者的电子邮件地址 %cn 提交者的名字 %ce 提交者的电子邮件地址 %cd 提交日期 %cr 提交日期（距今多长时间） %s 提交说明 作者指的是实际作出修改的人，提交者指的是最后将此工作成果提交到仓库的人。 6. 撤销操作修补上一次提交 有时候我们提交完了才发现漏掉了几个文件没有添加，或者提交信息写错了。 此时，可以运行带有 --amend 选项的提交命令来重新提交： 1$ git commit --amend 这个命令会将暂存区中的文件提交。如果自上次提交以来你还未做任何修改，那么快照会保持不变，而你所修改的只是提交信息。例如，你提交后发现忘记了暂存某些需要的修改，可以像下面这样操作： 123$ git commit -m 'initial commit'$ git add forgotten_file$ git commit --amend 最终你只会有一个提交——第二次提交将代替第一次提交的结果。实际上，这是完全用一个 新的提交 替换旧的提交。 取消暂存的文件 使用 git reset HEAD &lt;file&gt;… 来取消暂存的文件。 撤消对文件的修改 如果你并不想保留对文件的修改，如何方便地撤消修改——将它还原成上次提交时的样子？git checkout -- &lt;file&gt;... 命令能够撤销暂存区中文件在本地的修改——Git 会用最近提交的版本覆盖掉它。 请务必记得 git checkout — &lt;file&gt; 是一个危险的命令。 7. 管理远程仓库查看远程仓库 运行 git remote 命令，会列出你指定的每一个远程服务器的简写。如果你已经克隆了自己的仓库，那么至少应该能看到 origin ——这是 Git 给你克隆的仓库服务器的默认名字。 可以指定选项 -v，会显示需要读写远程仓库使用的 Git 保存的简写与其对应的 URL。如果你的远程仓库不止一个，该命令会将它们全部列出。这表示我们能非常方便地拉取其它用户的贡献。 添加远程仓库 运行 git remote add &lt;shortname&gt; &lt;url&gt; 添加一个新的远程 Git 仓库，同时指定一个方便使用的简写： 12345678$ git remoteorigin$ git remote add pb https://github.com/paulboone/ticgit$ git remote -vorigin https://github.com/schacon/ticgit (fetch)origin https://github.com/schacon/ticgit (push)pb https://github.com/paulboone/ticgit (fetch)pb https://github.com/paulboone/ticgit (push) 如果你想拉取 Paul 的仓库中有但你没有的信息，可以运行 git fetch pb： 1$ git fetch pb 从远程仓库中抓取与拉取 1$ git fetch &lt;remote&gt; 这个命令会访问远程仓库，从中拉取所有你还没有的数据，可以随时合并或查看。 注意 git fetch 命令只会将数据下载到你的本地仓库——它并不会自动合并或修改你当前的工作。 当准备好时你必须手动将其合并入你的工作。 git pull 命令会自动抓取后合并该远程分支到当前分支。 推送到远程仓库 1$ git push &lt;remote&gt; &lt;branch&gt; 只有当你有所克隆服务器的写入权限，并且之前没有人推送过时，这条命令才能生效。 当你和其他人在同一时间克隆，他们先推送到上游然后你再推送到上游，你的推送就会毫无疑问地被拒绝。你必须先抓取他们的工作并将其合并进你的工作后才能推送。 远程仓库的重命名与移除 运行 git remote rename 来修改一个远程仓库的简写名。例如，想要将 pb 重命名为 paul，可以用 git remote rename 这样做： 1$ git remote rename pb paul 这同样也会修改你所有远程跟踪的分支名字。 如果想要移除一个远程仓库，可以使用 git remote remove 或 git remote rm： 1$ git remote remove paul 一旦你使用这种方式删除了一个远程仓库，那么所有和这个远程仓库相关的远程跟踪分支以及配置信息也会一起被删除。 8. 打标签Git 可以给仓库历史中的某一个提交打上标签，以示重要。 列出标签 只需要输入 git tag 也可以按照特定的模式查找标签。例如，如果只对 1.8.5 系列感兴趣，可以运行： 1$ git tag -l &quot;v1.8.5*&quot; 创建标签 Git 支持两种标签：轻量标签（lightweight）与附注标签（annotated）。 轻量标签 轻量标签本质上是将提交校验和存储到一个文件中——没有保存任何其他信息。创建轻量标签，只需要提供标签名字： 1$ git tag v1.4-lw 这时，如果在标签上运行 git show，你不会看到额外的标签信息。 命令只会显示出提交信息。 辅助标签 附注标签是存储在 Git 数据库中的一个完整对象， 它们是可以被校验的，其中包含打标签者的名字、电子邮件地址、日期时间， 此外还有一个标签信息，并且可以使用 GNU Privacy Guard （GPG）签名并验证。 在 Git 中创建附注标签十分简单。 最简单的方式是当你在运行 tag 命令时指定 -a 选项： 12345$ git tag -a v1.4 -m &quot;my version 1.4&quot;$ git tagv0.1v1.3v1.4 -m 选项指定了一条将会存储在标签中的信息。 通过使用 git show 命令可以看到标签信息和与之对应的提交信息。 后期打标签 假设在 v1.2 时你忘记给项目打标签，你可以在之后补上标签。 要在那个提交上打标签，你需要在命令的末尾指定提交的校验和（或部分校验和）： 1$ git tag -a v1.2 9fceb02 共享标签 默认情况下，git push 命令并不会传送标签到远程仓库服务器上。在创建完标签后你必须显式地推送标签到共享服务器上。这个过程就像共享远程分支一样——你可以运行 git push origin &lt;tagname&gt;。 如果想要一次性推送很多标签，也可以使用带有 --tags 选项的 git push 命令。 这将会把所有不在远程仓库服务器上的标签全部传送到那里。 1$ git push origin --tags 删除标签 要删除掉你本地仓库上的标签，可以使用命令 git tag -d &lt;tagname&gt;。注意上述命令并不会从任何远程仓库中移除这个标签，你必须用 git push &lt;remote&gt; :refs/tags/&lt;tagname&gt; 来更新你的远程仓库。 第二种更直观的删除远程标签的方式是： 1$ git push origin --delete &lt;tagname&gt; 9. Git 分支Git 保存的不是文件的变化或者差异，而是一系列不同时刻的快照。 在进行提交操作时，Git 会保存一个提交对象（commit object）。该提交对象会包含一个指向暂存内容快照的指针。 当使用 git commit 进行提交操作时，Git 会先计算每一个子目录（本例中只有项目根目录）的校验和， 然后在 Git 仓库中这些校验和保存为树对象。随后，Git 便会创建一个提交对象， 它除了包含上面提到的那些信息外，还包含指向这个树对象（项目根目录）的指针。 如此一来，Git 就可以在需要的时候重现此次保存的快照。 现在，Git 仓库中有五个对象：三个 blob 对象（保存着文件快照）、一个 树 对象 （记录着目录结构和 blob 对象索引）以及一个 提交 对象（包含着指向前述树对象的指针和所有提交信息）。 做些修改后再次提交，那么这次产生的提交对象会包含一个指向上次提交对象（父对象）的指针。 Git 的分支，其实本质上仅仅是指向提交对象的可变指针。 分支创建 Git 是怎么创建新分支的呢？ 很简单，它只是为你创建了一个可以移动的新的指针。比如，创建一个 testing 分支， 你需要使用 git branch 命令： 1$ git branch testing Git 是怎么知道当前在哪一个分支上呢？很简单，它有一个名为 HEAD 的特殊指针，指向当前所在的本地分支。 分支切换 要切换到一个已存在的分支，需要使用 git checkout 命令： 1$ git checkout testing 这样 HEAD 就指向 testing 分支了。 新建分支 想要新建一个分支并同时切换到那个分支上，你可以运行一个带有 -b 参数的 git checkout 命令： 12$ git checkout -b iss53Switched to a new branch &quot;iss53&quot; 它是下面两条命令的简写： 12$ git branch iss53$ git checkout iss53 你在新建的分支上上工作，并且做了一些提交。 在此过程中，iss53 分支在不断的向前推进。 现在有个紧急问题等待你来解决。你不必把这个紧急问题和 iss53 的修改混在一起，也不需要花大力气来还原关于 53# 问题的修改，你所要做的仅仅是切换回 master 分支。在你这么做之前，要留意你的工作目录和暂存区里那些还没有被提交的修改， 它可能会和你即将检出的分支产生冲突从而阻止 Git 切换到该分支。 最好的方法是，在你切换分支之前，保持好一个干净的状态。 现在，我们假设你已经把你的修改全部提交了，这时你可以切换回 master 分支了： 1$ git checkout master 当你切换分支的时候，Git 会重置你的工作目录，使其看起来像回到了你在那个分支上最后一次提交的样子。 接下来，你要修复这个紧急问题。建立一个 hotfix 分支，在该分支上工作直到问题解决： 你可以运行你的测试，确保你的修改是正确的，然后将 hotfix 分支合并回你的 master 分支来部署到线上。可以使用 git merge 命令来达到上述目的： 12$ git checkout master$ git merge hotfix 现在，最新的修改已经在 master 分支所指向的提交快照中，你可以着手发布该修复了。 你应该先删除 hotfix 分支，因为你已经不再需要它了 —— master 分支已经指向了同一个位置。你可以使用带 -d 选项的 git branch 命令来删除分支： 1$ git branch -d hotfix 现在你可以切换回你正在工作的分支继续你的工作了。 合并分支 假设你已经修正了 #53 问题，并且打算将你的工作合并入 master 分支。你只需要检出到你想合并入的分支，然后运行 git merge 命令： 123456$ git checkout masterSwitched to branch 'master'$ git merge iss53Merge made by the 'recursive' strategy.index.html | 1 +1 file changed, 1 insertion(+) 这和之前合并 hotfix 分支的时候看起来有一点不一样。 在这种情况下，你的开发历史从一个更早的地方开始分叉开来（diverged）。 因为，现在的 master 分支所在提交并不是 iss53 分支所在提交的直接祖先，Git 不得不做一些额外的工作。 出现这种情况的时候，Git 会使用两个分支的末端所指的快照（C4 和 C5）以及这两个分支的公共祖先（C2），做一个简单的三方合并。 Git 将此次三方合并的结果做了一个新的快照并且自动创建一个新的提交指向它。 这个被称作一次合并提交，它的特别之处在于他有不止一个父提交。 既然你的修改已经合并进来了，就不再需要 iss53 分支了。 现在你可以在任务追踪系统中关闭此项任务，并删除这个分支。 1$ git branch -d iss53 遇到冲突时的分支合并 有时候合并操作不会如此顺利。 如果你在两个不同的分支中，对同一个文件的同一个部分进行了不同的修改，Git 就没法干净的合并它们，在合并它们的时候就会产生合并冲突。 此时 Git 做了合并，但是没有自动地创建一个新的合并提交。 Git 会暂停下来，等待你去解决合并产生的冲突。 你可以在合并冲突后的任意时刻使用 git status 命令来查看那些因包含合并冲突而处于未合并（unmerged）状态的文件，你可以打开这些包含冲突的文件然后手动解决冲突。 出现冲突的文件会包含一些特殊区段，看起来像下面这个样子： 1234567&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD:index.html&lt;div id=&quot;footer&quot;&gt;contact : email.support@github.com&lt;/div&gt;=======&lt;div id=&quot;footer&quot;&gt; please contact us at support@github.com&lt;/div&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53:index.html HEAD 所指示的版本在这个区段的上半部分（======= 的上半部分），而 iss53 分支所指示的版本在 ======= 的下半部分。 为了解决冲突，你必须选择使用由 ======= 分割的两部分中的一个，或者你也可以自行合并这些内容。 在你解决了所有文件里的冲突之后，对每个文件使用 git add 命令来将其标记为冲突已解决。 一旦暂存这些原本有冲突的文件，Git 就会将它们标记为冲突已解决。你可以再次运行 git status 来确认所有的合并冲突都已被解决。 如果你对结果感到满意，并且确定之前有冲突的的文件都已经暂存了，这时你可以输入 git commit 来完成合并提交。 分支管理 git branch 命令不只是可以创建与删除分支。 如果不加任何参数运行它，会得到当前所有分支的一个列表。 如果需要查看每一个分支的最后一次提交，可以运行 git branch -v 命令。 远程分支","link":"2021/10/02/Git%E4%BD%BF%E7%94%A8%E6%8C%87%E5%8D%97/"},{"title":"Neo4j Cypher 基本用法","text":"1. 简介 没有SQL 图形数据库 GDBMS 关系检索快 表示节点，关系和属性中的数据 节点和关系都包含属性 关系连接节点 属性是键值对 节点用圆圈表示，关系用方向键表示。 关系具有方向：单向和双向。 2. 构成Neo4j图数据库主要有以下构建块 节点 属性 关系 标签 数据浏览器 节点 节点是图表的基本单位。 它包含具有键值对的属性。如下图： 属性 键值对，用来描述图像、关系。 关系 表示两个节点之间的关系。有向。 标签 Label将一个公共名称与一组节点或关系相关联。 节点或关系可以包含一个或多个标签。 我们可以为现有节点或关系创建新标签。 我们可以从现有节点或关系中删除现有标签。 注： Neo4j将数据存储在节点或关系的属性中。 Neo4j 数据浏览器 neo4j start http://localhost:7474/browser/ 3. Cypher 查询语言 关键词 CQL 用法 CREATE 创建节点、关系和属性 MATCH 检索有关节点、关系和属性 RETURN 返回查询的结果 WHERE 查询条件 DELETE 删除节点、关系 REMOVE 删除节点、关系的属性 ORDER BY 排序 SET 添加或更新标签 数据类型 | 数据类型 | 说明 || :———: | :————————: || boolean | True，False || byte | 用于表示8位整数 || short | 用于表示16位整数 || int | 用于表示32位整数 || long | 用于表示64位整数 || float | 用于表示32位浮点数 || double | 用于表示64位浮点数 || char | 用于表示16位字符 || string | 用于表示字符串 | CREATE用于创建节点。 1234# 单个标签CREATE (&lt;node-name&gt;:&lt;label-name&gt;)# 多个标签CREATE (&lt;node-name&gt;:&lt;label-name1&gt;:&lt;label-name2&gt;.....:&lt;label-namen&gt;) : 节点名称 : 标签名称 创建时可以带有属性： 12345678CREATE ( &lt;node-name&gt;:&lt;label-name&gt; { &lt;Property1-name&gt;:&lt;Property1-Value&gt; ........ &lt;Propertyn-name&gt;:&lt;Propertyn-Value&gt; }) 例如： 1CREATE (dept:Dept { deptno:10,dname:&quot;Accounting&quot;,location:&quot;Hyderabad&quot; }) 注意：属性以字典形式声明，属性名不用加引号，属性值数字不加引号，字符加引号。属性间用逗号间隔，属性、标签和属性值之间不用逗号隔开。 MATCH1MATCH (&lt;node-name&gt;:&lt;label-name&gt;) : 节点名称 : 标签名称 例如： 1MATCH (e: Employee) RETURN e 注意：我们不能单独使用MATCH Command从数据库检索数据。 ​ MATCH (e: ‘Employee’) RETURN e ​ MATCH (e: “Employee”) RETURN e ​ MATCH (e: Employee) RETURN e 三个命令相同，我们可以选择这些命令中的任何一个。 RETURN用于检索节点、关系的属性 1234RETURN &lt;node-name&gt;.&lt;property1-name&gt;, ........ &lt;node-name&gt;.&lt;propertyn-name&gt; 例如： 1RETURN dept.deptno 注意：同样，不能单独使用RETURN语句返回属性 MATCH &amp; RETURNmatch 和 return 联合使用才能完成查询： 12MATCH (dept: Dept)RETURN dept.deptno,dept.dname 查询节点所有属性： 12MATCH (dept: Dept)RETURN dept 关系查询 创建节点之间的关系 123CREATE (&lt;node1-name&gt;:&lt;label1-name&gt;)- [(&lt;relationship-name&gt;:&lt;relationship-label-name&gt;)] -&gt;(&lt;node2-name&gt;:&lt;label2-name&gt;) 例如： 1CREATE (p1:Profile1)-[r1:LIKES]-&gt;(p2:Profile2) WHERE简单条件 1WHERE &lt;condition&gt; 复合条件 1WHERE &lt;condition&gt; &lt;boolean-operator&gt; &lt;condition&gt; 用法 1&lt;property-name&gt; &lt;comparison-operator&gt; &lt;value&gt; 其中，比较运算符有： 运算符 说明 = 等于 &lt;&gt; 不等于 &lt; 小于 &gt; 大于 &lt;= 小于等于 >= 大于等于 条件之间的布尔操作有： 布尔操作 说明 AND 与 OR 或 NOT 非 XOR 异或 例如： 123MATCH (emp:Employee) WHERE emp.name = 'Abc' OR emp.name = 'Xyz'RETURN emp 创建关系例如： 1234MATCH (cust:Customer),(cc:CreditCard) WHERE cust.id = &quot;1001&quot; AND cc.id= &quot;5001&quot; CREATE (cust)-[r:DO_SHOPPING_WITH{shopdate:&quot;12/12/2014&quot;,price:55000}]-&gt;(cc) RETURN r DELETE用于删除节点及其关联的属性 1DELETE &lt;node-name-list&gt; 1DELETE &lt;node1-name&gt;,&lt;node2-name&gt;,&lt;relationship-name&gt; 注意：我们应该使用逗号（，）运算符来分隔节点名。 例如： 删除节点 1MATCH (e: Employee) DELETE e 删除关系 1MATCH (cc: CreditCard)-[rel]-(c:Customer) DELETE cc,c,rel REMOVE用于删除现有节点或关系的属性 Neo4j CQL DELETE和REMOVE命令之间的主要区别： DELETE操作用于删除节点和关联关系。 REMOVE操作用于删除标签和属性。 注：两个命令都应该与MATCH命令一起使用。 删除节点属性： 1REMOVE &lt;property-name-list&gt; 例如： 123MATCH (book { id:122 })REMOVE book.priceRETURN book 等同于： 1234567MATCH (book: Book)WHERE book.id = 122REMOVE book.priceRETURN book 等同于： 12SELECT * FROM BOOK WHERE ID = 122;ALTER TABLE BOOK REMOVE COLUMN PRICE; 删除节点、关系的某个标签 1REMOVE &lt;label-name-list&gt; 例如： 12MATCH (m:Movie) REMOVE m:Picture SET用于为节点、关系添加或更新属性 1SET &lt;node-label-name&gt;.&lt;property1-name&gt; 例如： 123MATCH (book:Book)SET book.title = 'superstar'RETURN book ORDER BY用于查询结果排序，默认是DESC（降序） 例如： 123MATCH (emp:Employee)RETURN emp.empid,emp.name,emp.salary,emp.deptnoORDER BY emp.name DESC UNION &amp; UNION ALLUNION 用于将两组结果中的公共行组合并返回到一组结果中。 123&lt;MATCH Command1&gt; UNION&lt;MATCH Command2&gt; 例如： 123MATCH (cc:CreditCard) RETURN cc.id,cc.numberUNIONMATCH (dc:DebitCard) RETURN dc.id,dc.number 返回包含在两个查询结果中的不重复的行。 UNION ALL 则返回两组结果中的所有行。 LIMIT用来过滤或限制查询返回的行数。 1LIMIT &lt;number&gt; 例如： 123MATCH (emp:Employee) RETURN empLIMIT 2 只显示Top 2 的结果。 对应地，SKIP用于跳过最前面的查询结果： 1SKIP &lt;number&gt; 例如： 123MATCH (emp:Employee) RETURN empSKIP 2 返回从第3个开始的结果。 MERGE用于将新的节点添加到数据库，如果存在则返回该节点；否则创建新节点。 12345678MERGE = CREATE + MATCHMERGE (&lt;node-name&gt;:&lt;label-name&gt;{ &lt;Property1-name&gt;:&lt;Pro&lt;rty1-Value&gt; ..... &lt;Propertyn-name&gt;:&lt;Propertyn-Value&gt;}) 与CREATE的区别： CREATE 在创建新节点时，不检查数据库中是否已存在相同节点；因此会创建重复数据。 NULL空属性用NULL填充 IN类似于 AND 操作： 123MATCH (e:Employee) WHERE e.id IN [123,124]RETURN e.id,e.name,e.sal,e.deptno 4. Neo4j 函数字符串函数 功能 描述 UPPER 所有字母改为大写 LOWER 所有字母转为小写 SUBSTRING 字串 REPLACE 替换 1UPPER (&lt;input-string&gt;) 1SUBSTRING(&lt;input-string&gt;,&lt;startIndex&gt; ,&lt;endIndex&gt;) 例如： 12MATCH (e:Employee) RETURN e.id,UPPER(e.name),LOWER(e.sal),e.deptno 12MATCH (e:Employee) RETURN e.id,SUBSTRING(e.name,0,2),e.sal,e.deptno AGGERGATION 聚合类似于SQL中的GROUP BY子句。 功能 描述 COUNT 返回行数 MAX 返回最大值 MIN 返回最小值 SUM 求和 AVG 平均 例如： 1MATCH (e:Employee) RETURN COUNT(*) 12MATCH (e:Employee) RETURN MAX(e.sal),MIN(e.sal) 12MATCH (e:Employee) RETURN SUM(e.sal),AVG(e.sal) 关系函数STARTNODE、ENDNODE 返回关系的开始节点 1STARTNODE (&lt;relationship-label-name&gt;) 例如： 12MATCH (a)-[movie:ACTION_MOVIES]-&gt;(b) RETURN STARTNODE(movie),ENDNODE(movie) 12MATCH (a)-[movie:ACTION_MOVIES]-&gt;(b) RETURN ID(movie),TYPE(movie) 关系的ID和类型（名称） 5. Neo4j Admin索引 Neo4j SQL支持节点或关系属性上的索引，以提高应用程序的性能。 我们可以为具有相同标签名称的所有节点的属性创建索引。 Create Index 创建索引 Drop Index 丢弃索引 创建索引： 1CREATE INDEX ON :&lt;label_name&gt; (&lt;property_name&gt;) 上述语法描述它在节点或关系的的上创建一个新索引。 删除索引： 1DROP INDEX ON :&lt;label_name&gt; (&lt;property_name&gt;) 例如： 12CREATE INDEX ON :Customer (name)DROP INDEX ON :Customer (name) UNIQUE 创建UNIQUE约束 丢弃UNIQUE约束 12CREATE CONSTRAINT ON (&lt;label_name&gt;)ASSERT &lt;property_name&gt; IS UNIQUE 12CREATE CONSTRAINT ON (cc:CreditCard)ASSERT cc.number IS UNIQUE 创建新的节点时，如果被UNIQUE约束的属性已存在，则会创建失败。 12DROP CONSTRAINT ON (&lt;label_name&gt;)ASSERT &lt;property_name&gt; IS UNIQUE 删除约束","link":"2021/09/01/Neo4j%E4%BD%BF%E7%94%A8/"},{"title":"PaddleOCR C++ CPU 推理部署","text":"1. 运行准备新建目录 ppocr-cpp-cpu，将最新分支 PaddleOCR/deploy/cpp_infer 下的所有源文件复制到该文件夹下。 123git clone https://github.com.cnpmjs.org/PaddlePaddle/PaddleOCR.gitmkdir ppocr-cpp-cpucp -r PaddleOCR/deploy/cpp_infer/* ppocr-cpp-cpu 将推理模型拷贝到 inference 文件夹下。 12mkdir ppocr-cpp-cpu/inferencecp -r inference ppocr-cpp-cpu/inference 其中，inference 包含推理所需的 det, rec, cls 模型，可以参考模型预测说明，例如文件结构如下： 12345678910inference/|-- det_db| |--inference.pdiparams| |--inference.pdmodel|-- rec_rcnn| |--inference.pdiparams| |--inference.pdmodel|-- cls| |--inference.pdiparams| |--inference.pdmodel 2. 编译 opencv 库首先，下载 opencv 源码到项目路径下。 123cd ppocr-cpp-cpuwget https://paddleocr.bj.bcebos.com/libs/opencv/opencv-3.4.7.tar.gztar -xvf opencv-3.4.7.tar.gz 修改 tools/build_opencv.sh： 12345678910111213141516171819202122232425262728root_path=/root/ppocr-cpp-cpu/opencv-3.4.7 # 下载的 opencv 源码路径，这里假设项目文件夹在 /root 下，需根据实际情况修改install_path=${root_path}/opencv3build_dir=${root_path}/buildrm -rf ${build_dir}mkdir ${build_dir}cd ${build_dir}cmake .. \\ -DCMAKE_INSTALL_PREFIX=${install_path} \\ -DCMAKE_BUILD_TYPE=Release \\ -DBUILD_SHARED_LIBS=OFF \\ -DWITH_IPP=OFF \\ -DBUILD_IPP_IW=OFF \\ -DWITH_LAPACK=OFF \\ -DWITH_EIGEN=OFF \\ -DCMAKE_INSTALL_LIBDIR=lib64 \\ -DWITH_ZLIB=ON \\ -DBUILD_ZLIB=ON \\ -DWITH_JPEG=ON \\ -DBUILD_JPEG=ON \\ -DWITH_PNG=ON \\ -DBUILD_PNG=ON \\ -DWITH_TIFF=ON \\ -DBUILD_TIFF=ONmake -j # 服务器性能差的话，去掉 -j 或者 改为 make -j 2make install 然后，编译 opencv（编译依赖 gcc g++ cmake）： 1sh tools/build_opencv.sh 编译完成后，在 opencv-3.4.7 路径下会生成 opencv3 文件夹，这取决于上面的安装路径 install_path，文件结构如下： 123456opencv3/|-- bin|-- include|-- lib|-- lib64|-- share 3. 编译 Paddle你也可以直接去官网选择和下载编译好的预测库，解压后安装路径为 paddle_inference ，然后跳过这一步。 如果后续编译 PaddleOCR 没有问题，那么 fine！如果出现问题一直不能解决，那么选择亲自编译 Paddle 预测库。 首先，下载 Paddle 最新源码。 123git clone https://github.com.cnpmjs.org/PaddlePaddle/Paddle.gitcd Paddlegit checkout release/2.2 在 Paddle 目录下新建 build.sh： 1vim build.sh 添加以下内容： 12345678910111213141516rm -rf buildmkdir buildcd buildcmake .. \\ -DWITH_CONTRIB=OFF \\ -DWITH_MKL=ON \\ -DWITH_MKLDNN=ON \\ -DWITH_TESTING=OFF \\ -DCMAKE_BUILD_TYPE=Release \\ -DWITH_INFERENCE_API_TEST=OFF \\ -DON_INFER=ON \\ -DWITH_PYTHON=OFF # 如果不需要用 python 的话，设置为 OFFmake -j # 同样，服务器性能不好的话，建议去掉 -j 或者 make -j 2make inference_lib_dist 然后，进行编译： 1sh build.sh 编译完成后，生成了build/paddle_inference_install_dir文件下，文件结构如下： 12345build/paddle_inference_install_dir/|-- CMakeCache.txt|-- paddle|-- third_party|-- version.txt 其中，paddle 就是 C++ 推理所需的 Paddle 库。如果成功到这里，那么已经完成 90% 了~ 4. 编译 PaddleOCR修改 tools/build.sh 中的环境路径。 12cd ../ # 进入到项目路径下vim tools/build.sh 修改内容如下： 这里真正用到的库有两个：opencv 编译安装目录和 paddle 编译安装目录。 12345# 以下路径均填写绝对路径OPENCV_DIR=/root/ppocr-cpp-cpu/opencv-3.4.7/opencv3 # opencv 编译安装路径LIB_DIR=/root/ppocr-cpp-cpu/Paddle/build/paddle_inference_install_dir # Paddle 编译安装路径，或者下载的编译好的 Paddle 预测库路径CUDA_LIB_DIR=/usr/local/cuda/lib64 # 如果仅使用 CPU 推理的话，这个路径不重要；如果使用 GPU 推理，则填写 CUDA lib 实际路径CUDNN_LIB_DIR=/usr/lib/x86_64-linux-gnu # 同上 然后，先修改 ppocr_keys_v1.txt 字典文件路径，假设放在项目目录下，则修改 src/main.cpp 的第 65 行： 1DEFINE_string(char_list_file, &quot;./ppocr_keys_v1.txt&quot;, &quot;Path of dictionary.&quot;); # 每次修改源代码都要重新编译 这里也可以不修改，不过需要在后面运行的时候传入 char_list_file 参数来指定字典文件路径。 接着，编译： 12sudo sh tools/build.sh # 安装位置需要 sudo 权限# 记得将目录的 owner 切换为自己 编译完成后，会在项目目录下生成 build 文件夹，其中包含 ppocr 的可执行文件。 恭喜你，可以运行推理和部署了！ 推理需要用到的文件： 1234build # 编译安装的程序inference # 推理模型ppocr_keys_v1.txt # 字典imgs # 需要预测的图片 5. 运行推理只调用检测： 123./build/ppocr det \\ --det_model_dir=inference/det_db \\ --image_dir=test.jpg 只调用识别： 123./build/ppocr rec \\ --rec_model_dir=inference/rec_crnn \\ --image_dir=test.jpg 整体调用： 12345678910111213# 不使用方向分类器./build/ppocr system \\ --det_model_dir=inference/det_db \\ --rec_model_dir=inference/rec_crnn \\ --image_dir=test.jpg# 使用方向分类器./build/ppocr system \\ --det_model_dir=inference/det_db \\ --use_angle_cls=true \\ --cls_model_dir=inference/cls \\ --rec_model_dir=inference/rec_crnn \\ --image_dir=test.jpg 更多参数： 参数名称 类型 默认参数 意义 use_gpu bool false 是否使用GPU gpu_id int 0 GPU id，使用GPU时有效 gpu_mem int 4000 申请的GPU内存 cpu_math_library_num_threads int 10 CPU预测时的线程数，在机器核数充足的情况下，该值越大，预测速度越快 use_mkldnn bool true 是否使用mkldnn库 det_model_dir string - 检测模型inference model地址 max_side_len int 960 输入图像长宽大于960时，等比例缩放图像，使得图像最长边为960 det_db_thresh float 0.3 用于过滤DB预测的二值化图像，设置为0.-0.3对结果影响不明显 det_db_box_thresh float 0.5 DB后处理过滤box的阈值，如果检测存在漏框情况，可酌情减小 det_db_unclip_ratio float 1.6 表示文本框的紧致程度，越小则文本框更靠近文本 use_polygon_score bool false 是否使用多边形框计算bbox score，false表示使用矩形框计算。矩形框计算速度更快，多边形框对弯曲文本区域计算更准确。 visualize bool true 是否对结果进行可视化，为1时，会在当前文件夹下保存文件名为ocr_vis.png的预测结果。 use_angle_cls bool false 是否使用方向分类器 cls_model_dir string - 方向分类器inference model地址 cls_thresh float 0.9 方向分类器的得分阈值 rec_model_dir string - 识别模型inference model地址 char_list_file string ../../ppocr/utils/ppocr_keys_v1.txt 字典文件 也可以通过软连接将可执行文件连接到用户命令： 1ln -s /root/ppocr-cpp-cpu/build/ppocr /usr/bin/ppocr 然后就可以直接调用命令执行了： 1ppocr system --image_dir test.jpg 6. 部署过程可能出现的问题GCC 版本编译要求支持 c++11，因此 gcc 和 g++ 版本至少为 4.8.5。 cmake 版本编译 Paddle 需要 cmake 版本至少为 3.19.2。 依赖 openssl： 1sudo yum install -y openssl openssl-devel 编译安装 cmake： 12345678910111213wget https://github.com/Kitware/CMake/releases/download/v3.22.1/cmake-3.22.1.tar.gztar -xvf cmake-3.22.1.tar.gzcd cmake-3.22.1.tar.gz./bootstrapmakesudo make install # 安装位置需要 sudo 权限# 此时，还要需要添加到环境变量vim ~/.bashrcexport CMAKE_ROOT=/usr/local/share/cmake-3.22 # 添加 cmake 安装路径source ~/.bashrc # 使配置生效 编译 Paddle 时 github 网络访问编译 Paddle 时需要在线下载编译所需 github 仓库，所以要求能够正常访问 github。 最大的问题是 github 访问慢甚至无法访问的问题，可以使用国内镜像 cnmpjs.org ，具体为，修改 CMakeLists.txt 183 行： 12# github.com --&gt; github.com.cnpmjs.orgset(GIT_URL &quot;https://github.com.cnpmjs.org&quot;) 同样，在 cmake/external 中，部分 cmake 文件需要修改 git 仓库地址为国内镜像地址，包括： 1brpc.cmake eigen.cmake gflags.cmake leveldb.cmake protobuf.cmake rocksdb.cmake spappy.cmake 以上修改了原始 git 仓库地址，但是有些仓库包含子模块，下载地址仍为 github.com，对于这些仓库，需要修改其 .submodule 中的地址为代理地址。 我想到的最笨的方法就是将这些包含子模块的仓库下载下来，修改 .submodule 文件，然后上传到自己创建的仓库，并将对应的 cmake 文件中的仓库地址改为对应的自己创建的仓库的地址（国内镜像的）。 eigen3 编译执行 eigen.cmake 的时候，可能会报 ./../../Eigen/Core: No such file or directory 的错，即找不到 Eigen/Core 路径，不知道是哪里的问题。于是尝试先手动安装 eigen3，然后修改 eigen3.cmake 中的 EIGEN_INCLUDE_DIR 为 eigen3 的安装路径。 12345678910111213# 下载和解压源文件wget https://gitlab.com/libeigen/eigen/-/archive/3.4.0/eigen-3.4.0.tar.gztar -zxvf eigen-3.4.0.tar.gz# 编译安装cd eigen-3.4.0.tar.gzmkdir build &amp;&amp; cd buildmakemake install # 可能需要 sudo 权限# eigen3 默认安装在了 /usr/local/include/eigen3# 修改 eigen3.cmake 第 40 行set(EIGEN_INCLUDE_DIR /usr/local/include/eigen3) # eigen3 安装路径 找不到 OPENCVConfig.cmakeCould not find a package configuration file provided by “OpenCV” with any of the following names: OpenCVConfig.cmake opencv-config.cmake 这是由于 CMakeLists.txt 中的 OpenCV 路径是针对 OpenCV 3.x 版本的，Opencv 4.x 版本的安装路径不同。 对于 OpenCV 4.x 版本，修改 CMakeLists.txt 中的 47 行为： 1find_package(OpenCV REQUIRED PATHS ${OPENCV_DIR}/lib64/cmake/opencv4 NO_DEFAULT_PATH)","link":"2022/01/11/PaddleOCR-C++%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/"},{"title":"PaddleOCR Docker 服务部署","text":"1. 简介本项目利用强悍的 PaddleOCR 工具库和 Docker 完成通用的 OCR 服务部署。OCR 推理功能采用基于 paddlepaddle 框架的 C++ 推理代码实现文本检测与文本识别，具体的 OCR 功能（通用 OCR 识别、身份证识别、文档识别等）使用 Python 实现。C++ 大大提高了推理速度，而 Python 提高了 OCR 功能的编写效率。最后用 FastAPI 封装接口并通过 Docker 部署，使用 streamlit 实现了简单的前端 demo。 项目代码：https://github.com/BruceHan98/ocr-docker 2. 项目结构1234567891011├── build│ ├── opencv3 # opencv 安装文件│ ├── paddle_inference # paddle 预测库├── ppocr # C++ 推理代码├── app.py # OCR 功能模块代码├── Dockerfile├── LICENSE.txt├── ocr-web.py # streamlit 前端 demo├── requirements.txt├── run_docker.sh├── server.py # FastAPI Paddle 预测库 使用 Paddle 官方 docker 减少了很多自己编译 paddle 预测库的麻烦，直接去官网下载对应的 paddle 预测库后解压。 OpenCV 库 编译 OpenCV 库（参考手动编译那篇）后的安装文件夹。 cpp infer C++ 推理的代码修改自 PaddleOCR/deploy/cpp_infer，放在 ppocr 文件夹下。 以上文件可通过链接下载。（文件较多，git 传输容易断） 3. 部署1.克隆项目代码 12git clone https://github.com/BruceHan98/ocr-dockercd ocr-docker 2.通过链接下载 build 与 ppocr 文件并解压到项目路径下。 3.制作 docker 镜像并运行 docker 容器 1bash run_docker.sh 可在 run_docker.sh 中修改 FastAPI 端口（默认为 9000）。 4.运行前端 demo 修改 ocr-web.py 中的 your_ip_address 为项目部署的 ip 地址，your_port_num 为FastAPI 端口号。 12pip install streamlit # 安装 streamlitstreamlit run ocr-web.py FastAPI 接口文档：http://your_ip_address:your_port_num/docs 前端 demo：http://localhost:8501 (注：localhost 为运行 streamlit 的主机地址） 效果展示：","link":"2022/03/10/PaddleOCR-Docker%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2/"},{"title":"XLNet","text":"0. 简介 XLNet is a new unsupervised language representation learning method based on a novel generalized permutation language modeling objective. Additionally, XLNet employs Transformer-XL as the backbone model, exhibiting excellent performance for language tasks involving long context. Model MNLI QNLI QQP RTE SST-2 MRPC CoLA STS-B BERT 86.6 92.3 91.3 70.4 93.2 88.0 60.6 90.0 XLNet 89.8 93.9 91.8 83.8 95.6 89.2 63.6 91.8 XLNet是BERT的升级模型，在以下三方面进行了优化： 采用AR模型替代AE模型，解决mask带来的负面影响 双流 (two-stream) 注意力机制 引入transformer-xl 1. AR v.s. AE AR (Auto-Regressive) Language Modeling 自回归语言模型，主要任务在于评估语料的概率分布，例如，给定一个序列X=(x1,…,xT)，AR模型就是在计算其极大似然估计p(X)，即已知xt之前的序列，预测xt的值。当然也可以反着来，即已知xt之后的序列，预测xt的值。(Unidirectional Transformer) 问题：单向建模，不能捕捉双向的语义信息。 AE (Auto-Encoding) (BERT) 对输入进行编码并解码，Masked Language Model采用了一个标志位[MASK]来随机替换一些词，再用[MASK]的上下文来预测[MASK]的真实值。(Bidirectional Transformer) 问题：1.独立假设 (假设每个MASK的词是独立的)，事实上MASK的词是有关的。 ​ 2.人工噪声：预训练和微调数据的不统一，从而引入了一些人为误差。 XLNet：对上面的问题进行了优化，AR + 无MASK噪声 + 双向上下文语义 2. Premutation Language Modeling​ 为了解决上文提到的问题，作者提出了排列语言模型，该模型不再对传统的AR模型的序列的值按顺序进行建模，而是最大化所有可能的序列的因式分解顺序的期望对数似然。 ​ 举例来说，对于一个序列[1,2,3,4]，如果我们的预测目标是3，传统的AR模型，结果是： p(3)= ∏_{t=1} ^ 3 p(3∣x _{","link":"2021/05/01/XLNet/"},{"title":"关系数据库","text":"1. 关系数据库关系数据库的结构关系数据库由表的集合构成，每个表有唯一的名字。表中一行代表了一组值之间的一种联系。 关系模型中，关系用来指代表，元组用来指代行，属性指代表中的列。关系实例表示表中一组特定的行。 关系的每个属性取值的集合叫作属性的域。如果域中的元素是不可再分的单元，则域是原子的。空值表示值未知或不存在。 数据库模式关系模式由属性序列及个属性对应域组成。 一个元组的属性值必须是能够唯一区分元组的，没有两个元组在所有属性上的取值都相同。 超码是一个或多个属性的集合，能够在一个关系中唯一标识一个元组。最小的超码称为候选码。用主码代表选中的候选码，习惯上把主码属性列在其他属性前面。 一个关系模式 r1 可能在它的属性中包括另一个关系模式 r2 的主码，该属性在 r1 上称作参照 r2 的外码。r1 称为外码依赖的参照关系，r2 称为外码的被参照关系。参照完整性约束要求在参照关系中任意元组在特定属性上的取值必然等于被参照关系中某个元组在特定属性上的取值。 模式图用来表示含有主码和外码依赖的数据库模式。 2. SQLSQL 是使用最为广泛的数据库查询语言。有以下几个部分： 数据定义语言（DDL）：DDL 提供定义关系模式、删除关系及修改关系模式的命令。 数据操纵语言（DML）：DML 提供从数据库中查询信息，以及插入、删除、修改元组的能力。 完整性：保存在数据库中的数据必须满足所定义的完整性约束。破坏完整性约束的更新是不允许的。 视图定义：DDL 包括定义视图的命令。 事务控制：SQL 包括定义事务的开始和结束的命令。 嵌入式 SQL 和动态 SQL 授权：DDL 包括定义对关系和视图的访问权限的命令。 SQL 数据定义DDL 不仅能定义一组关系，还能定义每个关系的信息，包括： 每个关系的模式 每个属性的取值类型 完整性约束 每个关系维护的索引集合 每个关系的安全性和权限 每个关系在磁盘上的物理存储结构 基本数据类型 char(n)：指定长度 n 的字符串 varchar(n)：可变长的字符串，最大长度 n int：整数 smallint：小整数 numeric(p, d)：定点数，p 位整数和 d 位小数 real，double precision：浮点数域双精度浮点数 float(n)：精度至少为 n 位的浮点数 char(n) 为定长字符串，varchar(n) 为变长字符串，建议使用 varchar。 基本模式定义 一个创建关系的例子： 12345create table department # 关系名 (dept_name varchar (20), # 属性名及域 building varchar (15), budget numeric (12, 2), primary key (dept_name)); # 完整性约束 完整性约束： primary key (A1, A2, …, Am)：关系的主码，主码属性必须非空且唯一。 foreign key (B1, B2, …, Bn) references s：关系中任意元组在属性 (B1, B2, …, Bn) 上的取值必须对应于关系 s 中某元组在主码属性上的取值。 not null：约束该属性不允许空值。 基本操作 插入 一个新创建的关系最初是空的，可以用 insert 命令将数据加载到关系中。例如 12insert into instructor values (10211, 'Smith', 'Biology', 66000); 删除 可以使用 delete 命令从关系中删除所有元组： 1delete from student; drop 命令可以直接去掉一个关系： 1drop table r; 更新 alter 命令为已有关系更新属性，关系中所有元组在新属性上的取值将被设为 null。 12alter table r add A D;alter table r drop A; 2.1 SQL 查询SQL 查询由三个子句构成：select、from 和 where。 单关系查询只在一个关系（表）中进行查询。例如 1select name from instructor; 使用 distinct 关键词对查询结果去重，默认是不去重的： 1select distinct dept_name from instructor; where 用于根据属性值的条件判断对查询结果进行筛选： 1select name from instructor where dept_name = 'Comp. Sci.' and salary &gt; 70000; where 允许使用逻辑连词 and、or 和 not，运算对象可以包含比较运算符 &lt;、&lt;=、&gt;、&gt;=、= 和 &lt;&gt;（不等于）、between … and …。 多关系查询结合多个关系（表）查询信息。例如 123select name, instructor.dept_name, buildingfrom instructor, departmentwhere instructor.dept_name = department.dept_name; 子句的运算顺序：首先是 from，然后是 where，最后是 select。 为 from 子句中列出的关系产生笛卡儿积 在步骤 1 的结果上应用 where 子句中指定的谓词 对步骤 2 结果中的每个元组，输出 select 子句中指定的属性（或表达式的结果） 笛卡儿积把第一个关系的每个元组与第二个关系的所有元组都进行连接。 上述例子的连接属于自然连接，自然连接只考虑那些在两个关系模式中都出现的属性上取值相同的元组对。 可以将自然连接写为 12select name, course_idfrom instructor natural join teachers; natural join 会对两个关系中所有共同出现的属性进行连接，有时候我们只想对指定的某个属性进行连接，需要使用 join … using 运算。 12select name, titlefrom (instructor natural join teachers) join course using (course_id); r1 join r2 using (A1, A2, …) 两个关系中必须都具有指定名称的属性。 附加的基本运算更名 有时候为了表达简单或避免属性名重复，需要使用 as 对选择的属性名进行更名运算。 12345678# 避免属性名重复select name as instructor_name, course_idfrom instructor, teacherswhere instructor.ID = teachers.ID;# 替换长名称select T.name, S.course_idfrom instructor as T, teachers as Swhere T.ID = S.ID; 字符串运算 SQL 使用一对单引号来表示字符串，例如 ‘Computer’。如果字符串包含单引号，就用两个单引号来表示其中的单引号。 SQL 支持一些字符串函数，如串联（使用“||”）、提取字串、计算字符串长度、大小写转换（upper(s)，lower(s)）、去掉字符串后面的空格（trim(s)）等，具体函数参阅具体数据库系统手册。 like 模式匹配 百分号（%）：匹配任意字串 下划线（_）：匹配任意一个字符 escape 关键词转移字符 123select dept_namefrom departmentwhere building like '% Watson%'; 属性说明 星号 “*” 在 select 子句中表示 “所有的属性”： 12select instructor.*from instructor natural join teachers 表示选中 instructor 中的所有属性。 排序 order by 子句使结果按照某个属性排序显示，默认为升序。可以使用 desc 表示降序，使用 asc 表示升序。另外，排序可以在多个属性上进行。 1select * from instructor order by salary desc, name asc; 2.2 集合运算SQL 作用在关系上的 union、intersect 和 except 运算对应于集合中的 $\\cup$、$\\cap$ 和 $-$。例如： 在2009年秋季学期开设的所有课程的集合： 123select course_idfrom sectionwhere semester = 'Fall' and year = 2009; 在2010年春季学期开设的所有课程的集合： 123select course_idfrom sectionwhere semester = 'Spring' and year = 2010; 以上两个查询结果分别记作 c1、c2。注意到 c2 包含两个 CS-319 的元组，该课程有两个时间段开课。 并运算 1234567(select course_idfrom sectionwhere semester = 'Fall' and year = 2009;)union(select course_idfrom sectionwhere semester = 'Spring' and year = 2010;) 与 select 子句不同，union 运算自动去除重复。 如果我们想保留所有重复，就必须用 union all 代替 union： 交运算 1234567(select course_idfrom sectionwhere semester = 'Fall' and year = 2009;)intersect(select course_idfrom sectionwhere semester = 'Spring' and year = 2010;) 同样，intersect 运算自动去除重复，如果想保留所有重复，用 intersect all 代替 intersect。 差运算 1234567(select course_idfrom sectionwhere semester = 'Fall' and year = 2009;)except(select course_idfrom sectionwhere semester = 'Spring' and year = 2010;) c1 except c2，找出在 c1 中出现但不在 c2 中出现的元组。 同样，except 运算自动去除重复，如果想保留所有重复，用 except all 代替 except。 2.3 空值如果算术表达式的任一输入为空，则该算术表达式（涉及诸如 +、-、* 或 /）结果为空。 SQL 将涉及空值的任何比较运算的结果视为 unknown（既不是谓词 is null，也不是 is not null），这创建了除 true 和 false 之外的第三个逻辑值。 对于 unknown 的布尔运算： and：true and unknown 的结果是 unknown，false and unknown 的结果是 false，unknown and unknown 的结果是 unknown。 or：true or unknown 的结果是 true，false or unknown 的结果是 unknown，unknown or unknown 的结果是 unknown。 not：not unknown 的结果是 unknown。 SQL 在谓词中使用特殊的关键词 null 测试空值。 1select name from instructor where salary is null; 如果元组在所有属性上的取值相等，那么它们就被当作相同元组，即使某些值为空。所以诸如 {(‘A’, null), (‘A’, null)} 这样的两个元组拷贝被认为是相同的。 2.4 聚集函数聚集函数是以值的一个集合（集或多重集）为输入、返回单个值的函数。SQL 提供了五个固有聚集函数： 平均值：avg 最小值：min 最大值：max 总和：sum 计数：count 其中，sum 和 avg 的输入必须是数字集。 基本聚集例如： 123select avg(salary) as avg_salaryfrom instructorwhere dept_name = 'Comp. Sci.'; 有些情况下在计算聚集函数前需先删掉重复元组。如果我们确实想删除重复元组，可在聚集表达式中使用关键词 distinc： 123select count (distinct ID)from teacherswhere semester = 'Spring' and year = 2010; 使用聚集函数 count 计算一个关系中元组的个数，在 SQL 中该函数的写法是count(*)： 1select count(*) from course; 分组聚集group by 子句中给出的一个或多个属性是用来构造分组的。在 group by 子句中的所有属性上取值相同的元组将被分在一个组中。 考虑查询“找出每个系的平均工资”： 123select dept_name, ave(salary) as avg_salaryfrom instructorgroup by dept_name; 分组结果为： 查询结果为： 如果不添加分组，则整个关系被当作是一个分组。 注意：当 SQL 查询使用分组时，一个很重要的事情是需要保证出现在 select 语句中但没有被聚集的属性只能是出现在 group by 子句中的那些属性。换句话说，任何没有出现在 group by 子句中的属性如果出现在 select 子句中的话，它只能出现在聚集函数内部，否则这样的查询就是错误的。 123456789# 错误查询select dept_name, ID, avg(salary)from instructorgroup by dept_name;# 正确查询select dept_name, ave(salary)from instructorgroup by dept_name; having 子句有时候，对分组限定条件比对元组限定条件更有用。例如，我们也许只对教师平均工资超过 42000 美元的系感兴趣。该条件并不针对单个元组，而是针对 group by 子句构成的分组。为表达这样的查询，我们使用 SQL 的 having 子句。having 子句中的谓词在形成分组后才起作用，因此可以使用聚集函数。 1234select dept_name, avg(salary) as avg_salaryfrom instructorgroup by dept_namehaving avg(salary) &gt; 42000; 任何出现在 having 子句中，但没有被聚集的属性必须出现在 group by 子句中，否则查询就被当成是错误的。 对空值和布尔值的聚集除了 count(*) 外所有的聚集函数都忽略输入集合中的空值。 2.5 嵌套子查询子查询是嵌套在另一个查询中的 select-from-where 表达式。子查询嵌套在 where 子句中，通常用于对集合的成员资格、集合的比较以及集合的基数进行检查。 集合成员资格SQL 允许测试元组在关系中的成员资格。连接词 in 测试元组是否是集合中的成员。 考虑查询“找出在2009年秋季和2010年春季学期同时开课的所有课程”。先前我们通过对两个集合进行交运算进行查询，现在使用 in 进行结果相同的嵌套查询。 123456select distinct course_idfrom sectionwhere semester = 'Fall' and year = 2009 and course_id in (select course_id from section where semester = 'Spring' and year = 2010); 这种写法更接近自然，但比较冗余。 集合的比较考虑查询“找出满足下面条件的所有教师的姓名，他们的工资至少比 Biology 系某一个教师的工资要高”。我们可以将查询写作： 123select distinct T.namefrom instructor as T, instructor as Swhere T.salary &gt; S.salary and S.dept_name = 'Biology'; 而“至少比某一个要大”在 SQL 中用 &gt; some 表示。因此上述问题的查询还可以写作： 12345select namefrom instructorwhere salary &gt; some (select salary from instructor where dept_name = 'Biology'); SQL 也允许 &lt; some, &lt;= some, &gt;= some, = some 和 &lt;&gt; some 的比较。 结构 &gt; all 对应于词组“比所有的都大”。 类似于 some, SQL 也允许 &lt; all, &lt;= all, &gt;= all, = all 和 &lt;&gt; all 的比较。 空关系测试SQL 还有一个特性可测试一个子查询的结果中是否存在元组。exists 结构在作为参数的子查询非空时返回 true 值。 查询“找出在2009年秋季学期和2010年春季学期同时开课的所有课程”: 123456select course_idfrom section as Swhere semester = 'Fall' and year = 2009 and exists (select * from section as T where semester = 'Spring' and year = 2010 and S.course_id = T.course_id); 重复元组存在性测试如果作为参数的子查询结果中没有重复的元组，unique 结构将返回 true值；我们可以用 unique 结构书写查询”找出所有在2009年最多开设一次的课程”： 12345select T.course_idfrom course as Twhere unique (select R.course_id from section as R where T.course_id = R.course_id and R.year = 2009); 可以用 not unique 结构测试在一个子查询结果中是否存在重复元组。 from 子句中的子查询任何 select-from-where 表达式返回的结果都是关系，因而可以被插入到另一个 select-from-where 中任何关系可以出现的位置。 考虑查询“找出系平均工资超过 42000 美元的那些系中教师的平均工资”。 12345select dept_name, avg_salaryfrom (select dept_name, avg(salary) as avg_salary from instructor group by dept_name)where avg_salary &gt; 42000; 在 from 子句嵌套的子查询中不能使用来自 from 子句其他关系的相关变量，用关键词 lateral 作为前缀，就可以在 from 子句中使用其他关系的属性，否则，子查询就不能访问来自外层查询的相关变量。 1234select name, salary, avg_salaryfrom instructor I1, lateral (select avg(salary) as avg_salary from instructor I2 where I2.dept_name = I1.dept_name); 目前只有少数SQL实现支持 lateral 子句，例如 IBM DB2。 with 子句with 子句提供定义临时关系的方法，这个定义只对包含 with 子句的查询有效。考虑找出具有最大预算值的系： 12345with max_budget (value) as (select max(budget) from department)select dept_namefrom department, max_budgetwhere department.budget = max_budget.value; 2.6 数据库的修改删除删除请求的表达与查询非常类似。我们只能删除整个元组，而不能只删除某些属性上的值。 12delete from r # 关系where P; # 谓词 注意 delete 命令只能作用于一个关系。 当 where 子句为空时，将删除关系中所有元组，但关系本身依然存在。 插入要往关系中插入数据，我们可以指定待插入的元组，或者写一条查询语句来生成待插入的元组集合。待插入元组的属性值必须在相应属性的域中。同样，待插入元组的分量数也必须是正确的。 12insert into course values ('CS-437', 'Database Systems', 'Comp. Sci.', 4); 考虑到用户可能不记得关系属性的排列顺序，SQL 允许在 insert 语句中指定属性。 12insert into course (course_id, title, dept_name, credits) values ('CS -437', 'Database Systems', ' Comp. Sci.', 4)； 我们可能想在查询结果的基础上插入元组： 1234insert into instructor select ID, name, dept_name, 18000 from student where dept_name = 'Music' and tot_cred &gt; 144; 更新有些情况下，我们可能希望在不改变整个元组的情况下改变其部分属性的值。为达到这一目的，可以使用 update 语句。与使用 insert、delete 类似，待更新的元组可以用查询语句找到。 123456update instructorset salary = salary * 1.05;update instructorset salary = salary * 1.05where salary &lt; 70000; update、insert 中的 where 子句同样可以包含任何合法结构（嵌套等）。 3. 中级 SQL3.1 连接表达式on 连接上面我们介绍了自然连接（natural join）运算，并介绍了 join…using 子句，可在指定属性上进行匹配。SQL 支持另一种更加灵活的连接，允许在参与连接的关系上设置通用的谓词。该谓词的写法与 where 子句类似，只不过使用关键词 on，与 using 条件一样，on 条件出现在连接表达式的末尾。 1select * from students join takes on student.ID = takes.ID; 实际上，上述查询与以下查询等价： 1select * from students, takes where students.ID = takes.ID; on 条件可以表示任何 SQL 谓词，从而使用 on 条件的连接表达式就可以表示比自然连接更为丰富的连接条件。然而，使用带 on 条件的连接表达式的查询可以用不带 on 条件的等价表达式来替换，只要把 on 子句中的谓词移到 where 子句中即可。 外连接在参与连接的任何一个或两个关系中的某些元组可能会以这种方式“丢失”，比如某一关系中参与连接的属性值为空。自然连接会丢弃这样的结果，有些时候这并不是我们想要的。 外连接运算通过在结果中创建包含空值元组的方式，保留了那些在连接中丢失的元组。实际上有三种形式的外连接： 左外连接（left outer join）：只保留出现在运算之前左边关系中的元组 右外连接（right outer join）：只保留出现在运算之后右边关系中的元组 全外连接（full outer join）：保留出现在两个关系中的元组。 为了与外连接运算相区分，我们此前学习的不保留未匹配元组的连接运算被称作内连接运算。 左外连接操作过程： 首先，像前面那样计算出内连接的结果；然后，对于在内连接的左侧关系中任意一个与右侧关系中任何元组都不匹配的元组 t，向连接结果中加入一个元组 r，r 的构造如下： 元组 r 从左侧关系得到的属性被赋为 t 中的值 r 的其他属性被赋为空值 查询“找出所有一门课程也没有选修的学生”： 123select *from student natural left outer join takeswhere course_id is null; 右外连接操作过程： 右外连接和左外连接是对称的。来自右侧关系中的不匹配左侧关系任何元组的元组被补上空值，并加入到右外连接的结果中。 如果我们使用右外连接来重写前面的查询，并交换列出关系的次序，得到的结果是一样的，只不过结果中属性出现的顺序不同。 12select *from takes natural right outer join student; 全外连接操作过程： 全外连接是左外连接与右外连接类型的组合。在内连接结果计算出来之后，左侧关系中不匹配右侧关系任何元组的元组被添上空值并加到结果中。类似地，右侧关系中不匹配左侧关系任何元组的元组也被添上空值并加到结果中。 3.2 视图上面的所有例子中，我们一直都在逻辑模型层操作.即我们假定了给定的集合中的关系都是实际存储在数据库中的。让所有用户都看到整个逻辑模型是不合适的。出于安全考虑，可能需要向用户隐藏特定的数据。 SQL 允许通过查询来定义“虚关系”，它在概念上包含查询的结果。虚关系并不预先计算并存储，而是在使用虚关系的时候才通过执行查询被计算出来。 任何像这种不是逻辑模型的一部分，但作为虚关系对用户可见的关系称为视图。 在 SQL 中用 create view 命令定义视图。为了定义视图，我们必须给视图一个名称，并且必须提供计算视图的查询。 1create view v as &lt;query expression &gt;; 其中 \\ 可以是任何合法的查询表达式，v 表示视图名。 例如，要访问 instructor 关系中除 salary 之外的所有职员的数据： 123create view faculty asselect ID, name, dept_namefrom instructor; 视图关系在概念上包含查询结果中的元组，但并不进行预计算和存储。当视图关系被访问时，其中的元组是通过计算查询结果而被创建出来的。 3.3 事务事务由查询和（或）更新语句的序列组成。SQL标准规定当一条SQL语句被执行，就隐式地开始了一个事务。下列SQL语句之一会结束一个事务： Commit work：提交当前事务，也就是将该事务所做的更新在数据库中持久保存。在事务被提交后，一个新的事务自动开始。 Rollback work：回滚当前事务，即撤销该事务中所有SQL语句对数据库的更新。这样，数据库就恢复到执行该事务第一条语句之前的状态。 事务提交就像对编辑文档的变化存盘，而回滚就像不保存变化退出编辑。一旦某事务执行了 commit work，它的影响就不能用 rollback work 来撤销了。数据库系统保证在发生诸如某条 SQL 语句错误、断电、系统崩溃这些故障的情况下，如果一个事务还没有完成 commit work，其影响将被回滚。 3.4 完整性约束完整性约束保证授权用户对数据库所做的修改不会破坏数据的一致性。 not null 约束 空值是所有域的成员，因此在默认情况下是SQL中每个属性的合法值。然而对于一些属性来说，空值可能是不合适的。在这些情况下，我们希望禁止空值，例如： 12name varcahr(20) not nullbudget numeric(12, 2) not null not null 声明禁止在该属性上插入空值。 unique 约束 unique 声明指出属性 $A_1, A_2, …, A_m$ 形成了一个候选码；即在关系中没有两个元组能在所有列出的属性上取值相同。然而候选码属性可以为 null，除非它们已被显式地声明为 not null。（空值不等于其他的任何值） 1unique (A1, A2, ...) check 子句 当应用于关系声明时，check(P) 子句指定一个谓词 P，关系中的每个元组都必须满足谓词 P。例如，check( budget &gt;0) 子句将保证 budget 上的取值是正数。 123456create table section (course_id varchar (8), sec_id varchar (8), semester varchar (8), primary key (course_id, sec_id), check (semester in ('Fall', 'Winter', 'Spring', 'Summer'))); 参照完整性 我们常常希望保证在一个关系中给定属性集上的取值也在另一关系的特定属性集的取值中出现，这种情况称为参照完整性。例如，外码可以用作为 SQL 中 create table 语句一部分的 foreign key 子句来声明。 默认情况下，SQL 中外码参照的是被参照表中的主码属性。SQL还支持一个可以显式指定被参照关系的属性列表的 references 子句。然而，这个指定的属性列表必须声明为被参照关系的候选码，要么使用 primary key 约束，要么使用 unique 约束。 由于有了与外码声明相关联的 on delete cascade 子句，如果删除 department 中的元组导致了此参照完整性约束被违反，则删除并不被系统拒绝，而是对 course 关系作“级联”删除，即删除参照了被删除系的元组。类似地，如果更新被参照字段时违反了约束，则更新操作并不被系统拒绝。 3.5 SQL 的数据类型与模式日期和时间 date：’2022-10-10’ time：’09:30:00’ timestamp：’2022-10-10 09:30:00.45’ 默认值SQL 允许为属性指定默认值，如下面的 create table 语句所示： 12345create table student (ID varchar (5), name varchar (20) not null, tot_cred numeric (3, 0) default 0, primary key (ID)); 创建索引在关系的属性上所创建的索引(index)是一种数据结构，它允许数据库系统高效地找到关系中那些在索引属性上取给定值的元组，而不用扫描关系中的所有元组。索引也可以建立在一个属性列表上，例如在 student 的属性 name 和 dept_name 上，从而不用扫描所有元组。 1create index studentID_index on student(ID); 用户定义的类型SQL 支持两种形式的用户定义数据类型。第一种称为独特类型，另一种称为结构化数据类型。 可以用 create type 子句来定义新类型。例如 12345create type Dollars as numeric (12, 2) final;create table department (dept_name varchar (20), budget Dollars); 创建表时的扩展应用常常要求创建与现有的某个表的模式相同的表。SQL提供了一个 create table like 的扩展来支持这项任务： 12345create table temp_instructor like instructor;create table tl as (select * from instructor where dept_name = 'Music')with data; 如果省略 with data 子句，表会被创建，但不会载入数据。 3.6 授权我们可能会给一个用户在数据库的某些部分授予几种形式的权限。对数据的授权包括： 授权读取数据 授权插入新数据 授权更新数据 授权删除数据 每种类型的授权都称为一个权限（privilege）。当用户提交查询或更新时，SQL执行先基于该用户曾获得过的权限检查此查询或更新是否是授权过的。如果查询或更新没有经过授权，那么将被拒绝执行。 权限的授予与收回SQL 标准包括 select, insert, update 和 delete权限。权限所有权限 (all privileges) 可以用作所有允许权限的简写形式。 SQL 数据定义语言包括授予和收回权限的命令。grant 语句用来授予权限： 12grant select on department to Amit, Satoshi;grant update (budget) on department to Amit, Satoshi; 使用 revoke 语句来收回权限。此语句的形式与 grant 几乎是一样的： 12revoke select on department from Amit, Satoshi;revoke update (budget) on department to Amit, Satoshi; 角色在数据库中建立一个角色集.可以给角色授予权限，就和给每个用户授权的方式完全一样。在SQL中创建角色如下所示： 1create role instructor; 然后角色就可以像用户那样被授予权限： 1grant select on takes to instructor; 4. MySQL","link":"2022/10/20/SQL-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"ffmpeg 常用命令","text":"精确裁剪视频1ffmpeg -ss 10 -t 15 -accurate_seek -i test.mp4 -codec copy -avoid_negative_ts 1 cut.mp4 -ss: 起始位置，单位：秒 -t: 持续时间，单位：秒 合并音视频1ffmpeg -i video.mp4 -i audio.wav -c:v copy -c:a aac -strict experimental output.mp4 如果视频中已经包含了音频，这个时候还可以替换视频中的音频，使用下面命令行。 12ffmpeg -i video.mp4 -i audio.wav -c:v copy -c:a aac -strict experimental -map 0:v:0 -map 1:a:0 output.mp4 拼接音频1ffmpeg -i input1.mp3 -i input2.mp3 -filter_complex amerge -ac 2 -c:a libmp3lame -q:a 4 output.mp3 提取视频中的音频1ffmpeg -i input.mp4 -vn -y -acodec copy output.m4a 压缩视频123456789ffmpeg -i input.mp4 -r 20 output.mp4ffmpeg -i input.mp4 -fs 20MB output.mp4ffmpeg -i input.mp4 -b:v 1.5M output.mp4ffmpeg -i input.mp4 -s 960x540 output.mp4-r: 改变帧率-fs: 指定大小-b:v 改变码率为1.5M/s-s: 改变分辨率为960x540 视频类型转换12# 快速转换ffmpeg -i input.flv -codec copy output.mp4 视频、音频倍速1234567# 视频滤波器通过改变每一个 pts 时间戳来实现，pts 时间长则播放慢# -r 指定帧数# 0.25 代表四倍速ffmpeg -i input.mkv -r 120 -filter:v &quot;setpts=0.25*PTS&quot; output.mkv# 2.0 代表 0.5 倍速ffmpeg -i input.mkv -filter:v &quot;setpts=2.0*PTS&quot; output.mkv 12# 通过 av filter 中的 atempo 来实现音频的倍速播放ffmpeg -i input.mkv -filter:&quot;atempo=2.0&quot; -vn output.mkv 12# 同时对视频和音频倍速播放ffmpeg -i input.mkv -filter_complex &quot;[0:v]setpts=0.5*PTS[v];[0:a]atempo=2.0[a]&quot; -map &quot;[v]&quot; -map &quot;[a]&quot; output.mkv","link":"2021/12/14/ffmpeg%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"title":"人工智能会议","text":"第一梯队IJCAIAI最好的综合性会议, 1969年开始, 每两年开一次, 奇数年开. 每届基本上能录100多篇（现在已经到200多篇了），但分到每个领域就没几篇了。 AAAI美国人工智能学会AAAI的年会.比IJCAI还是要稍弱一点 NIPS神经计算方面最好的会议之一, NIPS主办, 每年举行.NIPS里有相当一部分神经科学的内容, 和机器学习有一定的距离. 但由于会议的主体内容是机器学习, 或者说与机器学习关系紧密, 所以 不少人把NIPS看成是机器学习方面最好的会议之一.对Jordan系以外的人来说, 发NIPS的难度比ICML更大. 换句话说, ICML比较开放, 小圈子的影响不象NIPS那么大, 所以北美和欧洲人都认, 而NIPS则有些人(特别是一些欧洲人, 包括一些大家)坚决不投稿.Michael Jordan是伯克利大学教授，统计机器学习的老大，大牛中的巨牛 ICLRInternational Conference on Learning Representations,深度学习领域最重要的会议之一，尽管才第五届，已经有很多非常重要的文章，比如VGG Net,attention等 CVPR计算机视觉和模式识别方面最好的会议之一, IEEE主办, 每年举行. 虽然题目上有计算机视觉, 但个人认为它的模式识别味道更重一些. 事实上它应该是模式识别最好的会议, 而在计算机视觉方面, 还有ICCV与之相当. ICCV计算机视觉方面最好的会之一. IEEE主办, 每年举行. ICML机器学习方面最好的会议之一. 现在是IMLS主办, 每年举行. ACL计算语言学/自然语言处理方面最好的会议, ACL (Association of Computational Linguistics) 主办, 每年开. KR知识表示和推理方面最好的会议之一, 实际上也是传统AI(即基于逻辑的AI) 最好的会议之一. KR Inc.主办, 现在是偶数年开. SIGIR信息检索方面最好的会议, ACM主办, 每年开. 这个会现在小圈子气越来越重. 信息检索应该不算AI, 不过因为这里面用到机器学习越来越多, 最近几年甚至有点机器学习应用会议的味道了 SIGKDD数据挖掘方面最好的会议, ACM主办, 每年开. COLT计算学习理论最好的会议, ACM主办, 每年举行. 计算学习理论基本上可以看成理论计算机科学和机器学习的交叉。“一小群数学家在开会” 第二梯队AAMAS (2+)agent方面最好的会议. 但是现在agent已经是一个一般性的概念, 几乎所有AI有关的会议上都有这方面的内容, 所以AAMAS下降的趋势非常明显. ECCV (2+)计算机视觉方面仅次于ICCV的会议, 因为这个领域发展很快, 有可能升级到1-去. ECML (2+)机器学习方面仅次于ICML的会议, 欧洲人极力捧场,因为机器学习发展很快, 这个会议的reputation上升非常明显. ICDM (2+)数据挖掘方面仅次于SIGKDD的会议, 目前和SDM相当. 这个会只有5年历史, 上升速度之快非常惊人. 几年前ICDM还比不上PAKDD, 现在已经拉开很大距离了. SDM (2+)数据挖掘方面仅次于SIGKDD的会议, 目前和ICDM相当. SIAM的底子很厚, 但在CS里面的影响比ACM和IEEE还是要小, SDM眼看着要被ICDM超过了, 但至少目前还是相当的. COLLING (2)计算语言学/自然语言处理方面仅次于ACL的会, 但与ACL的差距比ICCV-ECCV和ICML-ECML大得多. ECAI (2)欧洲的人工智能综合型会议, 历史很久, 但因为有IJCAI/AAAI压着,很难往上升. EMNLP (2-)计算语言学/自然语言处理方面一个不错的会. 有些人认为与COLLING相当, 但我觉得它还是要弱一点. 第三梯队ACCV (3+)亚洲的计算机视觉会议, 在亚太级别的会议里算很好的了. ICIP (3)图像处理方面最著名的会议之一, 盛会型. ICPR (3)模式识别方面最著名的会议之一, 盛会型. IJNLP (3)计算语言学/自然语言处理方面比较著名的一个会议. IJCNN (3)神经网络方面最重要的会议, 盛会型, 参见CEC的介绍. CCF 推荐国际学术会议/期刊目录","link":"2021/12/14/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E9%A2%86%E5%9F%9F%E4%BC%9A%E8%AE%AE/"},{"title":"hexo-icarus 从零搭建个人博客","text":"hexo-icarus 使用详解0. 安装Requirements Node.js ($\\geq$ 10.13) Git Install hexo 1npm install -g hexo 安装之后便可以使用命令： 1npx hexo &lt;command&gt; Install icarus 在 Hexo 网站目录下，安装 icarus 主题： 1npm install -S hexo-theme-icarus 推荐下载 icarus 仓库到 theme 目录下，方便后续调整 icarus 配置： 1git clone https://github.com/ppoffice/hexo-theme-icarus.git theme/icarus 在 hexo 的默认配置文件 _config.yml 中开启 icarus 主题： 1theme: icarus 1. 初始化初始化 Hexo 目录： 123npx hexo init &lt;folder&gt;cd &lt;folder&gt;npm install 初始化之后的目录： 12345678 .├── _config.yml # 配置文件├── package.json # 应用依赖的版本├── scaffolds # 脚手架模板├── source # 网站内容 | ├── _drafts # Hexo 忽略'_'开头的隐藏文件（夹），除了&quot;_posts&quot; | └── _posts└── themes # 主题 2. 基础配置_config.yml 为全局配置文件，主题下的 _config.yml 具有最低的优先级。 Site title 网站标题 subtitle 副标题 description 网站描述 keywords 关键词，支持多个 author 作者 language 语言，e.g.，en timezone 时区，默认为当前计算机时区 Url url 网址，必须以 ‘http://’ 或 ‘https://’ 开头 root 网站的根目录 permalink 文章的永久链接形式，:year/:month/:day/:title/ pretty_urls.trailing_index 文章链接跟随 ‘index.html’，设为 false 以删除 pretty_urls.trailing_html 文章链接跟随 ‘.html’，设为 false 以删除 如果网站是一个子目录，例如 ‘http://example.com/blog‘，那么把 url 设为 ’http://example.com/blog‘，把 ‘root’ 设为 ’/blog/‘。 Directory source_dir 存放内容的目录 public_dir 生成的静态文件的目录 tag_dir 标签目录 archive_dir 归档目录 category_dir 分类目录 code_dir 包含的代码的目录 i18n_dir 国际化语言目录 skip_render 跳过渲染，直接拷贝到 ‘public_dir’ 的路径 例如，skip_render: “mypage/*/\\”，跳过 ‘source/mypage/’ 下的所有文件。 skip_render: ”_posts/test-post.md”，跳过 ‘source/_posts/test_post.md’ 这篇文章。 Writing new_post_name 新的文章的文件名，:title.md default_layout 默认布局，post titlecase 将标题转为小写 external_link.enable 使用新标签打开外部链接 external_link.field 用于整个网站（site）还是仅该帖子（post） external_link.exclude 排除的主机名 filename_case 文件名大小写，0: 不转些, 1: 转小写, 2: 转大写 render_drafts 渲染草稿 post_asset_folder 启用 Assert 目录 relative_link 使用相对路径 future 显示未来的帖子 highlight 代码语法高亮设置 prismjs 同上 Extensions theme 使用主题 theme_config 主题配置，覆盖默认主题 deploy 部署设置 meta_generator 更多信息：Configuration 3. icarus 主题配置基础配置和常见问题：hexo-theme-icarus 有问题先翻一翻官方说明和 issues，很多都已解决！ 其他优化配置： 设置图片居中：Hexo博客主题之Icarus的设置与美化（进阶） Latex 公式问题：Hexo折腾系列（六）数学公式渲染优化 夜间模式：Hexo主题Icarus的自定义 Mathjax 自动换行：Hexo主题Icarus的自定义 4. 命令初始化网站初始化一个网站 1hexo init [folder] 新建帖子1hexo new [layout] &lt;title&gt; layout 默认为 _config.yml 中的 default_layout。 title 是不可少的，如果 title 中有空格，请将 title 用引号引起来。 选项： -p, —path 自定义帖子路径 -r, —replace 如果帖子存在，则替换 -s, —slug 自定义帖子的 url 默认情况下，hexo 将使用 post 的 title 来定义文件的路径。使用 —path 来覆盖此行为，例如： 1hexo new --path about/me &quot;About me&quot; 该命令会在 source/_posts/about/ 路径下生成 me.md 帖子，帖子标题为 “About me”。 生成静态文件12hexo generatehexo g # 简写 选项： -d, —deploy 生成文件结束后部署 -w, —watch 观察文件变化 -b, —bail 如果生成文件出错，则抛出错误 -f, —force 强制重新生成文件 -c, —concurrency 并行生成文件数，默认最大 发布草稿1hexo publish [layout] &lt;filename&gt; 启动服务12hexo serverhexo s # 简写 启动本地服务，默认网址为 http://localhost:4000/ 。 选项： -p, —port 端口号 -s, —static 只使用静态文件 -l, —log 开启日志记录 部署文件12hexo deployhexo d # 简写 选项： -g, —generate 部署浅先生成静态文件 清理缓存1hexo clean 清理缓存文件（db.json）和生成的文件（public）。 其他命令12345678hexo list &lt;type&gt; # 列出所有路径hexo version # 查看版本号hexo --safe # 禁止加载插件和脚本hexo --debug # 将详细信息记录到终端和 debug.loghexo --silent # 终端忽略输出hexo --drafts # 发布草稿hexo --config config.yml # 设置配置路径hexo --cwd /path/to/cwd # 自定义当前工作目录 5. 开始写作修改初始化头部信息 头部信息块是一个 YAML 或者 json 代码块，包含以下信息： layout Layout，config.default_layout title date updated comments 是否允许评论 tags 标签 categories 分类 toc 是否使用 toc，注意，如果想使用 toc 的话这个一定要设为 true cover 封面图路径 thumbnail 缩略图路径 excerpt 简介 注意：在生成新帖子时，会按照 scaffolds 目录中的模板生成头部信息，因此最好先定义好模板中的头部信息。 创建帖子 1hexo new [layout] &lt;title&gt; 有三种 layout： post source/_posts（默认） page source draft source/drafts","link":"2021/12/13/hexo-icarus%20%E4%BB%8E%E9%9B%B6%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E5%8D%9A%E5%AE%A2/"},{"title":"初识 BERT","text":"0. NLP 发展史 2001 - Neural language models（神经语言模型） 2008 - Multi-task learning（多任务学习） 2013 - Word embeddings（词嵌入） 2013 - Neural networks for NLP（NLP神经网络） 2014 - Sequence-to-sequence models 2015 - Attention（注意力机制） 2015 - Memory-based networks（基于记忆的网络） 2018 - Pretrained language models（预训练语言模型） 1. NLP 任务分类 SRL (语义角色标注) Coref (指代消歧) SNLI (自然语言推理) SQuAD (阅读理解，问答系统) SST-S (情感分析) NER (命名实体识别) 2. Seq2Seq 在encode阶段，第一个节点输入一个词，之后的节点输入的是下一个词与前一个节点的hidden state，最终encoder会输出一个context，这个context又作为decoder的输入，每经过一个decoder的节点就输出一个翻译后的词，并把decoder的hidden state作为下一层的输入。该模型对于短文本的翻译来说效果很好，但是其也存在一定的缺点，如果文本稍长一些，就很容易丢失文本的一些信息，为了解决这个问题，Attention应运而生。 Attention Attention与传统的Seq2Seq模型主要有以下两点不同。 1）encoder提供了更多的数据给到decoder，encoder会把所有的节点的hidden state提供给decoder，而不仅仅只是encoder最后一个节点的hidden state。2）decoder并不是直接把所有encoder提供的hidden state作为输入，而是采取一种选择机制，把最符合当前位置的hidden state选出来，具体的步骤如下 确定哪一个hidden state与当前节点关系最为密切 计算每一个hidden state的分数值（具体怎么计算我们下文讲解） 对每个分数值做一个softmax的计算，这能让相关性高的hidden state的分数值更大，相关性低的hidden state的分数值更低 Attention模型并不只是盲目地将输出的第一个单词与输入的第一个词对齐。实际上，它在训练阶段学习了如何在该语言对中对齐单词(示例中是法语和英语)。Attention函数的本质可以被描述为一个查询（query）到一系列（键key-值value）对的映射。 3. Transformer《Attention Is All You Need》是一篇Google提出的将Attention思想发挥到极致的论文。这篇论文中提出一个全新的模型，叫 Transformer，抛弃了以往深度学习任务里面使用到的 CNN 和 RNN ，Bert就是基于Transformer构建的，这个模型广泛应用于NLP领域，例如机器翻译，问答系统，文本摘要和语音识别等等方向。关于Transrofmer模型的理解特别推荐一位国外博主文章《The Illustrated Transformer》。 总体结构 和Attention模型一样，Transformer模型中也采用了 encoer-decoder 架构。但其结构相比于Attention更加复杂，论文中encoder层由6个encoder堆叠在一起，decoder层也一样。 每一个encoder和decoder的内部简版结构如下图： Encoder对于encoder，包含两层，一个self-attention层和一个前馈神经网络，self-attention能帮助当前节点不仅仅只关注当前的词，从而能获取到上下文的语义。decoder也包含encoder提到的两层网络，但是在这两层中间还有一层attention层，帮助当前节点获取到当前需要关注的重点内容。 首先，模型需要对输入的数据进行一个embedding操作，（也可以理解为类似w2c的操作），enmbedding结束之后，输入到encoder层，self-attention处理完数据后把数据送给前馈神经网络，前馈神经网络的计算可以并行，得到的输出会输入到下一个encoder。 Self-Attention 1、首先，self-attention会计算出三个新的向量，在论文中，向量的维度是512维，我们把这三个向量分别称为Query、Key、Value，这三个向量是用embedding向量与一个矩阵相乘得到的结果，这个矩阵是随机初始化的，维度为（64，512）注意第二个维度需要和embedding的维度一样，其值在BP的过程中会一直进行更新，得到的这三个向量的维度是64低于embedding维度的。 ​ $q = x⋅W^q$ ​ $k = x⋅W^k$ ​ $v = x⋅W^v$ 2、计算self-attention的分数值，该分数值决定了当我们在某个位置encode一个词时，对输入句子的其他部分的关注程度。这个分数值的计算方法是Query与Key做点乘，以下图为例，首先我们需要针对Thinking这个词，计算出其他词对于该词的一个分数值，首先是针对于自己本身即q1·k1，然后是针对于第二个词即q1·k2. 3、接下来，把点成的结果除以一个常数，这里我们除以8，这个值一般是采用上文提到的矩阵的第一个维度的开方即64的开方8，当然也可以选择其他的值，然后把得到的结果做一个softmax的计算。得到的结果即是每个词对于当前位置的词的相关性大小，当然，当前位置的词相关性肯定会会很大. 4、下一步就是把Value和softmax得到的值进行相乘，并相加，得到的结果即是self-attetion在当前节点的值。 这种通过 query 和 key 的相似性程度来确定 value 的权重分布的方法被称为scaled dot-product attention。其实scaled dot-Product attention就是我们常用的使用点积进行相似度计算的attention，只是多除了一个（为K的维度）起到调节作用，使得内积不至于太大。 对于使用自注意力机制的原因，论文中提到主要从三个方面考虑（每一层的复杂度，是否可以并行，长距离依赖学习），并给出了和RNN，CNN计算复杂度的比较。可以看到，如果输入序列n小于表示维度d的话，每一层的时间复杂度self-attention是比较有优势的。当n比较大时，作者也给出了一种解决方案self-attention（restricted）即每个词不是和所有词计算attention，而是只与限制的r个词去计算attention。在并行方面，多头attention和CNN一样不依赖于前一时刻的计算，可以很好的并行，优于RNN。在长距离依赖上，由于self-attention是每个词和所有词都要计算attention，所以不管他们中间有多长距离，最大的路径长度也都只是1。可以捕获长距离依赖关系。 Positional Encoding 到目前为止，transformer模型中还缺少一种解释输入序列中单词顺序的方法。为了处理这个问题，transformer给encoder层和decoder层的输入添加了一个额外的向量Positional Encoding，维度和embedding的维度一样，这个向量采用了一种很独特的方法来让模型学习到这个值，这个向量能决定当前词的位置，或者说在一个句子中不同的词之间的距离。这个位置向量的具体计算方法有很多种，论文中的计算方法如下其中pos是指当前词在句子中的位置，i是指向量中每个值的index，可以看出，在偶数位置，使用正弦编码，在奇数位置，使用余弦编码。最后把这个Positional Encoding与embedding的值相加，作为输入送到下一层。为了让模型捕捉到单词的顺序信息，我们添加位置编码向量信息（POSITIONAL ENCODING），位置编码向量不需要训练，它有一个规则的产生方式（上图公式）。 如果我们的嵌入维度为4，那么实际上的位置编码就如下图所示： Layer Normalization 在transformer中，每一个子层（self-attetion，ffnn）之后都会接一个残差模块，并且有一个Layer normalization. Normalization有很多种，但是它们都有一个共同的目的，那就是把输入转化成均值为0方差为1的数据。我们在把数据送入激活函数之前进行normalization（归一化），因为我们不希望输入数据落在激活函数的饱和区。 Batch Normalization 的主要思想就是：在每一层的每一批数据上进行归一化。我们可能会对输入数据进行归一化，但是经过该网络层的作用后，我们的数据已经不再是归一化的了。随着这种情况的发展，数据的偏差越来越大，我的反向传播需要考虑到这些大的偏差，这就迫使我们只能使用较小的学习率来防止梯度消失或者梯度爆炸。 BN的具体做法就是对每一小批数据，在批这个方向上做归一化。如下图所示：可以看到，右半边求均值是沿着数据 batch_size的方向进行的，其计算公式如下： 那么Layer normalization 呢？它也是归一化数据的一种方式，不过 LN 是在每一个样本上计算均值和方差，而不是BN那种在批方向计算均值和方差！ 下面看一下 LN 的公式： 在self-attention需要强调的最后一点是其采用了残差网络中的short-cut结构，目的是解决深度学习中的退化问题。 Decoder 可以看到decoder部分其实和encoder部分大同小异，不过在最下面额外多了一个masked mutil-head attetion，这里的mask也是transformer一个很关键的技术，我们一起来看一下。 Mask mask 表示掩码，它对某些值进行掩盖，使其在参数更新时不产生效果。Transformer 模型里面涉及两种 mask，分别是 padding mask 和 sequence mask。其中，padding mask 在所有的 scaled dot-product attention 里面都需要用到，而 sequence mask 只有在 decoder 的 self-attention 里面用到。 Padding mask 什么是 padding mask 呢？因为每个批次输入序列长度是不一样的也就是说，我们要对输入序列进行对齐。具体来说，就是给在较短的序列后面填充 0。但是如果输入的序列太长，则是截取左边的内容，把多余的直接舍弃。因为这些填充的位置，其实是没什么意义的，所以我们的attention机制不应该把注意力放在这些位置上，所以我们需要进行一些处理。 具体的做法是，把这些位置的值加上一个非常大的负数(负无穷)，这样的话，经过 softmax，这些位置的概率就会接近0！ 而我们的 padding mask 实际上是一个张量，每个值都是一个Boolean，值为 false 的地方就是我们要进行处理的地方。 Sequence mask sequence mask 是为了使得 decoder 不能看见未来的信息。也就是对于一个序列，在 time_step 为 t 的时刻，我们的解码输出应该只能依赖于 t 时刻之前的输出，而不能依赖 t 之后的输出。因此我们需要想一个办法，把 t 之后的信息给隐藏起来。 那么具体怎么做呢？也很简单：产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。 对于 decoder 的 self-attention，里面使用到的 scaled dot-product attention，同时需要padding mask 和 sequence mask 作为 attn_mask，具体实现就是两个mask相加作为attn_mask。 其他情况，attn_mask 一律等于 padding mask。 编码器通过处理输入序列启动。然后将顶部编码器的输出转换为一组注意向量k和v。每个解码器将在其“encoder-decoder attention”层中使用这些注意向量，这有助于解码器将注意力集中在输入序列中的适当位置： 完成编码阶段后，我们开始解码阶段。解码阶段的每个步骤从输出序列（本例中为英语翻译句）输出一个元素。以下步骤重复此过程，一直到达到表示解码器已完成输出的符号。每一步的输出在下一个时间步被送入底部解码器，解码器像就像我们对编码器输入所做操作那样，我们将位置编码嵌入并添加到这些解码器输入中，以表示每个字的位置。 输出层当decoder层全部执行完毕后，怎么把得到的向量映射为我们需要的词呢，很简单，只需要在结尾再添加一个全连接层和softmax层，假如我们的词典是1w个词，那最终softmax会输入1w个词的概率，概率值最大的对应的词就是我们最终的结果。 4. BERT模型结构 L表示的是transformer的层数，H表示输出的维度，A表示mutil-head attention的个数，uncased和cased的区别在于uncased将全部样本变为小写，而cased则要区分大小写。 预训练模型 首先我们要了解一下什么是预训练模型，举个例子，假设我们有大量的维基百科数据，那么我们可以用这部分巨大的数据来训练一个泛化能力很强的模型，当我们需要在特定场景使用时，例如做文本相似度计算，那么，只需要简单的修改一些输出层，再用我们自己的数据进行一个增量训练，对权重进行一个轻微的调整。 预训练的好处在于在特定场景使用时不需要用大量的语料来进行训练，节约时间效率高效，bert就是这样的一个泛化能力较强的预训练模型。 BERT的预训练阶段包括两个任务，一个是Masked Language Model，还有一个是Next Sentence Prediction。 Masked Language Model MLM可以理解为完形填空，作者会随机mask每一个句子中15%的词，用其上下文来做预测，例如：my dog is hairy → my dog is [MASK] 此处将hairy进行了mask处理，然后采用非监督学习的方法预测mask位置的词是什么，但是该方法有一个问题，因为是mask15%的词，其数量已经很高了，这样就会导致某些词在fine-tuning阶段从未见过，为了解决这个问题，作者做了如下的处理： 80%的时间是采用[mask]，my dog is hairy → my dog is [MASK] 10%的时间是随机取一个词来代替mask的词，my dog is hairy -&gt; my dog is apple 10%的时间保持不变，my dog is hairy -&gt; my dog is hairy Next Sentence Prediction 选择一些句子对A与B，其中50%的数据B是A的下一条句子，剩余50%的数据B是语料库中随机选择的，学习其中的相关性，添加这样的预训练的目的是目前很多NLP的任务比如QA和NLI都需要理解两个句子之间的关系，从而能让预训练的模型更好的适应这样的任务。 输入 BERT的输入词向量是三个向量之和： Token Embedding：WordPiece tokenization subword词向量。Segment Embedding：表明这个词属于哪个句子（NSP需要两个句子）。Position Embedding：学习出来的embedding向量。这与Transformer不同，Transformer中是预先设定好的值。 输出 BERT预训练模型的输出结果，无非就是一个或多个向量。下游任务可以通过精调（改变预训练模型参数）或者特征抽取（不改变预训练模型参数，只是把预训练模型的输出作为特征输入到下游任务）两种方式进行使用。BERT原论文使用了精调方式，但也尝试了特征抽取方式的效果，比如在NER任务上，最好的特征抽取方式只比精调差一点点。但特征抽取方式的好处可以预先计算好所需的向量，存下来就可重复使用，极大提升下游任务模型训练的速度。 总结BERT优点 Transformer Encoder因为有Self-attention机制，因此BERT自带双向功能 因为双向功能以及多层Self-attention机制的影响，使得BERT必须使用Cloze版的语言模型Masked-LM来完成token级别的预训练 为了获取比词更高级别的句子级别的语义表征，BERT加入了Next Sentence Prediction来和Masked-LM一起做联合训练 为了适配多任务下的迁移学习，BERT设计了更通用的输入层和输出层 微调成本小 BERT缺点 task1的随机遮挡策略略显粗犷，推荐阅读《Data Nosing As Smoothing In Neural Network Language Models》 [MASK]标记在实际预测中不会出现，训练时用过多[MASK]影响模型表现; 每个batch只有15%的token被预测，所以BERT收敛得比left-to-right模型要慢（它们会预测每个token） BERT对硬件资源的消耗巨大（大模型需要16个tpu，历时四天；更大的模型需要64个tpu，历时四天。","link":"2021/04/11/%E5%88%9D%E8%AF%86BERT/"},{"title":"对话系统综述","text":"1. 引言​ ”机器能否思考？”是一个历史悠久的问题，是二元并存论与唯物论思想之间的区别。唯物主义者笛卡尔认为机器不能像人那样作出适当的反应，并借此区分机器和人。狄德罗也曾说过，“如果他们发现一只鹦鹉可以回答一切问题，我会毫不犹豫宣布它存在智能。” 1950年，密码学和人工智能先驱图灵发表了一篇关于计算机器与智能的论文，提出了著名的 “图灵测试”：如果一台机器能够与人类展开对话（通过电传设备）而不被辨别出其机器身份，那么称这台机器具有智能。令人信服地说明“思考的机器”是可能的。 ​ 图灵测试作为当时衡量人工智能 “智能性” 的最常用标准，指导了后世的计算机与人工智能的发展与技术创新。如今，人工智能的快速发展使得机器在某些领域（如图像识别、语音识别等领域）通过了图灵测试甚至超越了人类的水平，图灵测试虽然不能真正回答 “机器能否思考” 这一问题，但它一直是对话系统的重要评测手段和目标。 2. 发展​ 图灵测试最初是人通过文字与机器进行交流的，伴随着人工智能的研究，人们一直在努力开发高度智能化的人机对话系统。对话系统是人工智能最具挑战性、最综合性的技术之一，涵盖了语义理解、知识表示、逻辑与推理、语言生成等各个方面。第一代对话系统主要是基于规则的。具有代表性的是1966年由MIT开发的使用模板匹配的心理医学聊天机器人 ELIZA 系统。这种基于有限状态自动机的流程图对话系统在当时很流行，优点是具有清晰的内部逻辑，易于分析和调试。但由于高度依赖专家干预，其灵活性和可扩展性很差。 ​ 随着大数据技术的兴起，由统计数据驱动的第二代对话系统应运而生。当时，强化学习在对话系统种被广泛研究和应用。在2005年由剑桥大学Steve Young教授提出的基于部分可观测马尔可夫决策过程 (POMDP)的统计对话系统，在鲁棒性上显著超越了基于规则的对话系统。它通过基于语音识别结果的贝叶斯推理来维护每一轮对话的状态，然后根据对话状态选择对话策略以生成自然语言回复。基于强化学习，该系统不断与仿真用户或真实用户交互，以检测错误并相应地优化对话策略。基于统计数据的对话系统不高度依赖专家干预，但是它的可扩展性依旧较差，并且模型很难维护。 ​ 最近几年，随着深度学习在图像、语音、文本等领域取得的突破，出现了基于深度学习的第三代对话系统。这些系统延续了统计对话系统的架构，但在各个模块应用论神经网络。神经网络的优点是具有很强的表示、分类和生成的能力，使用深度学习的对话系统超越了第二代对话系统，但在训练时需要大量的标注数据。因此，在提高模型的迁移性上仍需要大量研究。 3. 分类​ 通常，对话系统按照任务类型可分为三类：闲聊型对话系统、任务型对话系统以及问答型对话系统。 ​ 闲聊型对话系统的任务是跟随人机交互生成有趣且信息丰富的回答。 ​ 问答型对话系统分析提出的问题并在知识库中找出正确的答案。 ​ 任务型对话系统是任务驱动的多轮对话，理解用户的需求并给出正确结果，是一系列的决策过程。例如订餐、订票、查找产品等。在对话过程中，机器更新和保持内部对话状态，然后基于当前状态选择最佳回应，例如确定需求、查询限制并提供结果。 ​ 三种类型的对话系统并非完全割裂，在任务型、问答型对话系统中往往会穿插闲聊。 ​ 同时，对话系统按照领域又可分为开放域对话和限定域对话，按照对话的生成方式可分为检索式和生成式。 4. 开放域闲聊对话系统​ 主流的开放域闲聊对话系统大致上可以分为生成式和检索式两种，前者通过生成式神经网络模型根据用户输入逐词生成直至构成一个完整对话，代表工作有ECM[1]、CCM[2]，ConceptFlow[3]。后者通过在大规模对话语料库中检索得到合适的对话进行回复，代表工作有SMN[4]、DAM[5]等。近年来，随着大规模预训练模型的兴起，基于预训练的对话模型在性能上进一步提升，逐渐成为主流，代表工作有DialoGPT[6] (Microsoft)，Blender[7] (Facebook)，Meena[8] (Google)，PLATO-2[9] (Baidu) 等。 ​ 单轮的生成式闲聊对话系统可以看成是序列-序列（Seq2Seq）的编码-解码问题，不同于机器翻译，对话生成的输出空间非常大，并且用户输入的对话内容十分有限，单纯依赖用户输入来生成对话会产生重复、信息量缺乏、没有意义的回复。因此，单轮的生成式对话往往需要根据用户输入引入主题、知识库、知识图谱等信息，从而使生成的回复更加多样性和包含丰富的信息。多轮的生成式对话比单轮对话更加复杂，因为需要考虑聊天历史的上下文信息。大规模预训练模型在多轮对话闲聊任务上的表现更好。 ​ 生成式的闲聊对话系统很难处理评估和控制这两个问题。在评估上，生成式对话的结果没有标准答案，其评估往往需要依赖人工评定。在控制上，在生成过程中面临的条件非常多，尤其是开放域闲聊系统，需要考虑上下文信息、说话人的属性、场合等等，很难控制生成器的生成过程。这也是行业更多采用检索式对话系统的原因。 ​ 检索式方法是从候选回复中选择一个最佳回复，其关键在于用户输入的消息与候选回复的匹配。单轮的检索式对话相对简单，通常将用户输入的消息和候选回复进行编码，定义匹配算法检索最佳匹配。多轮的检索式对话则将历史对话与当前消息共同编码作为检索请求来匹配最佳回复。 5.问答型对话系统​ 问答型对话系统多为一问一答的形式，检索式的方法计算用户问句与问答库中问句的相似度，选择最相似的问句并给出其对应的回答（FAQ）。另一种方法是基于知识库的问答（KBQA），该方法分析和理解用户问句，识别意图，提取实体、属性等信息，并利用知识图谱中的结构化知识进行查询、推理，最后生成回复。 6. 任务型对话系统​ 任务型系统需要结合知识领域和后端的数据库或应用接口链接以完成具体操作。任务型对话系统主要分为两类：一类是具有模块化结构的流水线系统（pipeline system），另一类是最近流行于学术研究的端到端（end-to-end）系统。 ​ 任务型对话系统中广泛应用的是流水线系统，该系统由四个模块组成： 自然语言理解（Natural Language Understanding，NLU）：识别并解析用户的文本输入，以获得可供计算机使用的语义标记，如槽值（slot-values）和意图（intentions）。这些slot是根据场景预先设定的，槽值填充通常为序列标注（sequence labeling）问题，通过命名实体识别获取输入文本中包含的槽值。意图检测为分类问题，将输入文本分类为预设的意图类型。 对话状态追踪（Dialog State Tracking，DST）：根据对话历史维护当前对话状态，通常表示为槽值对。 对话策略（Dialog Policy，DP）：根据当前对话状态输出系统下一步动作。与对话状态追踪构成对话管理（Dialog Management，DM）模块。 自然语言生成（Natural Language Generation，NLG）：将系统动作转化为自然语言输出。 ​ 这种对话系统具有很好的可解释性，模块之间相互独立，在行业中应用最多。但缺点是结构不够灵活，模块之间独立导致很难一起优化，使得很难适应不同的应用场景，并且需要耗费精力去设计各个模块。 ​ 为了减少人工参与并提高不同领域之间的扩展性，端到端的任务型对话系统直接将用户输入转化为系统输出，高度灵活且扩展性强，去除了模块之间的独立性。然而，这种方法非常依赖高质量和大数量的数据，过程难以控制且可解释性不高，模型仍需要进一步探索 ，目前在行业中应用很少。 7. 数据集单轮对话​ 生成式 Reddit dataset (Zhou et al., 2018) Weibo dataset (Wu et al., 2020) Persona-chat dataset (Zhang et al., 2018) Each dialogue was constructed from a pair of crowd-workers, who chat to know each other. Each worker was assigned a persona profile, describing their characteristics, and this profile serves as knowledge in the conversation. There are 151,157 turns (each turn corresponds to an utterance and a response pair) of conversations in Persona-chat, which we divide into 122,499 for train, 14,602 for validation and 14,056 for test. The average size of a knowledge collection (the average number of sentences in a persona profile) in this dataset is 4.49. Wizard-of-Wikipedia (Dinan et al., 2018) 多轮对话​ Chatbot MovieTriples (Serban et al., 2016) Developed by expanding and preprocessing the Movie-DiC dataset by Banchs et al. (2012) Span a wide range of topics, contain long interactions with few participants and relatively few spelling mistakes and acronyms. ​ Task-oriented bAbI dialog (Bordes and Weston, 2017) The bAbI dialog includes five end-to-end dialog learning tasks in the restaurant domain, which are simulated dialog data. Task 1 to 4 are about API calls, refining API calls, recommending options, and providing additional information, respectively. Task 5 is the union of tasks 1-4. DSTC2 (Henderson et al., 2014) Extracted from real human-bot dialogs, which is noisier and harder since the bots made mistakes due to speech recognition errors or misinterpretations. In-Car Assistant / SMD (Eric et al., 2017) It has three distinct domains: calendar scheduling, weather information retrieval, and point-of-interest navigation. This dataset has shorter conversation turns, but the user and system behaviors are more diverse. In addition, the system responses are variant and the KB information is much more complicated. Hence, this dataset requires stronger ability to interact with KBs, rather than dialog state tracking. Penn Tree Bank (Marcus et al., 1993) Text8 (Mikolov et al., 2014) MultiWOZ 2.1 (Kim et al., 2020) Each newly inserted turn is grounded on unstructured knowledge in one of the four domains: restaurant, hotel, taxi and train, with the label of its relevant document in the document base. CMUDoG (Zhou, Prabhumoye, and Black, 2018) The dataset addresses the concern of the grounding in conversation responses, context, and coherence in responses. Each movie document consists of four sections corresponding to basic information and three key scenes of the movies. The 4 sections are shown to one or both workers one by one every 3 turns. The dataset consists of total 4112 conversations with an average of 21.43 turns and has been divided into a training set, validation set and test set by publishers. CamRest Contains 676 multi-turn dialogs be\u0002longing to restaurant reservation domain. There are 5 turns on average per dialog. Selected Douban Ubuntu E-commerce 8. Baselines单轮对话​ 生成式 (Generation) Seq2Seq (Sutskever et al., 2014) /+Attention (Luong et al., 2015) CopyNet (Gu et al., 2016) GenDS (Zhu et al., 2017) MemNet (hard/soft) (Ghazvininejad et al., 2018) PostKS (concat/fusion) (Lian et al., 2019) CCM (Zhou et al., 2018) GPT-2 (Radford et al., 2019) ​ 检索式 (Retrieval) 多轮对话​ Chatbot HRED (Serban et al., 2016) ​ Task-oriented Seq2Seq / + Attention MemNN (Sukhbaatar et al., 2015) Ptr-Unk (Gulcehre et al., 2016) GMemNN (Liu and Perez (2017) Mem2Seq (Madotto et al., 2018) GLMP (Wu et al., 2019) 9. 评价指标生成式 PPL BLEU ROUGE Nist Meteor Dist-1/2 Ent-4 P/R/F1 检索式 R10@1/2/5 参考文献[1] Emotional Chatting Machine: Emotional Conversation Generation with Internal and External Memory, AAAI 2018. [2] CCM: Commonsense Knowledge Aware Conversation Generation with Graph Attention, IJCAI 2018. [3] Grounded Conversation Generation as Guided Traverses in Commonsense Knowledge Graphs, ACL 2020. [4] Sequential Matching Network: A New Architecture for Multi-turn Response Selection in Retrieval-Based Chatbots, ACL 2017. [5] Multi-Turn Response Selection for Chatbots with Deep Attention Matching, ACL 2018. [6] DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation, ACL 2020. [7] Recipes for building an open-domain chatbot, facebook 2020. [8] Towards a Human-like Open-Domain Chatbot, google 2020. [9] PLATO-2: Towards Building an Open-Domain Chatbot via Curriculum Learning, ACL 2021.","link":"2021/12/14/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F%E7%BB%BC%E8%BF%B0/"},{"title":"图神经网络综述","text":"本文对综述“A comprehensive survey on graph neural networks, IEEE 2021”进行了翻译，初步学习图神经网络并加深对文章内容的理解。原文：A comprehensive survey on graph neural networks, IEEE 2021。 &emsp;&emsp;近年来，深度学习在许多机器学习任务上（如图像分类、视频处理、语音识别、自然语言处理）取得了开创性的成果。通常，这些任务的数据（如图像、序列信息）都是表示在欧氏空间的（Euclidean space）。然而越来越多的应用数据是非欧氏空间的，例如图数据用图结构来表示，不同对象之间有复杂的关系和内在依赖，使得现有的机器学习算法变得不再有效。 &emsp;&emsp;最近，出现了许多针对图数据的深度学习方法。本文对数据挖掘（data mining）和机器学习（machine learning）领域的图神经网络（Graph Neural Networks, GNNs）进行全面梳理，并将其分为四类：循环图神经网络（recurrent GNNs，RecGNNs），卷积神经网络（convolutional GNNs，ConvGNNs），图自编码器（graph autoencoders，GAEs）和时空神经网络（spatial-temporal GNNs，STGNNs）。 1. 引言&emsp;&emsp;神经网络近年来的成果促进了模式识别和数据挖掘的研究。许多机器学习任务，例如目标检测、机器翻译和语音识别等，曾严重依赖特征工程人工地提取特征信息，最近被各种端到端的深度学习方法所取代。深度学习方法的成功得益于快速发展的计算资源（如 GPU）和大量训练数据，以及从欧式空间数据（如图像、文本和视频）提取隐含表示的有效性。以图像数据为例，图像可以表示为欧氏空间的规则网格数据。CNN 能够利用图像数据的移位不变性、局部连通性和组合性提取可全局共享的有意义的局部特征。 &emsp;&emsp;深度学习能够有效捕获欧氏空间数据的隐藏模式，然而越来越多的应用数据表示为图的形式。例如，在电子商务中，一个基于图的学习系统可以利用用户和产品之间的交互做出高度准确的推荐；在化学领域，分子被表示为图模型，鉴定分子的生物活性可用于药物发现；在引文网络中，文章通过引文系统相互连接，需要将文章分为不同的组。图数据的复杂性给现有的机器学习算法带来了巨大的挑战。由于图是不规则的，节点无序且多变，并且节点的邻居数量也可能不同，导致一些重要操作（如卷积）在图像中容易计算但在图领域中却很难计算。另外，现有的机器学习算法的一个重要假设是数据实例之间的相互独立性，这个假设在图数据中不再成立，因为图的每个实例（节点）通过多变的连接相互关联，并非独立。 2. 背景介绍图神经网络简史&emsp;&emsp;Sperduti 和 Starita （1997）首次将神经网络应用到有向无环图（directed acyclic graphs）中 [1]，激发了早期的对 GNNs 的研究。大多数早期的 GNNs 是 RecGNNs，它们通过迭代的方式传播邻居信息直到达到一个稳定值，以此来学习目标节点的表示，然而这个过程的计算成本很高。 &emsp;&emsp;鉴于 CNNs 在计算机视觉领域的成功，ConvGNNs 得到了大量研究，主要分为两类：基于谱（spectral-based）的方法和基于空间（spatial-based）的方法。基于谱的 ConvGNNs 的第一个重要研究是由 Bruna 等人（2014）提出的基于谱图论（spectral graph theory）的图卷积 [2]。从此，基于谱的 ConvGNNs 得到了越来越多的改进、扩展和研究。实际上，基于空间的 ConvGNNs 的研究要早于基于谱的 ConvGNNs。2009年，Micheli 继承了 RecGNNs 中消息传递的思想，通过复合非递归层（composite nonrecursive layers）首次解决了图的相互依赖性 [3]。然而这篇重要的工作的在当时被忽略了，直到最近才出现了许多基于空间的 ConvGNNs。 &emsp;&emsp;除了 RecGNNs 和 ConvGNNs，最近几年出现了一些其他的 GNNs，包括 GAEs 和 STGNNs 等。 图神经网络 vs 网络嵌入&emsp;&emsp;GNNs 的研究与图嵌入或网络嵌入是十分相关的。网络嵌入（Network Embedding）目的是在保留网络拓扑结构和节点内容信息的同时将网络节点表示为低维向量表示，从而方便将现有的机器学习算法应用到下游的图分析任务中，例如分类、聚类和推荐等。另一方面，GNNs 是以端到端的方式解决图相关任务的深度学习模型，目的是提取图、节点等的高层表示。GNNs 和网络嵌入之间的主要区别是，GNNs 是针对不同任务设计的一组神经网络模型，而网络嵌入涵盖网络节点嵌入的不同方法，包括非深度学习方法，例如矩阵分解、随机游走等。 图神经网络 vs 图核方法&emsp;&emsp;图核（Graph kernels）是早期解决图分类问题主流方法，这种方法使用核函数来度量图之间的相似性。与 GNNs 类似，图核方法可以通过映射函数将图或节点嵌入向量空间。不同的是，图核的映射函数是确定的，而 GNNs 是可学习的。 3. 分类和框架A. 图神经网络分类 循环图神经网络（Recurrent Graph Neural Networks, RecGNNs）：大多数早期的 GNNs 是循环图神经网络（下称 RecGNNs），它通过循环神经结构学习节点表示。RecGNNs 认为图中的一个节点不断地与其邻居节点交换信息直到达到稳定平衡。其消息传递（message passing）的思想在基于空间的卷积图神经网络（spatial-based ConvGNNs）中也得到了应用。 卷积图神经网络（Convolutional Graph Neural Networks, ConvGNNs）：这是将卷积操作从网格数据到图数据的拓展。主要思想是通过整合节点本身及其邻居节点的特征来表示该节点。 图自编码器（Graph Autoencoders, GAEs）：GAEs 是无监督的学习框架，编码器将节点、图编码到隐含向量空间，然后通过解码器从编码的信息重构图数据。 B. 框架&emsp;&emsp;图神经网络的输入是图结构（graph structure）和节点的内容信息（node content information），输出可用于不同的图分析任务。 节点级别（Node level）：用于节点回归（node regression）和节点分类（node classification）任务。RecGNNs 和 ConvGNNs 分别通过信息传播和图卷积捕获高层节点表示，然后接多层感知机或 softmax 层作为输出层，从而可以端到端地处理节点级别的任务。 边级别（Edge level）：用于边分类（edge classification）和链接预测（link prediction）任务。通过 GNNs 得到两个节点的隐藏表示，使用相似度函数或者神经网络可以预测节点之间的边的标签（label）或连接强度（connection strength）。 图级别（Graph level）：用于图分类（graph classification）任务。为了得到图级别的紧凑表示（compact representation），GNNs 通常与池化（pooling）和读出（readout）操作相结合。 训练框架 &emsp;&emsp;许多 GNNs 能够通过端到端的学习框架以（半）监督或完全无监督的方式训练，这取决于学习任务和可获得的标签信息。 节点分类的半监督学习：对于只有部分节点标签的图网络，ConvGNNs 能够学习一个鲁棒的模型用于高效地判断（identify）无标签节点的标签。 图分类的监督学习：图分类的目的是预测整个图的类别。该任务可以结合图卷积层、图池化层和读出层进行端到端学习。其中，图卷积层提取高层节点表示；图池化层进行下采样，将图粗化（coarsen）为子结构；读出层将每个图的节点表示细化为图表示；最后使用多层感知机或 softmax 层对图表示进行分类。 图嵌入的无监督学习：当图中没有类别标签时，可以通过端到端的框架以完全无监督的方式学习图嵌入。这些利用边信息的算法分为两种：第一种简单的方法是采用自编码器框架，其中编码器使用图卷积层将图嵌入到隐藏表示，解码器用隐藏表示重构图结构；另一种流行的方法是利用负采样将一部分节点作为负样本对（negative pairs），而图中存在连接的节点对作为正样本对。然后使用逻辑回归层来区分正负样本对，以此来学习图嵌入。 4. 循环图神经网络&emsp;&emsp;RecGNNs 是最早得到研究的 GNNs，考虑到计算能力，早期的研究主要集中在有向无环图（directed acyclic graphs）。Scarselli 等人（2009）[4] 将之前的递归模型扩展到处理一般类型的图的模型 GNN*（为区分于广泛意义上的 GNN，用 GNN*来表示该模型），如无环图、有环图、有向图、无向图。基于信息扩散机制（information diffusion mechanism），GNN* 通过循环交换邻居信息来更新节点状态直到达到一个稳定平衡。节点的隐藏状态通过下面的公式更新： h^{(t)}_v = \\sum_{u \\in N(v)} f(x_v, x^e_{(v, u)}, x_u, h^{(t-1)}_u)&emsp;&emsp;其中，$f(·)$ 是参数函数，$h^{(0)}_v$ 是随机初始化的。该求和函数使得 GNN* 适用于任何节点，即使节点的邻居数量不同且不知道邻居的顺序。为了保证收敛，递归函数 $f(·)$ 必须是收缩映射，将两个点映射到潜在空间后收缩两点之间的距离。如果 $f(·)$ 是神经网络，那么参数的雅可比矩阵必须要施加一个惩罚项。当满足收敛准则时，再将最后一层节点的隐藏状态前向传播到读出层。GNN* 通过计算交替节点状态传播核参数梯度来最小化训练目标，这种策略使 GNN* 能够解决有环图。在后续工作中，GraphESN（graph echo state network）（2010）[5] 扩展了 echo state 网络以提高 GNN* 的训练效率。 &emsp;&emsp;GGNN （Gated GNN）（2015）[6] 使用了门控循环单元（GRU）作为循环函数，将循环次数降低到一个固定的步骤数，其优点是不再需要约束参数来确保收敛。节点隐藏状态通过其先前的隐藏状态和相邻的隐藏状态更新： h^{(t)}_v = GRU(h^{(t-1)}_v, \\sum_{u \\in N(v)} Wh^{(t-1)}_u)&emsp;&emsp;其中，$h^{(0)}_v = x_v$ 。与 GNN* 和 GraphESN 不同，GGNN 使用时间反向传播（backpropagation through time, BPTT）算法来学习模型参数。GGNN 需要对所有节点多次迭代计算，并需要将所有结点的中间状态存储在内存中，这对于大型图来说是有问题的。 &emsp;&emsp;SSE（Stochastic steady-state embedding）（2018）[7] 是一种对大型图更具扩展性的学习算法，以随机和异步的方式重复更新节点隐藏状态。它对一批节点进行采样以进行状态更新，并对一批节点进行梯度计算。为了保持稳定性，SSE 的循环函数为历史状态和新状态的加权平均： h^{(t)}_v = (1 - \\alpha )h^{(t-1)}_v + \\alpha W_1 \\sigma (W_2[x_v, \\sum _{u \\in N(v)}[h^{(t-1)}_u, x_u]])&emsp;&emsp;其中，$\\alpha$ 是超参数，$h^{(0)}_v$ 是随机初始化的。虽然 SSE 在概念上很重要，但是它没有从理论上证明通过重复利用该函数，节点状态能够收敛到稳定值。 5. 卷积图神经网络&emsp;&emsp;由于卷积操作效率更高并且更易于与其他网络组合，近年来对 ConvGNNs 的研究迅速增长。ConvGNNs 分为两类：基于谱的（spectral-based）与基于空间的（spatial-based）。基于谱的方法从图信号处理（graph signal processing）的角度引入了滤波器，将图卷积运算解释为从图信号中去除噪声；基于空间的方法继承了 RecGNNs 的信息传播的思想，将图卷积定义为信息传播。由于 GCN（2017）[8] 弥合了基于谱的方法和基于空间的方法之间的鸿沟，基于空间的方法由于其高效、灵活和通用，最近得到了迅速发展。 A. 基于谱的 ConvGNNs&emsp;&emsp;基于谱的方法基于很强的图信号处理的数学基础。无向图的数学表示为归一化的图拉普拉斯矩阵（normalized graph Laplacian matrix）是，定义为 $ \\mathrm{L = In - D ^{(1/2)}AD^{-(1/2)}}$，其中 $\\mathrm D$ 是节点度的对角矩阵，$\\mathrm{D_ij = \\sum_j(A{i,j})}$。归一化的图拉普拉斯矩阵具有实对称半正定的性质，基于此，可以分解为 $\\mathrm{L = U\\Lambda U^T}$，其中 $\\mathrm U = [u0, u1, …, u{n-1}] \\in R^{n \\times n}$ 是按特征值排序的特征向量矩阵，$ \\Lambda $ 是特征值的对角矩阵（谱），$\\Lambda_{ij} = \\lambda_i$。在数学上，归一化拉普拉斯矩阵的特征向量形成一个正交空间 $\\mathrm {U^TU = I}$。在图信号处理中，一个图信号 $x \\in R^n $ 是图的所有节点的一个特征向量，其中 $x_i$ 是第 i 个节点的特征值。一个信号 $\\mathrm x$ 的图傅里叶变换定义为 $\\mathscr F(x) = \\mathbf {U^T} \\mathbf x$，逆变换为 $\\mathscr F^{-1}(\\hat x) = \\mathbf {U \\hat x}$，其中 $\\mathbf {\\hat {x}}$ 表示图傅里叶变换后的信号。图傅里叶变换将输入的图信号映射到正交空间，基由归一化的图拉普拉斯算子的特征向量构成。变换后的信号 $\\hat x$ 的元素是新空间中图信号的坐标，因此输入的信号可以表示为 $x = \\sum_i \\hat x_iu_i$，这正是图傅里叶逆变换。于是，输入信号 $\\mathbf x$ 与滤波器 $\\mathbf {g \\in R^n}$ 定义为: \\mathbf {x *_G g = \\mathscr F^{-1}(\\mathscr F(x) \\odot \\mathscr F(g)) = U(U^T x \\odot U^T g)}&emsp;&emsp;其中，$\\odot$ 表示元素乘积（elementwise product）。如果我们将滤波器表示为 $\\mathbf g_\\theta = diag(\\mathbf {U^T g})$，那么谱图卷积（spectral graph convolution）就被简化为： \\mathbf {x *_G g_\\theta = U g_\\theta U^T x}&emsp;&emsp;基于谱的 ConvGNNs 遵循以上定义，主要区别在于滤波器 $\\mathbf g\\theta$ 的选择。Spectral CNN（2014）[9] 将滤波器 $\\mathbf g\\theta = \\Theta^{(k)}_{i,j}$ 当作可学习的参数并且可以处理多通道的图信号。其图卷积层定义为： &emsp;&emsp;其中，$k$ 是层索引，$\\mathbf {H^{(k-1)} \\in R^{n \\times f{k-1}} }$ 是输入的图信号，$\\mathbf {H{(0)} = X}$，$f{k-1}$ 是输入的通道数，$fk$ 是输出的通道数，$\\Theta^{(k)}{i,j}$ 是可学习参数的对角矩阵。由于使用拉普拉斯矩阵特征分解（eigendecomposition），Spetral CNN 有三个限制：第一，图的任何扰动都会导致特征基的变化；第二，学习的滤波器依赖域，这意味着不能应用到具有不同结构的图中；第三，特征分解的计算复杂度为 $O(n^3)$。后续的工作中，ChebNet 和 GCN 通过一些近似和简化将复杂度降低到 $O(m)$。 &emsp;&emsp;ChebNet（CHebyshev spectral CNN）（2016）[10] 用对角矩阵中特征值的 Chebyshev 多项式来近似滤波器：$\\mathbf {g\\theta = \\sum ^K{i=0} \\thetai T_i (\\hat {\\Lambda})}$ ，其中 $\\mathbf {\\hat {\\Lambda} = 2 \\Lambda / \\lambda{max} - In}$，$\\hat {\\Lambda} \\in [-1, 1]$。 Chebyshev 多项式是递归定义的：$T_i(x) = 2xT{i-1}(x) - T{i-2}(x)$， $T_0(x) = 1$，$T_1(x) = x$。于是，图信号 $\\mathbf x$ 的卷积与滤波器 $\\mathbf g\\theta$ 的定义为： &emsp;&emsp;其中，$\\mathbf {\\hat L = 2L/\\lambda _{max} - I_n}$。由于可以通过归纳证明 $T_i(\\hat {\\mathbf L}) = \\mathbf {U} T_i (\\hat {\\mathbf \\Lambda}) \\mathbf {U}^T$，ChebNet 可以表示为： &emsp;&emsp;作为对 Spectral CNN 的改进，ChebNet 定义的滤波器在空间中局部化，使滤波器可以不考虑图的大小来提取局部特征。ChebNet 的光谱被线性映射到 [-1, 1]。CayleyNet（2019）[11] 进一步应用参数有理复函数（parametric rational complex functions）的 Cayley 多项式来捕获窄频带（narrow frequency bands）。Cayley 的谱图卷积定义为： &emsp;&emsp;其中，$\\mathrm {Re(·)}$ 返回复数的实部，$c_0$ 是实数系数，$c_j$ 是复数系数，$i$ 是虚数，$h$ 是控制 Cayley 滤波器频谱的参数。在保持空间局部性的同时，CayleyNet 证明 ChebNet 可以看作是 CayleyNet 的特例。 &emsp;&emsp;GCN（Graph convolutional network）对 ChebNet 进行了一阶近似。假设 $K = 1, \\lambda_{max} = 2$，于是 ChebNet 简化为： &emsp;&emsp;为了限制参数量且避免过拟合，GCN 进一步假设 $\\theta = \\theta_0 = -\\theta_1$，从而图卷积的定义如下： &emsp;&emsp;为了能满足多通道输入和输出，GCN 将上式修改为： &emsp;&emsp;其中，$\\mathbf {\\bar A = In + D^{-(1/2)}AD^{-(1/2)}}$，$f(·)$ 是激活函数。$\\bar {\\mathbf A}$ 会导致 GCN 的数值不稳定，为了解决这个问题，GCN 应用了一个规范化的技巧，用 $\\mathbf {\\bar A = \\tilde D^{-(1/2)} \\tilde A \\tilde D^{-(1/2)}}$ 代替$\\mathbf {\\bar A = I_n + D^{-(1/2)}AD^{-(1/2)}}$。其中，$\\mathbf {\\tilde A = A + I_n, \\tilde D{ii} = \\sumj \\tilde A{ij}}$。作为一种基于谱的方法，GCN 也可以解释为基于空间的方法。从基于空间的角度来看，GCN 可以看作是从节点的邻居聚合特征信息： &emsp;&emsp;最近的一些工作通过探索其他对称矩阵对 GCN 进行了改进。AGCN（Adaptive GCN）（2018）[12] 通过图邻接矩阵学习隐藏结构关系，它将两个节点的特征作为输入学习距离函数来构造残差图邻接矩阵。DGCN（Dual GCN）（2018）[13] 引入了一种双图（dual-graph）卷积结构，具有两个并行的共享参数的图卷积层，使用标准化的邻接矩阵 $\\mathbf {\\bar A}$ 和正点互信息（positive pointwise mutual information, PPMI）矩阵，对图随机游走采样捕获节点共现信息。PPMI 矩阵定义为： &emsp;&emsp;其中，$v1, v_2 \\in V, |D| = \\sum {v_1, v_2} count(v_1, v_2)$ 和 $count(·)$ 函数返回节点 $v$ 和/或节点 $u$ 在随机游走采样中共现/出现的频率。通过对双图卷积层的输出进行聚合，DGCN 可以对局部和全局结构信息进行编码，而不需要堆叠多个图卷积层。 B. 基于空间的 ConvGNNs&emsp;&emsp;与传统的图像卷积的 CNN 类似，基于空间的方法通过节点的空间关系来定义图卷积。如果将像素表示为节点，图像的卷积可以看作是图卷积的特例。在图像的卷积上，滤波器对每个通道上的中心节点及其邻居节点的像素值进行加权平均。类似地，基于空间的图卷积对中心节点表示及其邻居节点表示进行卷积，以此来更新中心节点的表示，如下图所示。从另一角度看，基于空间的 ConvGNNs 与 RecGNNs 具有相同的信息传播/消息传播思想。空间图卷积运算本质上是沿着边传播节点信息。 &emsp;&emsp;与 GNN* 同时提出的 NN4G（Neural network for graphs）（2009）[14] 是第一个基于空间的 ConvGNNs。与 RecGNNs 明显不同的是，NN4G 通过一个每层具有独立参数的组合神经结构学习图的相互依赖性，并通过增加架构结构来扩展节点的邻居数。NN4G 通过直接汇总节点的邻居信息进行图卷积，并且应用残差连接来记忆每一层的信息。NN4G 通过下面的公式更新下一层节点： &emsp;&emsp;其中，$f(·)$ 是激活函数，$h^{(0)}_v = 0$。上式的矩阵形式为： &emsp;&emsp;与 GCN 不同的是，NN4G 使用非规范化的邻接矩阵，这可能会导致隐藏节点状态大小差别非常大。CGMM（Contextual graph Markov model）（2018）[15] 提出了一个受 NN4G 启发的概率模型，在保持空间局部性的同时，CGMM 具有概率解释能力。DCNN（Diffusion CNN）(2016）[16] 将图卷积看作一个扩散过程，它假设信息以一定的转移概率从一个节点转移到其邻居节点上，从而信息分布在几轮之后达到平衡。DCNN 将扩散图卷积（diffusion graph convolutin, DGC）定义为： &emsp;&emsp;其中，$f(·)$ 是激活函数，概率变换矩阵 $\\mathbf {P \\in R^{n \\times n}}$ 通过 $\\mathbf {P = D^{-1}A}$ 计算。注意，在 DCNN 中，隐藏表示矩阵 $\\mathbf H^{(k)}$ 与输入特征矩阵 $\\mathbf X$ 维度相同，并且不是其先前隐藏表示矩阵 $\\mathbf H^{(k-1)}$ 的函数。DCNN 将 $\\mathbf {H^{(1)}, H^{(2)}, …, H^{(K)}}$ 连接（concatenation）在一起作为模型的最终输出。由于扩散过程的平稳分布是概率变换矩阵幂级数的总和，DGC（2018）[17] 将每次扩散的输出相加而不是连接： &emsp;&emsp;其中，$\\mathbf {W^{(k)} \\in R^{D \\times F}}$ ，$f(·)$ 是激活函数。使用转移概率矩阵的幂意味着较远的邻居对中心节点贡献的信息很少，PGC-DGCNN（2018）[18] 基于最短路径提高较远邻居的贡献，定义了最短路径邻接矩阵 $\\mathbf S^{(j)}$ 。如果从节点 $v$ 到节点 $u$ 的最短路径长度是 $j$ ，那么 $\\mathbf S^{(j)}_{v, u} = 1$，否则为0。PGC-DGCNN 通过超参数 $r$ 控制感受野大小，引入了一种图卷积运算： &emsp;&emsp;其中，$\\mathbf {H^{(0)} = X, \\bar{A}^{(j)} = (\\tilde{D}^{(j)})^{-(1/2)} \\tilde{A}^{(j)} (\\tilde {D}^{(j)})^{-(1/2)}, \\tilde{A}^{(j)} = A^{(j)} + I}$。 &emsp;&emsp;MPNN（message-passing neural net work）（2017）[19] 提出了基于空间的 ConvGNNs 的一般框架。它将图卷积视为消息传递（message-passing）过程，在这个过程中，信息可以沿着边直接从一个节点传递到另一个节点。MPNN 进行 K 步迭代进行消息传递，消息传递函数定义为： &emsp;&emsp;其中，$\\mathbf h^{(0)}_v = \\mathbf x_v$，$U_k(·), M_k(·)$ 是可学习参数的函数。在计算出每个结点的隐藏表示之后，可以将 $\\mathbf h^{(K)}_v$ 传递到输出层用于节点预测任务，或者传递到读出（readout）函数用于图预测任务。读出函数基于节点隐藏表示生成整个图的表示，通常被定义为： &emsp;&emsp;其中，$R(·)$ 表示可学习参数的读出函数。然而，GIN（Graph isomorphism network）（2019）[20] 发现以前基于 MPNN 的方法无法通过生成的图嵌入区分不同的图结构。为了解决这一点，GIN 通过一个可学习的参数 $\\epsilon^{(k)}$ 调整中心节点的权重。其图卷积定义为： &emsp;&emsp;其中，$\\mathbf MLP(·)$ 代表多层感知机。由于节点的邻居数量可能从一个到一千个甚至更多，因此获取节点所有的邻居是低效的。GraphSage（2017）[21] 为每个节点采样固定数量的邻居。其图卷积定义为： &emsp;&emsp;其中，$\\mathbf h^{(0)}v = \\mathbf x_v$，$f_k(·)$ 是聚合函数，$S{N(v)}$ 是节点 $v$ 的邻居的一个随机采样。聚合函数应该与节点的顺序无关，例如求平均值、求和或最大值等。 &emsp;&emsp;GAT（Graph attention network）（2017）[22] 中邻居节点对中心节点的贡献既不同于 GraphSage，也不同于 GCN。GAT 采用注意力机制学习两个相连节点之间的相对权重，其图卷积运算定义为： &emsp;&emsp;其中，$\\mathbf h^{(0)}v = \\mathbf x_v$。注意力权重 $\\alpha^{(k)}{vu}$ 代表节点 $v$ 与其邻居节点 $u$ 之间的连接强度： &emsp;&emsp;其中，$g(·)$ 是 $\\mathrm {LeakyReLU}$ 激活函数，$a$ 是可学习的向量。softmax 函数能保证节点 $v$ 的所有邻居节点的注意力权重总和为1。使用多头注意力（multihead attention）的 GAT 进一步增加了模型的表达能力。 与 GraphSage 相比，GAT 在节点分类任务上有明显的性能提升。GAT 的注意力头的贡献是相等的，而 GAAN （Gated attention network）（2018）[23] 引入了自注意力（self-attention）机制，为每个注意力头计算额外的注意力分数。除了在空间上应用图注意力外，GeniePath（2019）[24] 进一步提出了类似 LSTM 的门控机制来控制图卷积层之间的信息流。 &emsp;&emsp;MoNet（Mixture model network）（2017）[25] 引入节点的伪坐标来确定节点与其邻居节点之间的相对位置，以此为结点的邻居分配不同的权重。一旦两个节点之间的相对位置确定，权重函数就会将相对位置映射为这两个节点之间的相对权重。通过这种方式，图滤波器的参数可以在不同的位置共享。在 MoNet 框架下，一些现有的流形（manifolds）的方法，例如 GCNN（geodesic CNN）（2015）[26] 、ACNN（anisotropic CNN）（2016）[27] 和 spline CNN（2018）[28]，图方法，例如 GCN、DCNN 等，可以通过构造非参数权重函数将其归纳为 MoNet 的特例。MoNet 还提出了一种具有可学习参数的高斯核来自适应学习权重函数。 &emsp;&emsp;另一个不同的工作路线基于特定标准对结点的邻居进行排序，并将每个排序与可学习的权重相关联，从而实现不同位置的权重共享。PATCHY_SAN（2016）[29] 根据每个结点的图标签对其邻居进行排序，并选择前 $q$ 个邻居。图标签本质上是节点分数，可以通过节点度（degree）、中心度（centrality）和 Weisfeiler-Lehman（WL）颜色计算。由于每个节点有了固定数量的有序邻居，因此可以将图结构数据转换为网格数据。PATCHY-SAN 应用标准的一维卷积滤波器来聚合邻居特征信息，其中滤波器权重的顺序对应于节点邻居的顺序。PATCHY-SAN 的排序标准只考虑图结构，数据处理需要大量的计算。LGCN（Large-scale GCN）（2018）[30] 基于节点特征信息对节点的邻居进行排序。对于每个节点，LGCN 组合由其邻居组成的特征矩阵，并按列对该特征矩阵排序。排序后的特征矩阵的前 $q$ 行作为中心节点的输入数据。 训练效率的提升 &emsp;&emsp;训练 ConvGNNs，例如 GCN，通常需要把整个图数据和所有节点的中间状态存到内存中。当图包含的节点过多时，一起训练全部图数据会出现严重的内存溢出问题。为了节省内存，GraphSage 提出了批训练算法，以固定的样本大小通过 K 步递循环扩展每个节点的邻居节点。对于每个采样的树，GraphSage 通过从下到上分层聚合隐藏节点表示来计算根节点的隐藏表示。FastGCN（Fast learning with GCN）（2018）[31] 对每个图卷积层采样固定的节点数，而不是对每个节点采样固定数量的邻居。它将图卷积解释为概率度量下的节点嵌入函数的积分变换。蒙特卡洛近似（Monte Carlo approximation）和方差缩减（variance reduction）技术也被用于加速训练过程。由于 FastGCN 为每层独立采样节点，层之间的连接可能是稀疏的。Huang 等人（2018）[32] 提出了一种自适应分层采样方法，其中上层的节点采样作为下层的节点采样的前置条件。与 FastGCN 相比，该方法具有更高的精度，但采样方案更复杂。 &emsp;&emsp;另一个工作中，StoGCN（stochastic training of GCNs）（2018）[33] 使用历史节点表示作为控制变量，将图卷积的感受野大小大大降低。即使每个节点只有两个邻居，StoGCN 也能获得很好的性能。然而，StoGCN 仍然需要保存所有结点的中间状态，这对于大型图来说依然消耗内存。 &emsp;&emsp;Cluster-GCN（2019）[34] 使用图聚类算法对子图进行采样，并对采样子图的节点进行图卷积。由于在采样子图内进行邻居搜索，Cluster-GCN 能够用更少的时间和内存使用更深的结构处理更大的图。下表是一些模型的时间和内存的比较。 &emsp;&emsp;表中，$n$ 是节点总数，$m$ 是边总数，$K$ 是网络层数，$s$ 是批大小（batch-size）。GCN 是全批量（full-batch）训练的基线方法，GraphSage 牺牲时间效率降低了内存使用，同时，随着 $K$ 和 $r$ 的增加，GraphSage 的时间和内存呈指数增长。Sto-GCN 的时间复杂度最高，并且内存瓶颈也没有解决，然而在 $r$ 很小时能获得较好的性能。Cluster-GCN 的时间复杂度与基线方法相同，在所有方法中空间复杂度最低。 谱模型与空间模型的比较 &emsp;&emsp;谱模型有图信号处理的理论基础，通过设计新的图信号滤波器，可以构建新的 ConvGNNs。然而，在效率、通用性和灵活性上，空间模型优于谱模型。首先，谱模型的效率低于空间模型，谱模型要么需要进行特征向量计算，要么同时处理整个图。而空间模型通过信息传播直接图进行卷积，在处理大型图上更具可扩展性。而且空间模型可以对节点分批计算，而不是计算整个图。其次，依赖图傅里叶基的谱模型很难推广到新的图上，因为对图的任何扰动都会导致特征基的变化。另一方面，基于空间的模型在每个节点上计算局部图卷积，其权重在不同的位置和结构上很容易共享。第三，谱模型仅适用于对无向图，而空间模型处理多源图，例如输入边、有向图、符号图和超图等，更灵活，因为这些图可以很容易地合并到聚合函数中。 C. 图池化模块&emsp;&emsp;GNN 生成节点的特征向量后可用于下游任务。然而，直接使用所有特征是困难的，因此需要采用下采样（downsample）策略。池化（pooling）操作旨在通过对节点进行下采样来生成更小的表示来减小参数量，从而避免过拟合、置换不变性（permutation invariance）和计算复杂性问题。读出操作和池化操作非常类似，主要用于基于节点表示生成图表示。 &emsp;&emsp;在早期的工作中，图粗化（graph coarsening）算法根据图的拓扑结构使用特征分解对图进行粗化。然而，这些方法面临时间复杂度问题。Graclus 算法（2007）[35] 是特征分解的一种替代方法，用于计算原始图的聚类。最近的一些工作将其作为池化操作对图粗化。 &emsp;&emsp;目前，由于在池化窗口计算平均值、最大值、求和是很快的，因此常用来作为下采样最原始和最有效的方法： &emsp;&emsp;其中，$K$ 代表最后一个图卷积层数。Henaff 等人（2015）[36] 表明，在网络开始时执行简单的最大、平均池化对于降低图域中的维数和降低图傅里叶变换操作的成本是尤为重要的。此外，一些工作也使用注意力机制增强平均、求和池化操作。图嵌入将任何大小的图生成固定大小的嵌入，即使使用注意力机制，下采样的操作会使嵌入的效率降低。Vinyals 等人（2016）[37] 提出了 Set2Set 的方法来生成随输入大小增加的内存，然后它用一个 LSTM 在下采样之前整合顺序相关的信息到内存嵌入中，避免破坏信息。 &emsp;&emsp;Defferard 等人在 ChebNet 中设计了一个有效的池化策略，按照某种意义重新排序图的节点来解决这个问题。首先通过 Graclus 算法将输入图粗化为多个级别，然后将输入图的节点和粗化后的重新排列为平衡二叉树。将平衡二叉树相似的节点排列在一起，从下到上任意聚合，将排列好的信号池化比原始池化要有效得多。 &emsp;&emsp;Zhang 等人（2018）[38] 提出的 DGCNN 中使用了名为 SortPooling 的类似的池化策略。与 ChebNet 不同，DGCNN 根据节点在图中的结构角色对节点排序。来自空间图卷积的图的无序节点特征被视为连续的 WL 颜色，并以此来排序节点。除了对节点特征进行排序外，他还通过截断/扩展节点特征矩阵将图大小统一为 $q$。如果 $n &gt; q$，则删除最后的 $n - q$ 行；否则，添加 $q - n$ 个零行。 &emsp;&emsp;上述的池化方法主要考虑到图的特征而忽略了图的结构信息。最近提出了一种可微池化（differentiable pooling，DiffPool）（2018）[39] 生成图的层级表示。与以前所有的粗化方法相比，DiffPool 不止对图中节点聚类，而是在 $k$ 层学习一个聚合分配矩阵 $\\mathbf S$，在第 $k$ 层表示为 $\\mathbf S{(k)} \\in \\mathbf R^{nk \\times n{k+1}}$，其中， $n_k$ 是第 $k$ 层的节点数量。矩阵 $\\mathbf S^{(k)}$ 中的概率值是基于节点特征和拓扑结构，利用下面的公式计算的： &emsp;&emsp;其核心思想是综合考虑图的拓扑结构和特征信息来学习节点的表示，因此上式可以用任何标准的 ConvGNNs 来实现。但是，DiffPool 的缺点是它在池化后生成稠密图，计算复杂度为 $O(n^2)$。最近提出的 SAGPool（2019）[40]方法考虑了节点的特征和图拓扑结构，并以自注意力的方式学习池化。 &emsp;&emsp;总的来说，池化是减小图大小的一个基本操作。如何提高池化的有效性和计算复杂性是一个仍待研究的开放问题。 D. 理论方面的讨论 感受野形状（Shape of Receptive Field）：节点的感受野是决定其最终节点表示的一组节点。当合成多个空间图卷积层时，节点的感受野每次都朝着他的远处邻居节点扩展。Micheli （2009）[41] 证明了存在有限数量的空间图卷积层使得每个节点 $v \\in V$ 的感受野覆盖图中的所有节点。因此，ConvGNNs 能够通过叠加局部图卷积层来提取全局信息。 VC 维（VC Dimension）：VC 维是模型复杂度的一种度，关于 GNNs 的 VC 维分析的工作很少。给定模型的参数量 $p$ 和节点数量 $n$，Scarselli 等人（2018）[42] 推导出，如果使用 sigmoid 或 tangent 双曲线激活函数，GNN* 的 VC 维是 $O(p^4n^2)$；如果使用分段多项式激活函数，则 VC 维是 $O(p^2n)$。这一结果表明，如果使用 sigmoid 或 tangent 双曲线激活函数，GNN* 的模型复杂度随着 $p$ 和 $n$ 的增加而迅速增加。 图同构（Graph Isomorphism）：如果两个图拓扑相同，则称为同构。给定两个非同构图 $G_1$ 和 $G_2$，Xu 等人（2019）[43] 证明，如果 GNN 将 $G_1$ 和 $G_2$ 映射到不同的嵌入，则这两个图可以通过同构的 WL 检验确定为非同构的。他们表明，常见的 GNNs，如 GCN 和 GraphSage，不能区分不同的图结构。Xu 等人进一步证明，如果 GNN 的聚合函数和读出函数是内射的，则 GNN 能够像 WL 检测一样区分不同的图。 等变性与不变性（Equivariance and Invariance）：GNN 在处理节点级任务时必须是等边函数，在处理图级别任务时必须是不变函数。对于节点级任务，令 $f(\\mathbf {A, X}) \\in R^{n \\times d}$ 是一个 GNN，$\\mathbf Q$ 可以是节点顺序不同的任何置换矩阵。如果 GNN 满足 $f(\\mathbf {QAQ^T, QX}) = \\mathbf Q f(\\mathbf {A, X})$ 那么该 GNN 是等变的。对于图级别的任务，令 $f(\\mathbf {A, X}) \\in R^d$。如果满足 $f(\\mathbf {QAQ^T, QX}) = f(\\mathbf {A, X})$，则 GNN 是不变的。为了实现等变性与不变性，GNN 的组成部分必须对节点顺序保持不变。Maron 等人（2019）[44] 从理论上研究了图数据的置换不变和等变线形层的性质。 通用近似（Universal Approximation）：众所周知，具有一个隐藏层的多层感知机前馈神经网络可以逼近任何 Borel 可观测函数（1989）[45]，而 GNNs 的通用近似能力很少被研究。Hammer 等人（2005）[46] 证明了级联关系（cascade correlation）可以近似结构化输出的函数。Scarselli 等人（2009）[4] 证明了 RecGNN 可以近似任何保持展开等价性的任何精度的函数。（如果两个节点的展开树相同，则他们展开等价。其中一个节点的展开树是通过在一定深度上迭代展开节点的邻居节点来构造的。）Xu 等人（2019）[20] 表明，消息传递框架下的 ConvGNNs 不是定义在多级上的连续函数的通用近似。Maron 等人(20190[44] 证明了不变图网络可以近似图的任意不变函数。 6. 图自编码器&emsp;&emsp;GAEs 是将节点映射到潜在特征空间，并从潜在表示解码图信息的深层神经网络。GAEs 可用于学习网络嵌入或生成新的图。 A. 网络嵌入&emsp;&emsp;网络嵌入是保留节点拓扑信息的低维向量表示。GAEs 使用编码器提取网络嵌入，使用解码器保留图的拓扑信息（如 PPMI 矩阵和邻接矩阵）来学习网络嵌入。早期的方法主要使用多层感知机构建 GAE 来学习网络嵌入。DNGRs（Deep nerual networks for graph representations）（2016）[47] 使用多层去噪编码器来编码，并通过多层感知机对 PPMI 矩阵解码。同时，SDNE（structural deep network embedding）（2016）[48] 使用多层自编码器联合保持节点的一阶近似度和二阶近似度。SDNE 分别在编码器和解码器的输出端使用两个损失函数。第一个损失函数通过最小化节点与其邻居的网络嵌入之间的距离，使学习到的网络嵌入保持节点的一阶相似性。该损失函数定义为： &emsp;&emsp;其中，$\\mathbf xv = \\mathbf A{v,:}$，$enc(·)$ 是由多层感知机构成的编码器。第二个损失函数通过最小化节点输入与其重构输入之间的距离，使学习到的网络嵌入保持节点的二阶近似。具体定义为： &emsp;&emsp;其中，如果 $A{v,u} = 0$，则$b{v, u} = 1$；如果 $A{v,u} = 1$，则$b{v, u} = \\beta &gt; 1$，$dec(·)$ 使由多层感知机构成的解码器。 &emsp;&emsp;DNGR 和 SDNE 只考虑了节点之间连通性的结构信息，忽略了可能包含描述节点自身属性的特征信息。GAE*（避免歧义）（2016）[49] 利用 GCN 同时对节点的结构信息和特征信息进行编码。其编码器由两个图卷积组成： &emsp;&emsp;其中，$\\mathbf Z$ 代表图的网络嵌入矩阵，$f(·)$ 是 ReLU 激活函数，$Gconv(·)$ 是 GCN 的图卷积层。GAE* 的解码器目的是通过重构图的邻接矩阵来解码节点嵌入到关系信息，定义为： &emsp;&emsp;其中，$\\mathbf z_v$ 是节点 $v$ 的嵌入。GAE* 通过最小化原邻接矩阵 $\\mathbf A$ 和重构的邻接矩阵 $\\hat {\\mathbf A}$ 之间的负交叉熵来训练。 &emsp;&emsp;由于自编码器的容量有限，简单地重建图邻接矩阵可能会导致过拟合。VGAE（Variational GAE）[49] 学习数据的分布优化变分下界 $L$： &emsp;&emsp;其中，$KL(·)$ 是 Kullback-Leibler 散度，用于衡量两个分布之间的距离，$p(\\mathbf Z)$ 是高斯先验，$p(\\mathbf Z) = \\prod^n{i=1}p(\\mathbf z_i) = \\prod^n{i=1}N(\\mathbf zi|0,\\mathbf I)$，$p(A{ij} = 1|\\mathbf zi, \\mathbf z_j) = \\mathrm {dec}(\\mathbf z_i, \\mathbf z_j) = \\sigma (\\mathbf z^T_i \\mathbf z_j)$，$q(\\mathbf {Z|X,A}) = \\prod^n{i=1}q(\\mathbf z_i|\\mathbf {X, A})$，其中 $q(\\mathbf z_i|\\mathbf {X,A}) = N(\\mathbf z_i|\\mu_i, diag(\\sigma^2_i))$。$\\mu_i$ 是编码器输出的第 i 行的平均向量，$\\mathrm {log}\\sigma_i$ 是与另一个向量的近似度。根据上式，VGAE 认为经验分布 $q(\\mathbf {Z|X,A})$ 应尽可能接近先验分布 $p(\\mathbf Z)$。为了进一步使经验分布 $q(\\mathbf {Z|X,A})$ 近似于先验分布 $p(\\mathbf Z)$，ARVGA（adversarially regularized VGAE）（2018）[50] 采用 GANs（generative adversarial networks）的方式进行训练。 &emsp;&emsp;与 GAE* 类似，GraphSage（2017）[21] 使用两个图卷积层对节点特征进行编码。GraphSage 表示，负采样可以保留两个节点之间俺的关系信息，其损失定义为： &emsp;&emsp;其中，节点 $u$ 是节点 $v$ 的邻居，节点 $v_n$ 是从负采样分布 $P_n(v)$ 采样的节点 $v$ 的远节点，$Q$ 是负样本的数量。该损失本质上相近的节点有相似的表示，远处的节点有不相似的表示。DGI（2019）[51] 通过最大化局部互信息使局部网络嵌入获取到全局结构信息，在实验结果上比 GraphSage 有明显的改进。 &emsp;&emsp;上述方法本质上是通过解决链路预测问题来学习网络嵌入。然而，图的稀疏性导致正节点对的数量远小于负节点对的数量。为了解决网络嵌入中数据稀疏的问题，有一些工作通过随机排列或随机游走将图转换为序列来处理，于是那些适用于序列的深度学习方法可以直接用于处理图数据。DRNE（Deep recursive network embedding）（2018）[52] 认为节点的网络嵌入应该近似于其邻居的网络嵌入的聚合，它使用 LSTM 来聚合节点的邻居。其重构损失定义为： &emsp;&emsp;其中，$\\mathbf z_v$ 是节点 $v$ 通过查字典得到的网络嵌入，LSTM 网络的输入为节点 $v$ 的邻居节点，并使用度来排序。如上式所示，DRNE 通过 LSTM 网络隐式地学习网络嵌入，而不是用来生成网络嵌入，这避免了 LSTM 网络对节点序列的排序不变性的问题。NetRAs（Network representations with adversarially regularized autoencoders）（2018）[53] 提出了一种更具一般性的损失函数： &emsp;&emsp;其中，dist(·) 衡量节点及其重构节点嵌入之间的距离。NetRA 的编码器和解码器都是 LSTM，每个节点作为根节点随机游走作为输入。与 ARVGA 类似，NetRA 通过对抗训练将学习到的网络嵌入规则化到先验分布，实验结果证明了其有效性。 B. 图生成&emsp;&emsp;对于多个图，GAEs 能够通过将图编码为隐藏表示，并将隐藏表示解码为图结构来学习图的一般分布。大多数用于图生成的 GAEs 是为了解决分子图生成问题而设计的，在药物发现中具有很高的实用价值。 &emsp;&emsp;序列（Sequential）方法通过逐步生成节点和边来生成图。Gómez-Bombarelli 等人（2018）[54]，Kusner 等人（2017）[55]，Dai 等人（2018）[56] 分别用深度 CNNs 和 RNNs 作为编码器和解码器，构建了字符串表示的分子图生成模型。虽然这些方式是特定领域的，但也可应用于一般图中。DeepGMG（Deep generative model of graphs）（2018）[57] 认为图的概率是所有可能的节点置换的总和： &emsp;&emsp;其中，$\\pi$ 代表节点顺序。它提取了图中所有节点和边的复杂联合概率。DeepGMG 通过一系列决策生成图，包括是否添加节点、添加哪个节点、是否添加边、连接到哪个节点。生成节点和边的决策过程取决于由 RecGNN 更新的节点状态和图状态。在另一个工作中，GraphRNN（2018）[58] 提出了一个图级别 RNN 和一个边级别 RNN 来建模节点和边的生成过程。图级别 RNN 每次向节点序列添加一个新的节点，边级别 RNN 则生成一个二进制序列，表示新节点与之前生成的节点之间的连接。 &emsp;&emsp;全局（Global）方法一次输出整个图。GraphVAE（Graph variational autoencoder）（2018）[59] 将节点和边建模成独立的随机变量。假设编码器得到的后验分布为 $q(\\mathbf z|G)$ ，解码器生成的分布为 $p_\\theta(G|\\mathbf z)$ ，GraphVAE 优化了变分下界： &emsp;&emsp;其中，$p(\\mathbf z)$ 遵循高斯先验，$\\phi, \\theta$ 是可学习的参数。GraphVAE 使用 ConvGNN 作为编码器，使用简单的多层感知机作为解码器，输出生成的图及其邻接矩阵、节点属性和边属性。控制生成图的全局属性，如图的连通性（connectivity）、有效性（validity）和节点兼容性（node compatibility）是一项挑战。RGVAE（Regularized GraphVAE）（2018）[60] 进一步对 GraphVAE 施加有效性约束以调节解码器的输出分布。MolGAN（Molecular GAN）（2018）[61] 结合了 convGNNs（2018）[62]、GANs（2017）[63] 和强化学习目标来生成具有所需属性的图。在 MolGAN 中，生成器生成伪（fake）图，鉴别器鉴定伪样本，同时奖励（reward）网络根据评估器（evaluator）促进生成的图具有某些属性。NetGAN（2018）[64] 结合 LSTMs 与 Wasserstein GANs（2017）[65]，基于随机游走的方法生成图。NetGAN 通过 LSTM 训练生成器以生成合理的随机游走，并使用判别器鉴定随机游走的真伪。经过训练，可以通过生成器随机游走计算的规范化共现矩阵导出生成图。 &emsp;&emsp;简言之，序列方法将图线性化为序列，由于环的存在可能会丢失一些结构信息。全局方法一次生成一个图形，这种方法不能扩展到大型图，因为 GAE 的输出空间高达 $O(n^2)$。 7. 时空图神经网络&emsp;&emsp;在许多实际应用中，图的结构和输入是动态的。STGNNs 是处理动态图很重要的方法。这类方法旨在对动态节点输入进行建模，同时假设相连的节点之间存在相互依赖关系。例如，交通网络由放置在路上的速度传感器组成，边的权重由传感器之间的距离确定，由于道路的交通情况可能取决于其临近道路的情况，因此在预测交通速度时有必要考虑空间相关性。STGNNs 可以同时捕获图的空间和时间依赖关系，用来预测未来节点的值或标签，或预测时空图的标签。STGNNs 包括基于 RNN 的和基于 CNN 的两种方法。 &emsp;&emsp;大多数基于 RNN 的方法通过使用图卷积过滤传递给循环单元的输入和隐藏状态来捕获时空依赖性。假设一个简单的 RNN 表示为： &emsp;&emsp;其中，$\\mathbf X^{(t)} \\in \\mathbf R^{n \\times d}$ 是在时间步 t 时的节点特征矩阵。在加入图卷积后，上式变为： &emsp;&emsp;其中，Gconv(·) 是图卷积层。GCRN（2018）[66] 将 ChebNet 与 LSTM 结合；DCRNN（ Diffusion convolutional RNN）（2018）[67] 将扩散图卷积层合并到 GRU 中，并采用编码器-解码器的框架来预测未来 K 步节点的值。同时，一些工作中节点的时间信息和边的时间信息分别通过节点级别 RNNs 和边级别 RNNs 传递。为了整合空间信息，节点 RNN 将边 RNN 的输出作为输入。由于对不同的节点使用不同的 RNNs 会显著增加模型复杂度，因此它将节点和边拆分为语义组。同一语义组的节点或边共享相同的 RNN 模型，以此来降低计算成本。 &emsp;&emsp;基于 RNN 的方法存在梯度爆炸和梯度消失以及计算耗时的问题。基于 GNN 的方法以非递归的方式处理时空图，具有并行计算、梯度稳定、内存占用少的优势。基于 CNN 的方法将一维卷积层和图卷积层交错使用来学习时空依赖关系。CGCN 集成了 一维卷积层与 ChebNet（或 GCN 层），通过门控一维卷积层、图卷积层和另一个门控一维卷积层搭建了时空块（spatial-temporal block）。ST-GCN（2018）[68] 使用一个一维卷积层和一个 PGC 层 [36] 搭建了时空块。 &emsp;&emsp;前面的方法都使用预定义的图结构，认为预定义的图结构反映了节点之间的真正的依赖关系。然而，由于图数据在时空设置中有许多快照（snapshot），因此可以从数据中自动学习潜在的静态图结构。为实现这一点，Graph WaveNet（2019）[69] 提出了一种自适应邻接矩阵进行图卷积。自适应邻接矩阵定义为： &emsp;&emsp;其中，SoftMax 方法沿行方向计算，$\\mathbf {E1}$ 代表源节点嵌入，$\\mathbf {E2}$ 代表目标节点嵌入。通过将 $\\mathbf {E1}$ 与 $\\mathbf {E2}$ 相乘，可以得到源节点和目标节点之间的依赖性权重。Graph Wavenet 采用复杂的基于 CNN 的时空神经网络，无需给定邻接矩阵就有良好的表现。 &emsp;&emsp;学习潜在的静态空间依赖关系有助于发现网络中不同实体之间的可解释且稳定的相关性。然而，在某些情况下，学习潜在的动态空间依赖关系可能进一步提高模型精度。例如，在交通网络中，在两条道路的行驶时间可能取决于它们当前的交通状况。GaAN（2018）[70] 采用注意力机制，通过基于 RNN 的方法学习动态空间依赖性。给定两个相连节点输入，使用注意力函数更新两个节点之间的边权重。ASTGCN（2019）[71] 使用空间注意力函数和时间注意力函数，通过基于 CNN 的方法学习潜在的动态空间依赖和时间依赖，时间复杂度为 $O(n^2)$。 8. 应用&emsp;&emsp;图结构数据无处不在，GNNs 有着广泛的应用。 A. 数据集&emsp;&emsp;这里主要列举了四个方面的数据集：引用网络、生化图、社交网络以及其他。 B. 评价方法&emsp;&emsp;节点分类和图分类是评估 RecGNNs 和 ConvGNNs 性能的常见任务。 节点分类：节点分类的大多数方法遵循 benchmark 数据集（如 Cora、Citeseer、Pubmed、PPI 和 Reddit）的标准的 train/valid/test 的划分方法。通常使用多次测试的平均准确率（accuracy）或 F1 分数。更多信息请参考 [72]。 图分类：通常采用十倍交叉验证进行模型评估。然而，在不同的工作中实验设置不统一。更多信息请参考 [73]。 C. 实际应用 计算机视觉：场景图生成、点云分类、动作识别。 识别对象之间的语义关系有助于理解视觉场景背后的含义。场景图生成模型旨在将图像解析为由对象及其语义关系组成的语义图。相反地，由于自然语言可以被解析为语义图，其中每个单词表是一个对象，因此根据给定文本描述合成图像也是一个很有前途的解决方案。 点云是由激光雷达设备扫描到的三维点集。将点云转换为 k-近邻图或超点图，可使用 ConvGNNs 来探索其拓扑结构。 识别视频中人的行为有助于让机器理解视频内容。一些解决方案检测视频帧中人体相关位置，由骨骼连接的人体关节形成图，然后根据人关节位置的时空序列应用 STGNNs 来学习人类行为模式。 另外，GNNs 在计算机视觉中的应用在不断增加，包括人机交互、少样本图像分类、语义分割、视觉推理和问答等。 自然语言处理：常见的应用是文本分类，GNNs 利用文本或单词的相互关系来推断文本类别。尽管自然语言数据往往是序列形式的，但内部可能包含图结构，如句法依赖树。 交通：准确预测交通网络中的交通速度、交通量或道路密度对于智能交通系统至关重要。 推荐系统：基于图的推荐系统将用户和商品作为节点，利用节点之间、商品之间、节点和商品之间以及节点内容信息，生成高质量的推荐。 化学：在化学领域，GNNs 用于研究分子、化合物的图结构。在分子、化合物图中，原子被视为节点，化学键被视为边。节点分类、图分类和图生成是针对分子、化合物的三个重要任务，目的是学习分子指纹（molecular fingerprints）、预测分子特性、推断蛋白质结构以及合成化合物。 其他：GNNs 不限于以上应用。在程序验证、程序推理、社会影响预测、对抗性攻击预防、电子健康记录建模、大脑网络、事件检测和组合优化上都有人探索。 9. 未来方向A. 模型深度&emsp;&emsp;深度学习的成功在于深度的神经网络结构。然而，Li 等人（2018）[74] 表明，随着图卷积层数量的增加，ConvGNNs 的性能急剧下降。由于图卷积使相邻节点的表示更接近，在理论上，经过无限多的图卷积层，所有结点的表示都将收敛到一个值。于是，更深的网络结构是否是学习图数据的好的策略呢，这是一个问题。 B. 权衡可扩展性&emsp;&emsp;GNNs 的可扩展性以破坏图的完整性为代价，如论是使用采样还是聚类，模型都会丢失部分图信息。通过采样，节点可能会丢失部分邻居；通过聚类，图可能缺少了不同的结构模式。如何权衡算法的可扩展性和图的完整性可能是未来的研究方向。 C. 异质性&emsp;&emsp;当前的大多数 GNNs 都采用同质图，因此很难将这些方法应用于异构图。异构图是指可能包含不同节点类型和边类型，或者不同形式的节点和边作为输入（如图和文本）的图。因此，应该开发新的方法来处理异构图。 D. 动态性&emsp;&emsp;图的输入可能随时间发生变化，需要新的卷积来适应图的动态性，虽然图的动态性可以部分地由 STGNNs 来解决，但很少有人考虑在动态空间关系的情况下如何执行图卷积。 参考文献[1] A. Sperduti and A. Starita, “Supervised neural networks for the classification of structures,” IEEE Trans. Neural Netw., vol. 8, no. 3, pp. 714–735, May 1997. [2] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and locally connected networks on graphs,” in Proc. ICLR, 2014, pp. 1–14. [3] T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based neural machine translation,” in Proc. Conf. Empirical Methods Natural Lang. Process., 2015, pp. 1412–1421. [4] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, “The graph neural network model,” IEEE Trans. Neural Netw., vol. 20, no. 1, pp. 61–80, Jan. 2009. [5] C. Gallicchio and A. Micheli, “Graph echo state networks,” in Proc. Int. Joint Conf. Neural Netw. (IJCNN), Jul. 2010, pp. 1–8. [6] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, “Gated graph sequence neural networks,” in Proc. ICLR, 2015, pp. 1–20. [7] H. Dai, Z. Kozareva, B. Dai, A. Smola, and L. Song, “Learning steady-states of iterative algorithms over graphs,” in Proc. ICML, 2018, pp. 1114–1122. [8] T. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” in Proc. ICLR, 2017, pp. 1–14. [9] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and locally connected networks on graphs,” in Proc. ICLR, 2014, pp. 1–14. [10] M. Defferrard, X. Bresson, and P. Van der Gheynst, “Convolutional neural networks on graphs with fast localized spectral filtering,” in Proc. NIPS, 2016, pp. 3844–3852. [11] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein, “CayleyNets: Graph convolutional neural networks with complex rational spectral filters,” IEEE Trans. Signal Process., vol. 67, no. 1, pp. 97–109, Jan. 2019. [12] R. Li, S. Wang, F. Zhu, and J. Huang, “Adaptive graph convolutional neural networks,” in Proc. AAAI, 2018, pp. 3546–3553. [13] C. Zhuang and Q. Ma, “Dual graph convolutional networks for graph-based semi-supervised classification,” in Proc. World Wide Web Conf. World Wide Web (WWW), 2018, pp. 499–508. [14] A. Micheli, “Neural network for graphs: A contextual constructive approach,” IEEE Trans. Neural Netw., vol. 20, no. 3, pp. 498–511, Mar. 2009. [15] D. Bacciu, F. Errica, and A. Micheli, “Contextual graph Markov model: A deep and generative approach to graph processing,” in Proc. ICML, 2018, pp. 1–10. [16] J. Atwood and D. Towsley, “Diffusion-convolutional neural networks,” in Proc. NIPS, 2016, pp. 1993–2001. [17] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional recurrent neural network: Data-driven traffic forecasting,” in Proc. ICLR, 2018, pp. 1–16. [18] D. V. Tran, N. Navarin, and A. Sperduti, “On filter size in graph convolutional networks,” in Proc. IEEE Symp. Ser. Comput. Intell. (SSCI), Nov. 2018, pp. 1534–1541. [19] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, “Neural message passing for quantum chemistry,” in Proc. ICML, 2017, pp. 1263–1272. [20] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural networks,” in Proc. ICLR, 2019, pp. 1–17. [21] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” in Proc. NIPS, 2017, pp. 1024–1034. [22] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, “Graph attention networks,” in Proc. ICLR, 2017, pp. 1–12. [23] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D.-Y. Yeung, “GaAN: Gated attention networks for learning on large and spatiotemporal graphs,” in Proc. UAI, 2018, pp. 1–10. [24] Z. Liu et al., “GeniePath: Graph neural networks with adaptive receptive paths,” in Proc. AAAI Conf. Artif. Intell., Jul. 2019, pp. 4424–4431. [25] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein, “Geometric deep learning on graphs and manifolds using mixture model CNNs,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 5115–5124. [26] J. Masci, D. Boscaini, M. M. Bronstein, and P. Vandergheynst, “Geodesic convolutional neural networks on Riemannian manifolds,” in Proc. IEEE Int. Conf. Comput. Vis. Workshop (ICCVW), Dec. 2015, pp. 37–45. [27] D. Boscaini, J. Masci, E. Rodolà, and M. Bronstein, “Learning shape correspondence with anisotropic convolutional neural networks,” in Proc. NIPS, 2016, pp. 3189–3197. [28] M. Fey, J. E. Lenssen, F. Weichert, and H. Müller, “SplineCNN: Fast geometric deep learning with continuous B-spline kernels,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 869–877. [29] M. Niepert, M. Ahmed, and K. Kutzkov, “Learning convolutional neural networks for graphs,” in Proc. ICML, 2016, pp. 2014–2023. [30] H. Gao, Z. Wang, and S. Ji, “Large-scale learnable graph convolutional networks,” in Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Aug. 2018, pp. 1416–1424. [31] J. Chen, T. Ma, and C. Xiao, “FastGCN: Fast learning with graph convolutional networks via importance sampling,” in Proc. ICLR, 2018, pp. 1–15. [32] W. Huang, T. Zhang, Y. Rong, and J. Huang, “Adaptive sampling towards fast graph representation learning,” in Proc. NeurIPS, 2018, pp. 4563–4572. [33] J. Chen, J. Zhu, and L. Song, “Stochastic training of graph convolutional networks with variance reduction,” in Proc. ICML, 2018, pp. 941–949. [34] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh, “Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks,” in Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD), 2019, pp. 257–266. [35] I. S. Dhillon, Y. Guan, and B. Kulis, “Weighted graph cuts without eigenvectors a multilevel approach,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 11, pp. 1944–1957, Nov. 2007. [36] M. Henaff, J. Bruna, and Y. LeCun, “Deep convolutional networks on graph-structured data,” 2015, arXiv:1506.05163. [Online]. Available: http://arxiv.org/abs/1506.05163 [37] O. Vinyals, S. Bengio, and M. Kudlur, “Order matters: Sequence to sequence for sets,” in Proc. ICLR, 2016, pp. 1–11. [38] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, “An end-to-end deep learning architecture for graph classification,” in Proc. AAAI, 2018, pp. 1–8. [39] Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec, “Hierarchical graph representation learning with differentiable pooling,” in Proc. NeurIPS, 2018, pp. 4801–4811. [40] J. Lee, I. Lee, and J. Kang, “Self-attention graph pooling,” in Proc. ICML, 2019, pp. 3734–3743. [41] A. Micheli, “Neural network for graphs: A contextual constructive approach,” IEEE Trans. Neural Netw., vol. 20, no. 3, pp. 498–511, Mar. 2009. [42] F. Scarselli, A. C. Tsoi, and M. Hagenbuchner, “The Vapnik– Chervonenkis dimension of graph and recursive neural networks,” Neural Netw., vol. 108, pp. 248–259, Dec. 2018. [43] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural networks,” in Proc. ICLR, 2019, pp. 1–17. [44] H. Maron, H. Ben-Hamu, N. Shamir, and Y. Lipman, “Invariant and equivariant graph networks,” in ICLR, 2019, pp. 1–14. [45] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward networks are universal approximators,” Neural Netw., vol. 2, no. 5, pp. 359–366, Jan. 1989. [46] B. Hammer, A. Micheli, and A. Sperduti, “Universal approximation capability of cascade correlation for structures,” Neural Comput., vol. 17, no. 5, pp. 1109–1159, May 2005. [47] S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations,” in Proc. AAAI, 2016, pp. 1145–1152. [48] D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD), 2016, pp. 1225–1234. [49] T. N. Kipf and M. Welling, “Variational graph auto-encoders,” in Proc. NIPS Workshop Bayesian Deep Learn., 2016, pp. 1–3. [50] S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, “Adversarially regularized graph autoencoder for graph embedding,” in Proc. IJCAI, Jul. 2018, pp. 2609–2615. [51] P. Veliˇckovi´c, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D. Hjelm, “Deep graph infomax,” in Proc. ICLR, 2019, pp. 1–17. [52] K. Tu, P. Cui, X. Wang, P. S. Yu, and W. Zhu, “Deep recursive network embedding with regular equivalence,” in Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Aug. 2018, pp. 2357–2366. [53] W. Yu et al., “Learning deep network representations with adversarially regularized autoencoders,” in Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Aug. 2018, pp. 2663–2671. [54] R. Gómez-Bombarelli et al., “Automatic chemical design using a data-driven continuous representation of molecules,” ACS Central Sci., vol. 4, no. 2, pp. 268–276, Jan. 2018. [55] M. J. Kusner, B. Paige, and J. M. Hernández-Lobato, “Grammar variational autoencoder,” in Proc. ICML, 2017, pp. 1945–1954. [56] H. Dai, Y. Tian, B. Dai, S. Skiena, and L. Song, “Syntax-directed variational autoencoder for structured data,” in Proc. ICLR, 2018, pp. 1–17. [57] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, “Learning deep generative models of graphs,” in Proc. ICML, 2018, pp. 1–21. [58] J. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec, “GraphRNN: A deep generative model for graphs,” in Proc. ICML, 2018, pp. 1–12. [59] M. Simonovsky and N. Komodakis, “Graphvae: Towards generation of small graphs using variational autoencoders,” in Proc. ICANN. Cham, Switzerland: Springer, 2018, pp. 412–422. [60] T. Ma, J. Chen, and C. Xiao, “Constrained generation of semantically valid graphs via regularizing variational autoencoders,” in Proc. NeurIPS, 2018, pp. 7110–7121. [61] N. De Cao and T. Kipf, “MolGAN: An implicit generative model for small molecular graphs,” ICML Workshop Theor. Found. Appl. Deep Generative Models, 2018, pp. 1–11. [62] M. Schlichtkrull, T. N. Kipf, P. Bloem, R. van den Berg, I. Titov, and M. Welling, “Modeling relational data with graph convolutional networks,” in ESWC. Cham, Switzerland: Springer, 2018, pp. 593–607. [63] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, “Improved training of Wasserstein GANs,” in Proc. NIPS, 2017, pp. 5767–5777. [64] A. Bojchevski, O. Shchur, D. Zügner, and S. Günnemann, “NetGAN: Generating graphs via random walks,” in Proc. ICML, 2018, pp. 1–16. [65] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein GAN,” 2017, arXiv:1701.07875. [Online]. Available: http://arxiv.org/abs/1701.07875 [66] Y. Seo, M. Defferrard, P. Vandergheynst, and X. Bresson, “Structured sequence modeling with graph convolutional recurrent networks,” in Proc. NeurIPS. Springer, 2018, pp. 362–373. [67] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional recurrent neural network: Data-driven traffic forecasting,” in Proc. ICLR, 2018, pp. 1–16. [68] S. Yan, Y. Xiong, and D. Lin, “Spatial temporal graph convolutional networks for skeleton-based action recognition,” in Proc. AAAI, 2018, pp. 1–9. [69] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, “Graph WaveNet for deep spatial-temporal graph modeling,” in Proc. IJCAI, Aug. 2019, pp. 1–7. [70] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D.-Y. Yeung, “GaAN: Gated attention networks for learning on large and spatiotemporal graphs,” in Proc. UAI, 2018, pp. 1–10. [71] S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan, “Attention based spatial-temporal graph convolutional networks for traffic flow forecasting,” in Proc. AAAI, 2019, pp. 922–929. [72] O. Shchur, M. Mumme, A. Bojchevski, and S. Günnemann, “Pitfalls of graph neural network evaluation,” in Proc. NeurIPS workshop, 2018, pp. 1–11. [73] F. Errica, M. Podda, D. Bacciu, and A. Micheli, “A fair comparison of graph neural networks for graph classification,” in Proc. ICLR, 2020, pp. 1–15. [Online]. Available: https://openreview. net/forum?id=HygDF6NFPB [74] Q. Li, Z. Han, and X.-M. Wu, “Deeper insights into graph convolutional networks for semi-supervised learning,” in Proc. AAAI, 2018, pp. 1–8.","link":"2021/12/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%BB%BC%E8%BF%B0/"},{"title":"概率与统计","text":"一、古典概型和概率空间1.1 试验与事件试验 我们用 概率 测量一个未来事件发生的可 能性大小. 我们把按照一定的想法去作的事情称为随机试验. 随机试验的简称是 试验 (experiment). 样本空间 投掷一枚硬币, 用 $ω+$ 表示硬币正面朝上, 用 $ω−$ 表示硬币反面朝上, 则试验有两个可能的结果：$ω+$ 和 $ω−$. 我们称 $ω+$ 和 $ω−$ 是样本点, 称样本点的集合 $Ω = {ω+, ω−}$ 为试验的样本空间. 样本点 (sample point): 称试验 S 的可能结果为样本点, 用 ω 表示. 样本空间 (sample space): 称试验 S 的样本点构成的集合为样本空间, 用 Ω 表示. 事件 事件 (event): 设 Ω 是试验 S 的样本空间. 当 Ω 中只有有限个样本点时, 称 Ω 的子集为事件. 当试验的样本点 (试验结果) ω 落在 A 中, 称事件 A 发生, 否则称 A 不发生. 按照上述约定, 子集符号 A ⊂ Ω 表示 A 是事件. 通常用大写字母 A, B, C, D 或 A1, A2, · · · , B1, B2, · · · 等表示事件. 空集 ϕ 是 Ω 的子集. 由于 ϕ 中没有样本点, 永远不会发生, 所以称 ϕ 是 不可能事件. Ω 也是样本空间 Ω 的子集, 包含了所有的样本点, 因而总会发生. 我们称 Ω 是 必然事件. 事件与集合 当 A, B 都是事件, 则 ​ $A ∪ B, A ∩ B, A − B \\triangleq A ∩ \\bar B$ 都是事件. 也就是说事件经过集合运算得到的结果还是事件. 我们也用 AB 表示 A ∩ B. 当 AB = ϕ 时, 也用 A + B 表示 A ∪ B. 当事件 AB = ϕ, 称事件 A, B 不相容. 特别称 $\\bar A$ 为 A 的 对立事件 或 逆事件. 如果多个事件 $A1, A2, . . .$ 两两不相容: $A_iA_j = ϕ, i \\neq j$, 就称他们 互不相容. A = B 表示事件 A, B 相等 A ∪ B (或 A + B) 发生 等价于 至少 A, B 之一发生 A ∩ B (或 AB) 发生 等价于 A 和 B 都发生 事件的运算 A ∪ B = B ∪ A , A ∩ B = B ∩ A A ∪ (B ∪ C) = A ∪ B ∪ C, A ∩ (B ∩ C) = A ∩ B ∩ C A(B ∪ C) = (AB) ∪ (AC), A ∪ (B ∩ C) = (A ∪ B) ∩ (A ∪ C) $A ∪ B = A + \\bar AB, A = AB + A \\bar B$ 对偶公式: $\\overline {A \\cup B} = \\bar A ∩ \\bar B, \\overline {A ∩ B} = \\bar A ∪ \\bar B$ 1.2 古典概型与集合概型1.2.1 古典概型 古典概率模型 设 Ω 是试验 S 的样本空间. 对于 Ω 的事件 A, 我们用 P(A) 表示 A 发生的可能性的大小, 称 P(A) 是事件 A 发生的概率, 简称为 A 的概率. 概率是介于 0 和 1 之间的数, 描述事件发生的可能性的大小. 即 $P(A) \\in [0, 1]$ 按照以上原则, 如果事件 A, B 发生的可能性相同, 则有 P(A) = P(B). 如果事件 A 发生的可能性比 B 发生的可能性大 2 倍, 则有 P(A) = 2P(B). 定义 2.1：设试验 S 的样本空间 Ω 是有限集合, A ⊂ Ω. 如果 Ω 的每个样本点发生的可能性相同, 则称 ​ $P(A) = \\dfrac{^#A}{^#\\Omega}$ 为试验 S 下 A 发生的概率, 简称为事件 A 的概率. 能够用以上定义描述的模型称为 古典概率模型, 简称为 古典概型. 概率的性质 因为 $^#A ≥ 0$, 当 AB = ϕ 时，$^#(A+B) = ^#A + ^#B$, 所以从定义 (2.1) 可以得到概率 P 的以下性质: P(A) ≥ 0 P(Ω) = 1 如果 A, B 不相容, 则 P(A + B) = P(A) + P(B) 如果 A1, A2, · · · , An 互不相容, 则 P(A1 + A2 + · · · + An) = P(A1) + P(A2) + · · · + P(An) $P(ϕ) = 0, P(A) + P(A) = 1, P(A) = P(AB) + P(A \\bar B).$ 古典概型中的常用计数—加法原理 如果一个问题的做法分为两类，第一类有 n 种方法，第二类有 m 种 方法，这两类没有重叠而且仅有此两类，则问题的做法共有 n + m 种。 古典概型中的常用计数—乘法原理 如果一个问题要两步完成，第一步有 n 种做法，第二步有 m 种做法， 则问题有 nm 种做法。 古典概型中常用计数—有重复的排列数 从 n 个不同元素中有放回地每次随机抽取一个，共抽取 m 次，有序 地记录结果，共有 $n^m$ 种等可能的不同结果。 古典概型中常用计数—排列数 从 n 个不同元素中无放回地每次随机抽取一个，共抽取 m 次 (m ≤ n), 有序地记录结果，共有 ​ $A^m_n = n(n-1)…(n-m+1) = \\dfrac{n!}{(n-m)!}$ 种等可能的不同结果。 古典概型中常用计数—组合数 从 n 个不同元素中无放回地每次抽取一个，共抽取 m 次 (m ≤ n), 不计次序地记录结果（只要元素相同，不管次序是否相同都算是相同结果），共有 ​ $C^m_n = \\dfrac{n \\times (n-1) \\times … \\times (n-m+1)}{m \\times (m-1) \\times … \\times 1} = \\dfrac{A^m_n}{m!} = \\frac{n!}{m!(n-m)!}$ 种等可能的不同结果。 排列组合中的 A 与 C： 不区分个体差异和顺序时用组合 $C^m_n$, 区分个体差异和顺序时用排列 $A^m_n$. $A^m_n = n(n-1)…(n-m+1) = \\dfrac{n!}{(n-m)!}$ $C^m_n = \\dfrac{n \\times (n-1) \\times … \\times (n-m+1)}{m \\times (m-1) \\times … \\times 1} = \\dfrac{A^m_n}{m!} = \\frac{n!}{m!(n-m)!}$ 古典概型中常用计数—分组方式数 将 n 个不同元素分成有序号的 k 组，要求第 i 组恰好有 ni 个元素 (i = 1, 2, . . . , k)，分组结果中同组的元素不考虑次序。则这样分组的所有不同分法个数为 ​ $\\dbinom{n}{n_1,n_2,…,n_k} = \\dfrac{n!}{n_1!n_2!…n_k!}$ 当随机分组时，这些分法是等可能的。 古典概型中常用计数—可重复分组数 从 n 个不同的球中有放回地每次抽取一个，共抽取 m 次，结果不计次序，共有 $C^m_{n+m−1}$ 种不同的组合。 可重复分组数在随机分组时一般不是等可能的。 1.2.2 几何概型 欧氏空间中的体积 用 $R^r$ 表示 r 维欧氏空间，对于 $R^r$ 的子集 $A$，用 $m(A) = \\intA dx_1dx_2…d{x_r}$ 表示 $A$ 的体积。 几何概率 设样本空间 Ω 的体积 m(Ω) 是正数，样本点等可能地落在 Ω 中 （指 Ω 的体积相同的长方体事件发生的可能性相同），对于 A ⊂ Ω，称 ​ $P(A) = \\frac{m(A)}{m(\\Omega)}$ 为事件 A 发生的概率，简称为 A 的概率。 1.3 概率的公理化和加法公式公理化条件 非负性：对于任何事件 A, P(A) ≥ 0 , 完全性：P(Ω) = 1, 可列可加性：对于互不相容的事件 A1, A2, . . . , 有 $P(\\cup^\\infty{j=1}) = \\sum^\\infty{j=1}P(A_j)$ 不满足公理化条件的 P 不是概率。 概率的加法公式 概率的有限可加性和可列可加性是概率 P 的最基本性质, 由此推出概率的加法公式. P(A ∪ B) = P(A) +P(B) −P(AB) 如果 B ⊂ A, 则 P(A−B) = P(A) −P(B), P(A) ≥ P(B) Jordan 公式: 设 A1, A2, · · · , An 是事件, 记 $pk = \\sum{1 \\le j1 &lt; j_2 &lt; … &lt; j_k \\le n} P(A{j1}A{j2}…A{jk})$ 时，有 $P(\\cup{i=1}^nAi) = \\sum{k=1}^n(-1)^{k-1}p_k$. 概率的连续性 如果 A1 ⊂ A2 ⊂ · · · , 就称事件序列 {Aj} ≡ {Aj | j = 1, 2, · · · } 是单 调增的. 如果 A1 ⊃ A2 ⊃ · · · , 就称事件序列 {Aj} 是单调减的. 我们把单调增序列和单调减序列统称为单调序列. 1.4 条件概率和乘法公式我们称 P(B|A) 是已知 A 发生的条件下, B 发生的概率, 简称为条件概率. 条件概率公式：如果 P(A) &gt; 0, 则 ​ $P(B|A) = \\frac{P(AB)}{P(A)}$ 乘法公式：设 A,B, A1, A2, . . . , An 是事件, 则 P(AB) = P(A)P(B|A) 当 $P(A1A_2 . . . A{n−1}) \\ne 0$, 有 $P(A1A_2 . . . A_n) = P(A_1)P(A_2|A_1) . . . P(A_n|A_1A_2 . . . A{n−1})$ 官员受贿问题 1.5 事件的独立性事件独立 设 A 是试验 S1 下的事件, B 是试验 S2 下的事件, 且 A 的发生与否不 影响 B 的发生. 用公式表述出来就是 P(B|A) = P(B). (设 P(A) &gt; 0) 再用乘法公式得到 P(AB) = P(A)P(B|A) = P(A)P(B). 此式表示事 件 A, B 相互独立, 不要求 P(A) &gt; 0. 定义 5.1 如果事件 A, B 满足 P(AB) = P(A)P(B), 就称 A, B 相互 独立, 简称为 A, B 独立 (independent). 不可能事件, 必然事件与任何事件独立. 这是因为P(ϕA) = P(ϕ)P(A) = 0, P(ΩA) = P(Ω)P(A) = P(A) 总成立. 当 0 &lt; P(A) &lt; 1 时, A, B 独立当且仅当 P(B|A) = P(B| ¯A) = P(B), 即 B 的概率不受已知 A 是否发生的影响。 如果试验 S1 和 S2 是独立进行的, 可以证明试验 S1 的事件和试验 S2 的事件是相互独立的. 定理 5.1 A, B 独立当且仅当 $\\bar A, B$ 独立. 多个事件相互独立 如果对任何 $1 ≤ j_1 &lt; j_2 &lt; · · · &lt; j_k ≤ n$, 有 ​ $P(A{j_1}A{j2} · · · A{jk}) = P(A{j1})P(A{j2}) . . . P(A{j_k})$ 则称事件 $A_1, A_2,…$ 相互独立。 高炮击中飞机问题 古董失手打破问题 1.6 全概率公式与 Bayes 公式全概率公式 定理 6.1 如果事件 $A1,A_2,…,A_n$ 互不相容，$B\\sub \\cup^n{j=1}A_j$，则 ​ $P(B)=\\sum^n_{j=1}P(A_j)P(B|A_j)$ 完备事件组 如果事件 $A1,A_2,…,A_n$ 互不相容，$\\cup^n{j=1}A_j=\\Omega$，则称 $A_1,A_2,…,A_n$ 是完备事件组，这时定理6.1对任何事件 B 成立。 抽签问题 n 个签中有 m 个标有“中”, 证明无放回依次随机抽签时, 第 j 次抽中的概率是 m/n. 一个班里共有 100 人，男生有 40 人，女生有 60 人，每次随机选取一人，则第 j 次选取的是男生的概率是 40 / 100 = 0.4. 敏感问题调查 赌徒破产模型 Bayes 公式 定理 6.2 如果事件 $A1,A_2,…,A_n$ 互不相容，$B\\sub \\cup^n{j=1}A_j$，则 $P(B)&gt;0$ 时，有 ​ $P(Aj|B)=\\frac{P(A_j)P(B|A_j)}{\\sum^n{i=1}P(A_j)P(B|A_i)}$, $i \\le j\\le n.$ 证明：由条件概率公式 $P(B|A) = \\frac{P(AB)}{P(A)}$ 和全概率公式 $P(B)=\\sum^n_{j=1}P(A_j)P(B|A_j)$ 得到 $P(Aj|B)=\\frac{P(A_jB)}{P(B)}= \\frac{P(A_j)P(B|A_j)}{\\sum^n{i=1}P(A_j)P(B|A_i)}$ 当 P(B) &gt; 0 时， ​ $P(A|B)=\\frac{P(A)P(B|A)}{P(A)P(B|A)+P(\\bar A)P(B|\\bar A)} = \\frac{P(A)P(B|A)}{P(B)}$ 疾病普查问题 吸烟与肺癌问题 1.7 概率与频率设 A 是试验 S 的事件. 在相同的条件下将试验 S 独立地重复 N 次, 我们称 $fN =\\frac{N次试验中 A 发生的次数}{N}$是 N 次独立重复试验中, 事件 A 发生的频率 (frequency). 理论和试验都证明, 当 N → ∞, fN 会收敛到一个数 P(A). 我们称 P(A) 为事件 A 在试验 S 下发生的概率, 简称为 A 的概率. 随机变量和概率分布2.1 随机变量古典概率模型认为样本空间的每个样本点发生的概率相同。频率学派则不然，通过观察随机变量来确定概率。 随机变量是随机现 象的最基本的数学模型，我们用随机变量的值表示随机试验的结果。 随机变量 X 是定义在样本空间 Ω 上的实值函数: 对每一个 样本点 ω, X(ω) 是一个实数. 通常将随机变量 X(ω) 简记为 X. 随机变量的事件 我们用 ${X ≤ x}$, 或更简单地用 $X ≤ x$ 表示事件 ${\\omega | X(\\omega) \\le x }$ 对于实数的集合 A, 我们用 {X ∈ A}, 或更简单地用 X ∈ A 表示事件 {ω |X(ω) ∈ A}. 2.2 离散型随机变量如果随机变量 X 只取有限个值 x1, x2, · · · , xn, 或可列个值 x1, x2, · · · , 就称 X 是离散型随机变量, 简称为离散随机变量 (discrete random variable)。 定义 2.2 设 X 是离散随机变量，称 $P(X=x_k)=p_k, k \\ge 1$ 为 X 的概率分布。称 {pk} 是概率分布列， 简称分布列。 设函数 f(x) 取值于 {x1, x2, …}，$f(x_k) = p_k$，称 f(x) 为随机变量 X 的概率质量函数（PMF, probability mass function）。 分布列有如下性质： $p_k \\ge 0$ $\\sum^\\infin_{j=1}p_j=1$ 两点分布（Bernoulli 分布） 如果 X 只能取值 0 或 1，概率分布是 ​ $P(X=1)=p, P(X=0)=q, p+q=1$ 就称 X 服从两点分布，记作 $X \\sim B(1,p)$ 或 $X \\sim b(1,p)$. 二项分布（Binomial 分布） 如果随机变量 X 有如下的概率分布： ​ $P(X=k)=C^k_np^kq^{n-k}, k=0,1,2,…,n$ 其中，$pq&gt;0, p+q=1$，就称 X 服从二项分布，记作 $X \\sim B(n,p)$. 泊松分布（Poisson 分布） 如果随机变量 X 有如下的概率分布： ​ $P(X=k)=\\frac{\\lambda^k}{k!}e^{-\\lambda}, k=0,1,…$ 就称 X 服从参数是 $\\lambda$ 的 Poisson 分布，记作 $X \\sim Poisson(\\lambda)$，这里 $\\lambda$ 是正常数。 Poisson 分布的例子： 单位时间放射性粒子个数 某段高速公路上一年的事故数 某商场一天中顾客到来个数 一段时间内接到的电话个数 等等 超几何分布 H(n, M, N) 如果 X 的概率分布是 ​ $P(X=m)=\\frac{C^mMC^{n-m}{N-M}}{C^n_N}, m=0,1,…,M$ 就称 X 服从超几何分布，记作 $H(n,M,N)$. N 件产品中恰有 M 件次品, 从中任取 n 件, 用 X 表示这 n 件中的次品数, 则 X 服从超几何分布 几何分布 如果随机变量 X 有如下的分布： ​ $P(X=k)=q^{k-1}p, k=1,2,…$ ​ $pq&gt;0,p+q=1$ 就称 X 服从参数是 p 的几何分布。 设某试验成功概率为 p，独立地重复此试验直到第一次成功，则第一 次成功需要的试验次数分布为参数 p 的几何分布。 2.3 连续性随机变量在线段上随机投点的位置，温度、气压、电压、电流等物理量等等，理 论上可以在取到某个区间任何实数值。这样取值的随机变量称为连续 型随机变量。 概率密度函数 设 X 是随机变量, 如果存在非负函数 f(x) 使得对任何满 足 −∞ ≤ a &lt; b ≤ ∞ 的 a, b, 有 ​ $P(a&lt;X \\le b)=\\int_a^bf(x)dx$ 就称 X 是连续型随机变量，称 f(x) 是 X 的概率密度函数，简称概率密度 (probability density) 或密度。 概率密度 f(x) 有如下的基本性质： $\\int_{-\\infin}^{\\infin}f(x)dx=1$ $P(X=a)=0$，于是 $P(a&lt;X\\le b)=P(a\\le X &lt; b)$ 对数集 A，$P(X\\in A)=\\int_Af(x)dx$ 连续型随机变量取任何一个特定值的概率都等于零; f(x) 是一个相对的概念，如果 f(x2) = 2f(x1)，可以认为 X 在 x2“附近”取值的概 率比 X 在 x1 附近取值的概率大一倍. 均匀分布（Uniform 分布） 对于 a &lt; b，如果 X 的密度是 ​ $f(x)=\\begin{cases}\\frac{1}{b-a},&amp;x\\in(a,b),\\0,&amp;x\\notin(a,b).\\end{cases}$ 就称 X 服从区间 (a, b) 上的均匀分布，记作 $X \\sim U(a,b)$. 指数分布（Exponential 分布） 对正常数 λ，如果 X 的密度是 ​ $f(x)=\\begin{cases}\\lambda e^{-\\lambda x},&amp;x\\ge 0,\\0,&amp;x&lt;0,\\end{cases}$ 就称 X 服从参数 λ 的指数分布，记作 $X \\sim E(\\lambda)$. 指数分布经常用来表示电子元件寿命、事件到来间隔时间等。这样的量经常具有“无后效性”，即已经存活 (等待) 了多长时间对还会再存 活 (等待) 多长时间没有影响。 定理 3.1 设 X 是连续型非负随机变量, 则 X 服从指数分布的充分必要条件是对任何 s, t ≥ 0, 有 ​ $P(X&gt;s+t|X&gt;s)=P(X&gt;t)$ 则称随机变量 X 有无后效性。无后效性是指数分布的特征. 如果 X 表示某仪器的工作寿命, 无后效性的解释是: 当仪器工 作了 s 小时后再能继续工作 t 小时的概率等于该仪器刚开始就能工作 t 小时的概率. 说明该仪器的使用寿命不随使用时间的增加发生变化, 或说仪器是“永葆青春”的. 正态分布（Normal 分布） 设 μ 是常数，σ 是正常数，如果 X 的密度是 ​ $f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2}),x\\in R$ 就称 X 服从参数为 $(\\mu,\\sigma^2)$ 的正态分布，记作 $X \\sim N(\\mu,\\sigma^2)$. 特别，当 $X \\sim N(0,1)$ 时，称 X 服从标准正态分布，记作： ​ $\\varphi(x)=\\frac{1}{\\sqrt{2\\pi}}exp(-\\frac{x^2}{2}), x \\in R$ 特点 参数 µ 是密度的中心和最大值点，密度在 µ 两侧对称 参数 σ 代表了密度的宽度，σ 越大密度越宽 正态分布的随机变量 X 具有大部分值靠近 µ 的特点 (经验规则): Pr(|X − µ| ≤ σ) =68.27% Pr(|X − µ| ≤ 2σ) =95.45% Pr(|X − µ| ≤ 3σ) =99.73% Pr(|X − µ| &gt; 6σ) =1.96 × 10−9 记 $\\Phi(x)=\\int_{-\\infty}^x\\varphi(t)dt$，$\\Phi(-x)=1-\\Phi(x)$ 对 $X \\sim N(\\mu,\\sigma^2)$, $Pr(X\\in(a,b])=\\Phi(\\frac{b-\\mu}{\\sigma})-\\Phi(\\frac{a-\\mu}{\\sigma}))$ 特别地，如果 b = -a, 则 $Pr(X\\in(a,b])=\\Phi(\\frac{b-\\mu}{\\sigma})-\\Phi(\\frac{a-\\mu}{\\sigma}))=2\\Phi(\\frac{b-\\mu}{\\sigma})-1$ 正态分布最早由 Gauss 在研究测量误差时得到, 所以正态分布又被称为 Gauss 分布. 大量相互独立且有相同分布的随机变量的累积也近似服从正态分布。 Gamma 分布 设 α, λ 是正常数, Γ(α) 由积分 $\\Gamma(\\alpha)=\\int_0^{\\infty}x^{\\alpha -1}e^{-x}dx$ 定义。如果 X 的密度是 ​ $f(x)=\\begin{cases}\\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)x^{\\alpha-1}e^{-\\lambda x}},&amp;x\\ge0,\\0,&amp;x&lt;0. \\end{cases}$ 就称 X 服从参数 (α, λ) 的 Gamma 分布，记作 $X \\sim \\Gamma(\\alpha,\\lambda)$ 当 α = 1 时，Γ(1, λ) 即 E(λ)。 在气象学中, 干旱地区的年、季或月降水量被认为服从 Γ 分布, 指定时 间段内的最大风速等也被认为服从 Γ 分布. 2.4 概率分布函数概率分布函数 对随机变量 X, 称 x 的函数 ​ F(x) = P(X ≤ x), −∞ ≤ x ≤ ∞, 为 X 的概率分布函数, 简称为分布函数 (distribution function), 也称为累积 (cumulative) 分布函数。 离散型随机变量的分布函数 如果 X 是离散型随机变量, 有概率分布 ​ pk = P(X = xk), k = 1, 2, · · · , 则 X 的分布函数 ​ $F(x)=P(X\\le x)=P(\\bigcup\\limits{j:x_j\\le x}{X=x_j})=\\sum\\limits{j:x_j\\le x}p_j$ 连续型随机变量的分布函数 如果 X 是连续型随机变量, 有概率密度 f(x), 则 ​ $F(x)=\\int_{-\\infty}^xf(t)dt$ 是连续函数，并且在 f(x) 的连续点 x 有 $f(x)=F’(x)$。我们也称 F(x) 是 f(x) 的分布函数。 N(0,1), E(1.2), E(0.6), E(0.3) 分布函数图 分布函数 F(x) 的常用性质: F 单调不减右连续 F(∞) = 1，F(-∞) = 0 密度与分布函数 对于连续型的随机变量, 密度函数唯一决定分布函数. 反过来，如果 X 的分布函数 F(x) 在 (−∞,∞) 有连续的导函数，则 F′(x) 是 X 的密度函数. 更一般地，如果 F(x) 连续且除去有限个点之外都连续可导，则 F′(x) 是 X 的密度函数. 均匀分布的分布函数 若 $X \\sim U(0,1)$，则其分布密度为 ​ $f(x)=1, x\\in(0,1)$ 其分布函数为 若 $X \\sim U(a,b)$，则其分布密度为 ​ $f(x)=\\frac{1}{b-a}, x \\in (a,b)$ 其分布函数为 正态分布的分布函数 设 $X \\sim N(0,1)$, 则 X 的分布函数为 ​ $\\Phi(x)=\\int_{-\\infty}^x\\varphi(t)dt$ 其中 $\\varphi(x)=\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{x^2}{2}}$ 设 $X \\sim N(\\mu, \\sigma^2)$，则 X 的分布函数为 ​ $F(x)=\\Phi(\\frac{x-\\mu}{\\sigma})$ 指数函数的分布函数 若 $X \\sim E(\\lambda)$，则其分布函数为 ​ $f(x)=\\lambda e^{-\\lambda x}, x\\ge0$ 其分布函数为 ​ $F(x)=\\int_0^x\\lambda e^{-\\lambda t}dt=1-e^{-\\lambda x}, x\\ge0$ 2.5 随机变量函数的分布 设 X 有密度函数 f(x), D ⊂ R, Y = g(X), P(Y ∈ D) = 1. 如果存在函数 hi(y) 使得 对 $y ∈ D, {Y = y} = \\bigcup_{i=1}^n{X = h_i(y)}$ 每个 hi(y) 是 D 到其值域 Di 的可逆映射, 有连续的导数 值域 D1,D2, · · · ,Dn 互不相交 则 Y 有密度函数 正态分布的线性变换 设常数 $a\\ne0, X \\sim N(\\mu,\\sigma^2)$, 则 $Y = aX + b$ 服从正态分布 $N(a\\mu+b,a^2\\sigma^2)$。特别地，$Y=\\frac{X-\\mu}{\\sigma} \\sim N(0,1)$. 随机向量及其分布3.1 随机向量及其联合分布随机向量 如果 X, Y 都是随机变量, 就称 (X, Y) 是二维随机向量, 简称为随机向量 (random vector). 联合分布 对于随机向量 (X, Y), 我们称 ​ F(x, y) = P(X ≤ x,Y ≤ y) 为 (X, Y) 的联合概率分布函数, 简称为联合分布 (joint distribution). 联合分布函数 F(x, y) 是 x 的单调不减函数, 也是 y 的单调 不减函数. 边缘分布 设 F(x, y) 是 (X, Y) 的联合分布, 则 X,Y 分别有概率分布 ​ $F_X(x) =P(X ≤ x,Y ≤ ∞) = F(x,∞)$, ​ $F_Y(y) =P(X ≤ ∞, Y ≤ y) = F(∞, y)$. 我们称 X 的分布函数 FX(x), Y 的分布函数 FY(x) 为 (X, Y) 的边缘分布函数 (marginal distribution function). 独立随机变量 如果对任何实数 x, y, 事件 {X ≤ x} 和 {Y ≤ y} 独立，称随机变量 X, Y 独立。 独立的充分必要条件是对任何 x, y ​ $P(X ≤ x,Y ≤ y) = P(X ≤ x)P(Y ≤ y)$ 或等价地有 ​ $F(x, y) = F_X(x)F_Y(y)$ 当 X1,X2, · · · ,Xn 是来自相互独立进行的随机试验的随机变量时, 它们相互独立. 常数与任何随机变量独立. 3.2 离散型随机向量及其分布二元离散型随机向量 如果 X, Y 都是离散型随机变量, 就称 (X, Y) 是离散型随机向量. 设离散型随机向量 (X, Y) 有概率分布 ​ $p_{i,j}=P(X=x_i,Y=y_i), i,j\\ge1$ 则 X 和 Y 分别有概率分布 我们称 X 的分布 {pj}，Y 的分布 {qj} 为 (X, Y) 的边缘分布。 3.3 连续型随机向量及其分布联合密度 设 (X, Y) 是随机向量, 如果有 R2 上的非负可积函数 f(x, y) 使得对 R2 的所有长方形子集 D = { (x, y) | a &lt; x ≤ b, c &lt; y ≤ d } 有 ​ $P((X,Y)\\in D)=\\int\\int_Df(x,y)dxdy$ 就称 (X, Y) 是连续型随机向量，并称 f(x, y) 是 (X, Y) 的联合概率密度或联合密度. 按照上述定义, 连续型随机向量有概率密度, 没有概率密度的随机向量不是连续型随机向量. 注意，与一元连续型随机变量的定义类似，不是可以在 R2 连续取值 的二元随机向量就可以称为连续型随机向量，必须要有联合密度。 边缘密度 设 f(x, y) 是随机向量 (X, Y) 的概率密度, 则 X 和 Y 也都是连续型随机变量，我们称 X, Y 各自的概率密度为 f(x, y) 或 (X, Y) 的边缘 密度 (marginal density). 联合分布于联合密度 设 (X, Y) 有连续的分布函数 F(x, y), 定义 如果 $\\int\\int_{R^2}f(x,y)dxdy=1$ 则 f(x, y) 是 (X, Y) 的联合密度。 设 X, Y 分别有概率密度 fX(x), fY(y). 则 X, Y 独立的充分必要条件是随机向量 (X, Y) 有联合密度 f(x, y) = fX(x)fY(y). 均匀分布 两人约定时间会面，能相遇的概率 正态分布 脱靶量 独立同分布 如果随机变量 X1, X2, · · · , Xn 相 互独立并且有相同的分布, 就称它们独立同分布 (independent and identically distributed), 简称为 i.i.d.. 数学期望和方差4.1 数学期望随机变量的分布函数或密度函数描述了随机变量的统计性质, 从中可 以了解随机变量落入某个区间的概率, 但是还不能给人留下更直接的 总体印象. 我们需要为随机变量 X 定义一个实数, 这个数就是数学期望, 它反映随机变量的平均取值. 离散型 设 X 有概率分布 pj = P(X = xj), j = 0, 1, · · · , 只要级数 $\\sum_{j=0}^\\infty|x_j|p_j$ 收敛，就称 ​ $E(X)=\\sum\\limits_{j=0}^\\infty x_jp_j$ 为 X 或分布{pj} 的数学期望（expected value）或均值（mean）。 连续型 设 X 是有概率密度 f(x) 的随机变量, 如果下式成立, ​ $\\int_{-\\infty}^{\\infty}|x|f(x)dx&lt;\\infty$ 就称 $\\int_{-\\infty}^{\\infty}xf(x)dx$ 为 X 或 f(x) 的数学期望或均值。 两点分布 B(1, p) P(X = 1) = p, P(X = 0) = 1 − p, 则 E(X) = 1 · p + 0 · (1 − p) = p. 二项分布 B(n, p) E(X) = np 说明在 n 次独立重复试验中, 成功的概率 p 越大, 平均成功的次数越 多. 泊松分布 Possion(λ) E(X) = λ 几何分布 $P(X=j) = pq^{j-1}, j=1,2,…$ E(X) = 1/p 说明单次试验中的成功概率 p 越小, 首次成功所需要的平均试验 次数就越多. 指数分布 E(λ) $f(x) = \\lambda e^{-\\lambda x}, x&gt;0$ E(X) = 1/λ Gamma 分布 $\\Gamma(\\alpha,\\lambda)$ $f(x)=\\frac{\\lambda^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha-1}e^{-\\lambda x}, x&gt;0$ E(X) = α/λ 对称分布 设 X 的数学期望有限, 概率密度 f(x) 关于 µ 对称: f(µ + x) = f(µ − x), 则 E(X) = µ. 4.2 数学期望的性质设 g(x) 为一元函数，若随机变量 X 有密度 f(x) 且 $\\int|g(x)|f(x)dx&lt;\\infty$，则 $Eg(X)=\\int_{-\\infty}^\\infty g(x)f(x)dx)$ 若随机变量 X 有概率分布列 $Pr(X=x_k)=p_k, k=1,2,…$，且 $\\sum\\limits_k|g(x_k)|p_k&lt;\\infty$，则 $Eg(X)=\\sum\\limits_kg(x_k)p_k$. 4.3 随机变量的方差方差用来描述分布的分散程度，或宽窄。 如果随机变量 X 的数学期望 µ = EX 有限, 就称 $E(X-\\mu)^2$ 为 X 的方差，记作 Var(X) 或 $\\sigma _{XX}$. 当 Var(X) &lt; ∞, 称 X 的方差有限. 称 $\\sigma_X=\\sqrt{Var(X)}$ 为 X 的标准差。 方差的计算 当 X 有离散分布 P(X = xj), j = 1, 2, · · · 时, ​ $Var(X)=\\sum\\limits_{j=1}^\\infty(x_i-\\mu)^2P(X=x_j)$ 当 X 有概率密度 f(x) 时, ​ $Var(X)=\\int_{-\\infty}^\\infty(x-\\mu)^2f(x)dx$ 随机变量 X 的方差 Var(X) 由 X 的分布唯一决定. X 的方差描述了 X 的分散程度, Var(X) 越小, 说明 X 在数学期望 µ 附近越集中. 利用数学期望的线性性质得到, ​ $Var(X)=E(X^2-2X\\mu+\\mu^2)=EX^2-(EX)^2$ 两点分布 P(X = 1) = p, P(X = 0) = 1 − p. 由 $X^2 = X$ 和 $EX = p$, 得到 ​ $Var(X)=EX^2-(EX)^2=p-p^2=pq$ 二项分布 $Var(X)=npq$. 泊松分布 $Var(X)=\\lambda$ 几何分布 $Var(X)=q/p^2$ 均匀分布 $Var(X)=\\frac{(b-a)^2}{12}$ 指数分布 $Var(X)=1/\\lambda^2$ 正态分布 $Var(X)=\\sigma^2$ 方差的性质 $Var(a+bX)=b^2Var(X)$ $Var(X)=E(X-\\mu)^2&lt;E(X-c)^2$，只要 $c\\ne\\mu$ $Var(X)=0$ 的充分必要条件是 $P(X=\\mu)=1$ 当 $X1,X_2, · · · ,X_n$ 相互独立, $Var(\\sum{j=1}^nXj)=\\sum{j=1}^nVar(X_j)$ 分布 X 分布/密度 期望 EX 方差 Var(X) 两点分布 $B(1,p)$ $P(X=1)=p,P(X=0)=q=1-p$ $p$ $pq$ 二项分布 $B(n,p)$ $P(X=k)=C_n^kp^kq^{n-k}$ $np$ $npq$ 泊松分布 $Poission(\\lambda)$ $P(X=k)=\\frac{\\lambda^k}{k!}e^{-\\lambda}$ $\\lambda^2+\\lambda$ $\\lambda$ 几何分布 $P(X=j)=pq^{j-1}$ $\\frac{1}{p}$ $\\frac{q}{p^2}$ 均匀分布 $U(a,b)$ $f(x)=\\frac{1}{b-a}I_{(a,b)}$ $\\frac{a+b}{2}$ $\\frac{(b-a)^2}{12}$ 指数分布 $E(\\lambda)$ $f(x)=\\lambda e^{-\\lambda x}$ $\\frac{1}{\\lambda}$ $\\frac{1}{\\lambda^2}$ 正态分布 $N(\\mu,\\sigma^2)$ $f(x)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp(-\\frac{(x-\\mu)^2}{2\\sigma^2})$ $\\mu$ $\\sigma^2$ 4.4 协方差和相关系数设 $σX =\\sqrt{σ{XX}}, σY = \\sqrt{σ{YY}}$ 分别是 X, Y 的标准差. 设 $\\mu_X=EX,\\mu_Y=EY$ 存在， 当 $E|(X-\\muX)(Y-\\mu_Y)|&lt;\\infty$, 称 $E[(X-\\mu_X)(Y-\\mu_Y)]$ 为随机变量 X， Y 的协方差，记作 Cov(X, Y) 或 $\\sigma{XY}$. 当 Cov(X, Y) = 0 时，称 X, Y 不相关。 当 $0&lt;\\sigmaX\\sigma_Y&lt;\\infty$，称 $\\rho{XY}=\\frac{\\sigma_{XY}}{\\sigma_X\\sigma_Y}$ 为 X, Y 的相关系数。 计算协方差的常用公式： ​ $\\sigma_{XY}=E(XY)-(EX)(EY)$ 相关系数的性质 设 $ρ_{XY}$ 是 X, Y 的相关系数, 则有 $|\\rho_{XY}|\\le1$ $|\\rho_{XY}|=1$ 的充分必要条件是 有常数 a, b 使得 $P(Y=a+bX)=1$ 如果 X, Y 独立，则 X, Y 不相关 描述性统计6.1 总体和参数统计学 统计学研究如何收集数据、分析数据、从数据做出有依据的推断结果。 一言以蔽之，统计学是研究数据的科学。 统计学主要的数学工具是概率论，也广泛使用现代信息技术作为支撑， 通过计算机和信息网络获取数据、进行建模、数据分析计算。 统计学是一门科学，不再是数学的一个分支。 统计学的做法分为两种： 描述性统计：从数据样本中计算一些平均值、标准差、最小值、最大值等概括 统计量，画直方图、散点图等描述图形。 推断性统计：假定要研究的对象服从某种概率模型，收集数据后把数据用模型 解释，并做出有概率意义的结论。 总体、个体和均值 所要调查的对象全体叫做总体 (population), 总体中每个成员叫做个体。 总体参数是描述总体特性的指标，简称参数。 如果总体中的个体是有限个，称个体总数 N 为总体容量。 总体平均或总体均值是参数。常用 µ 表示。 总体方差是参数。常记为 σ2。 样本与估计 为了得到总体的信息，可以从总体中抽取一个有代表性的个体的集合， 称为总体的一个样本。样本中个体的个数叫做样本量 (sample size)。 从总体中抽取样本的工作叫做抽样 (sampling)。 利用样本计算出的对总体参数的估计值称为估计 (estimator 或 estimate)。 6.2 抽样调查方法随机抽样 无放回随机抽样指在总体中随机抽出一个个体后, 下次在余下的个体中再进行随机抽样. 有放回随机抽样指抽出一个个体, 记录下抽到的结果后放回, 摇匀后再进行下一次随机抽样. 无放回抽取从实现上和从精度上更好，总体容量 N 很大时两者差异很小。 分层抽样 总体当中分为不同人群时 (如城镇和乡村)，虽然仍然进行等可能随机抽样，这样不同人群差异过大引起估计误差变大，而且操作也不方便。 好的作法是按人口比例在不同人群中分别进行随机抽样。 计算平均值等统计量时要用加权求和（平均）计算。 系统抽样 随机抽样有时难于实施，当个体排列本身比较随机时，根据某种固定规律抽取，也能达到类似随机抽样效果，称为系统抽样。 最简单的系统抽样法是取得一个个体后, 按相同的间隔抽取其他个体. 系统抽样方法的主要优点是实施简单, 只需要先随机抽取第一个个体, 以后按规定抽取就可以了. 参数估计7.1 点估计和矩估计如果 X 是从总体中随机抽样得到的个体, 则 X 是随机变量, X 的分布就是总体的分布. 如果对总体进行有放回的随机抽样, 就得到独立同分布的, 和 X 同分布的随机变量 X1,X2, · · · ,Xn. 我们称 X1,X2, · · · ,Xn 是来自总体 X 的简单随机样本，简称为总体 X 的样本。 估计量 设 X1,X2, · · · ,Xn 是总体 X 的简单随机样本, θ 是总体 X 的未知参 数. 如果 g(x1, x2, · · · , xn) 是已知函数, 就称 $\\hat\\theta =g(X_1,X_2,…,X_n)$ 是 θ 的估计量，简称估计。 估计或估计量是从 观测数据 X1,X2, · · · ,Xn 能够直接计算的量. 计算后得到的值称为估 计值. 估计量也称为统计量 (statistic). 如果 $E\\hatθ = θ$, 称 $\\hatθ$ 是 θ 的无偏估计; 如果当样本量 n → ∞, $\\hatθ$ 依概率收敛到 θ, 就称 $\\hatθ$ 是 θ 的相合估计 (consistent estimator); 如果当样本量 n → ∞, $\\hatθ$ 以概率 1 收敛到 θ, 就称 $\\hatθ$ 是 θ 的强相合估计 (strongly consistent estimator). 样本均值 ¯Xn 是总体均值 µ 的强相合无偏估计 样本方差 S2 是总体方差 σ2 的强相合无偏估计 样本标准差 S 是总体标准差 σ 的强相合估计 7.2 最大似然估计 我们能够观测到一个事件是因为这个事件发生的概率较 大. 这样思考问题的思想被称为最大似然思想. 离散情况 设离散随机变量 X1,X2, · · · ,Xn 有联合分布 p(x1, x2, · · · , xn; θ) = P(X1 = x1,X2 = x2, · · · ,Xn = xn), 其中 θ 是未知参数, 给定观测数据 x1, x2, · · · , xn 后, 我们称 θ 的函数 ​ L(θ) = p(x1, x2, · · · , xn; θ) 为基于 x1, x2, · · · , xn 的似然函数, 称 L(θ) 的最大值点 ˆθ 为 θ 的最大似然估计 (maximum likelihood estimator). 连续情况 设随机向量 X = (X1,X2, · · · ,Xn) 有联合密度 f(x; θ), 其 中 θ 是未知参数. 得到 X 的观测值 x 后, 称 θ 的函数 ​ L(θ ) = f(x; θ ) 为基于 x 的似然函数. 称似然函数 L(θ) 的最大值点 ˆθ 为参数 θ 的最大似然估计. 最大似然估计通常被缩写成 MLE(Maximum Likelihood Estimator).","link":"2022/10/20/%E6%A6%82%E7%8E%87%E7%BB%9F%E8%AE%A1-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"数据结构与算法","text":"程序 = 数据结构 + 算法 1. 时间复杂度 与 空间复杂度时间复杂度 时间复杂度 的全称是 渐进时间复杂度，表示算法的执行时间与数据规模之间的增长关系。时间复杂度表示法并不能真正反映一个算法的执行时间，反映的是程序执行时间随数据规模变化的增长趋势。 常数阶 O(1) 这里的 1 是用来代指常量，也就是说这个算法的效率是固定的，无论你的数据量如何变化，效率都一样，这种复杂度也是最优的一种算法。 只要代码不存在循环，递归等循环类调用，不论代码有多少行，其复杂度都是常数阶。 线性阶 O(n) 程序的执行时间随数据量的变化线性增长。 对数阶 O(logn) 1234int i = 1;while (i &lt; n) { i = i * 2;} 线性对数阶 O(nlogn) 将时间复杂度为 O(logn) 的代码循环 n 次的话，那么它的时间复杂度就是 n * O(logn)，也就是 O(nlogn)。 123456for (int i =0; i &lt; n; i++) { int j = 1; while(i &lt; n) { j = j * 2; }} 平方阶 $O(n^2)$ 循环中嵌套循环 12345for (int i = 0; i &lt; n; i++) { for (int j = 0; j &lt; n; j++) { printf(&quot;Hello, world!&quot;) }} 其他时间复杂度： $O(1) &lt; O(logN) &lt; O(n) &lt; O(nlogN) &lt; O(n^2) &lt; O(n^3) &lt; O(n^k) &lt; O(2^n) &lt; O(n!)$ 时间复杂度类型 上面分析的时间复杂度都是比较简单的，实际算法中可能会比示例中复杂的多，有时候并不需要从头循环到尾，可能中途就会结束循环，所以根据实际情况，又可以将时间复杂度从以下四个方面来进一步分析： 最好时间复杂度 程序执行最好的情况，比如在一个指定数组中找到指定元素的下标，时间复杂度为 O(n)，如果算法是顺序遍历数据，要查找的元素刚好在数组的第一个位置，此时最好时间复杂度就为 O(1)。 最坏时间复杂度 类似地，比如循环遍历到最后。 平均时间复杂度 平均时间复杂度只有在特殊时候才会使用。每种情况的时间复杂度的平均。 空间复杂度 空间复杂度 全称就是渐进空间复杂度，用来表示算法的存储空间与数据规模之间的增长关系。 如果申请的是有限个数（常量）的变量，空间复杂度为 O(1)。 如果申请的是一维数组，队列或者链表等，那么空间复杂度为 O(n)。 如果申请的是二维数组，那么空间复杂度为 O(n²)。 如果是在循环体中申请的数组等，可能就需要取嵌套的乘积来作为空间复杂度，这种就需要具体的进一步分析。 2. 基本算法思想贪心算法贪心算法，又名贪婪法，是寻找最优解问题的常用方法，一般将求解过程分成若干个步骤，但每个步骤都应用贪心原则，选取当前状态下最好/最优的选择（局部最优策略），并以此希望最后堆叠出的结果也是最好/最优的解。 1. 找零钱问题 假设你开了间小店，要找给客户 41 分钱的硬币，钱柜里的货币只有 25 分、10 分、5 分和 1 分四种硬币，如何安排才能找给客人的钱既正确且硬币的个数最少？ 思路：从币值大的开始选，能找大的不找小的。 25 + 10 + 5 + 1 1234567891011121314151617181920212223242526#include&lt;iostream&gt;using namespace std;#define ONEFEN 1#define FIVEFEN 5#define TENFEN 10#define TWENTYFINEFEN 25int main() { int sum_money = 41; int num_25 = 0, num_10 = 0, num_5 = 0, num_1 = 0; //不断尝试每一种硬币 while (money &gt;= TWENTYFINEFEN) { num_25++; sum_money -= TWENTYFINEFEN; } while (money &gt;= TENFEN) { num_10++; sum_money -= TENFEN; } while (money &gt;= FIVEFEN) { num_5++; sum_money -= FIVEFEN; } while (mone y&gt;= ONEFEN) { num_1++; sum_money -= ONEFEN; } //输出结果 cout &lt;&lt; &quot;25分硬币数：&quot; &lt;&lt; num_25 &lt;&lt; endl; cout &lt;&lt; &quot;10分硬币数：&quot; &lt;&lt; num_10 &lt;&lt; endl; cout &lt;&lt; &quot;5分硬币数：&quot; &lt;&lt; num_5 &lt;&lt; endl; cout &lt;&lt; &quot;1分硬币数：&quot; &lt;&lt; num_1 &lt;&lt; endl; return 0;} 2. 背包问题 有一个背包，最多能承载重量为 C=150的物品，现在有7个物品，重量分别是 wi=[35,30,60,50,40,10,25]，价值分别是 pi=[10,40,30,50,35,40,30]，现在从这 7 个物品中选择一个或多个装入背包，要求在物品总重量不超过 C 的前提下，所装入的物品总价值最高。 思路：优先选择质量密度最大的物品。 3. 贪心算法的优缺点 对于找零钱问题，如果现有的硬币有25 分、20分、10 分、5 分和 1 分五种，贪心算法就不能得到最优解。 25 + 10 + 5 + 1：四枚硬币；20 + 20 + 1：三枚硬币 优点：简单，高效，省去了为了找最优解可能需要穷举操作，通常作为其它算法的辅助算法来使用。 缺点：不从总体上考虑其它可能情况，每次选取局部最优解，不再进行回溯处理，所以很少情况下得到最优解。 分治算法分治，即分而治之，就是把一个复杂的问题分成两个或更多的相同或相似的子问题，再把子问题分成更小的子问题（分）……直到最后子问题可以简单的直接求解，原问题的解即子问题的解的合并（治）。 分治算法是很多高效算法的基础，如排序算法(快速排序，归并排序)，傅立叶变换(快速傅立叶变换)等。 分治法所能解决的问题一般具有以下几个特征： 该问题的规模缩小到一定的程度就可以容易地解决 该问题可以分解为若干个规模较小的相同问题，即该问题具有最优子结构性质 利用该问题分解出的子问题的解可以合并为该问题的解 该问题所分解出的各个子问题是相互独立的，即子问题之间不包含公共的子问题 注：第三条特征是关键，能否利用分治法完全取决于问题是否具有第三条特征，如果具备了第一条和第二条特征，而不具备第三条特征，则可以考虑用贪心法或动态规划法。 第四条特征涉及到分治法的效率，如果各子问题是不独立的则分治法要做许多不必要的工作，重复地解公共的子问题，此时虽然可用分治法，但一般用动态规划法较好。 1. 二分搜索 二分搜索是一种在有序数组中查找某一特定元素的搜索算法。搜索过程从数组的中间元素开始，如果中间元素正好是要查找的元素，则搜索过程结束；如果某一特定元素大于或者小于中间元素，则在数组大于或小于中间元素的那一半中查找，而且跟开始一样从中间元素开始比较。如果在某一步骤数组为空，则代表找不到。这种搜索算法每一次比较都使搜索范围缩小一半。 12345678910111213141516int BinarySearch(int array[], int n, int value) { int left = 0; int right = n - 1; while (left &lt;= right) { int middle = left + ((right - left) &gt;&gt; 1); // 防止溢出，移位也更高效。同时，每次循环都需要更新。 if (array[middle] &gt; value) right = middle - 1; else if (array[middle] &lt; value) left = middle + 1; else return middle; } return -1;} 2. 大数乘法 在计算机上处理一些大数据相乘时，由于计算机硬件的限制，不能直接进行相乘得到想要的结果。可以将一个大的整数乘法分而治之，将大问题变成小问题，变成简单的小数乘法再进行合并，从而解决上述问题。 给定两个大数 X，Y 和位数 n，我们先把 X 和 Y 拆成两半，分别由高几位和低几位数字组成。例如，X = 61438521，Y = 94736407，将 X 拆为 A = 6143 和 B = 8521，Y 拆分为 C = 9473，D = 6407，则 $X \\times Y = (A \\times 10^{n/2} + B) \\times (C \\times 10^{n/2} + D) = AC \\times 10^n + (AD + BC) \\times 10^{n/2} + BD$ 递归地调用该算法进行四项乘法的计算，在一个适当的情形下停止（如数字位数为2），并对结果进行合并。 12345678910111213141516171819202122232425#include&lt;cstdio&gt;#include&lt;cmath&gt;using namespace std;#define SIGN(A) ((A &gt; 0) ? 1 : -1) int divideConquer(int X, int Y, int n) { int sign = SIGN(X) * SIGN(Y); int x = abs(X); int y = abs(Y); if (x == 0 || y == 0) { return 0; } else if (n == 1) { return sign * x * y; } else { int A = (int) x / pow(10, (int)(n / 2)); int B = x - A * pow(10, n / 2); int C = (int) y / pow(10, (int)(n / 2)); int D = y - C * pow(10, n / 2); int AC = divideConquer(A, C, n / 2); int BD = divideConquer(B, D, n / 2); int ABDC = divideConquer((A - B), (D - C), n / 2) + AC + BD; return sign * (AC * pow(10 , n) + ABDC * pow(10, (int)(n / 2)) + BD); }} 3. 最近点问题 在二维坐标轴上有 N 个点，求出最近的两个点的距离（欧氏距离）。如果暴力枚举，时间复杂度为 $O(n^2)$，计算量非常大。我们通常采用分治的方法来优化这种问题。 首先对所有的点按照 x 轴升序排列。 分： 点数小于等于二时：直接计算，求该两点之间的距离 d。 集合中有三个点：两两比较，求三个点中的最近的两个点距离 d。 点数大于三时：将集合 S 划分为集合 L 和集合 R，使得 L 中的每一个点位于 R 中所有点的左边，并且 L 的点数与 R 的点数之差最小。 按以上步骤递归计算，求得集合内的最小距离 d。 治： 最近距离的两个点位于划分集内部，则最小点对距离为 d 最近距离的两个点分别在左右两个点集中，这时要处理左右集合相邻区域之间的几个点（下图中绿色的点）。 由于以求得左右点集的最小点对距离为 d，我们可以把左侧集合最靠右的点的横坐标记为 midx，因此只需要考虑横坐标位于 [midx - d, midx + d] 之间的点（绿色点）。对于这部分点，暴力枚举计算距离的话复杂度还是很高，我们还可以进行进一步的优化：如果两个点的 y 坐标的差的绝对值大于 d，那么它们间的距离也一定大于 d。于是我们可以把这些点按照 y 坐标排序，按照 y 坐标从小到大依次考虑每个点i与它后面（即 y 坐标大于等于它的点）的所有点 j 之间的距离，如果 i 与 j 的 y 坐标差大于等于 d 就退出循环，计算出这些点的最小距离 $d’$，最后取 $min(d, d’)$ 作为该左右集合的最小点对距离返回，进行与上一层的合并。 这里，其实对于每个绿色点，考虑的点的范围为左右”日“字区域内（红色区域），纳入计算的点最多为 6 个。下面可以证明这一点。 我们将“日”字区域等分为 6 份，每份内的最大距离为 5/6 d &lt; d，而我们在之前已经计算了左右集合内点对的最小距离为 d，因此“日”字区域的每 1/6 区域内不可能存在两个点，因此“日”字区域内最多包含 6 个点。所以，对于中间区域的每个绿色点，我们最多只需考虑 6 × 2 = 12 个点（左右“日”字区域），这部分的时间复杂度为 O(n)。因此，算法的最终时间复杂度为 O(nlogn)。 123456789101112131415161718192021222324252627282930313233343536373839404142434445struct point { double x, y;} p[maxn]; double dis(point a, point b) { return sqrt((a.x - b.x) * (a.x - b.x) + (a.y - b.y) * (a.y - b.y));}bool cmpx(point a, point b) { return a.x &lt; b.x;}bool cmpy(point a,point b) { return a.y &lt; b.y;}void swap(int *a, int *b) { int t; t = *a; *a = *b; *b = t;}double cal(point a[], int len) { if(len==1) return inf; if(len==2) return dis(a[0], a[1]); double p = a[len / 2 - 1].x; int mid = len / 2; double d = min(cal(a, mid), cal(a + mid, len - mid)); int tot = 0; for (int i = 0; i &lt; len; ++i) if (fabs(p - a[i].x) &lt;= d) swap(a[tot++], a[i]); sort(a, a + tot, cmpy); for (int i = 0; i &lt; tot; ++i) { for (int j = i + 1; j &lt; tot; ++j) { if (a[j].y - a[i].y &gt; d) break; d = min(d, dis(a[i], a[j])); } } return d;} 动态规划动态规划（Dynamic Programming, DP）是运筹学的一个分支，是求解决策过程 (decision process) 最优化的数学方法。 计算机归根结底只会做一件事：穷举。所有的算法都是在让计算机【如何聪明地穷举】而已，动态规划也是如此。 动态规划与递归 动态规划是自底向上，递归树是自顶向下。 例如，对于求解斐波那契数列问题，递归的做法： 12345int Fibonacci(size_t n) { if(n == 1 || n == 2) return n; return Fibonacci1(n - 1) + Fibonacci1(n - 2);} 动态对话的做法： 1234567int fib(int N) { vector&lt;int&gt; dp(N + 1, 0); dp[1] = dp[2] = 1; for (int i = 3; i &lt;= N; i++) dp[i] = dp[i - 1] + dp[i - 2]; return dp[N];} 这也是为什么动态规划一般都脱离了递归，而是由循环迭代完成计算。 动态规划最核心的思想，就在于拆分子问题，记住过往，减少重复计算。 青蛙跳台阶问题 一只青蛙一次可以跳上1级台阶，也可以跳上2级台阶。求该青蛙跳上一个 10 级的台阶总共有多少种跳法？ 思路：要想跳到第10级台阶，要么是先跳到第9级，然后再跳1级台阶上去; 要么是先跳到第8级，然后一次迈2级台阶上去。 同理，要想跳到第9级台阶，要么是先跳到第8级，然后再跳1级台阶上去; 要么是先跳到第7级，然后一次迈2级台阶上去。 …… 假设跳到第 n 级台阶的跳数我们定义为 f(n)，很显然就可以得出以下公式： 12345f(10) = f(9) + f(8)f(9) = f(8) + f(7)......f(3) = f(2) + f(1)通用公式为：f(n) = f(n-1) + f(n-2) 又有 f(2) = 2, f(1) = 1。 如果采用递归算法： 123456789int numWays(int n) { if(n == 1) { return 1; } if(n == 2) { return 2; } return numWays(n-1) + numWays(n-2);} 代码很简单，但是时间复杂度非常高：$O(2^n)$，会超出时间限制。 分析： 要计算原问题 f(10)，就需要先计算出子问题 f(9) 和 f(8) 要计算 f(9)，又要先算出子问题 f(8) 和 f(7) 以此类推 递归时间复杂度 = 解决一个子问题时间 * 子问题个数 一个子问题时间 = f（n-1）+f（n-2），也就是一个加法的操作，所以复杂度是 O(1)； 问题个数 = 递归树节点的总数，递归树的总节点 = $2^n-1$，所以是复杂度 $O(2^n)$。 因此，青蛙跳阶的递归解法的时间复杂度为 $O(1) * O(2^n) = O(2^n)。 采用动态规划算法： 递归，是从 f(10) 往 f(1) 方向延伸求解的，所以也称为自顶向下的解法。这种方法无法利用已经求得的结果。 而动态规划是自底向上的方法，已经计算的结果可以被使用，减少重复计算。 动态规划有几个典型特征，最优子结构、状态转移方程、边界、重叠子问题。在青蛙跳阶问题中： f(n-1) 和 f(n-2) 称为 f(n) 的最优子结构 f(n) = f(n-1) + f(n-2) 称为状态转移方程 f(1) = 1, f(2) = 2 就是边界 比如 f(10)= f(9) + f(8), f(9) = f(8) + f(7) , 这里 f(8)就是重叠子问题 动态规划的优点就是对重叠子问题的重复利用。 1234567891011121314151617int numWays(int n) { if (n&lt;= 1) { return 1; } if (n == 2) { return 2; } int a = 1; int b = 2; int temp = 0; for (int i = 3; i &lt;= n; i++) { temp = (a + b); a = b; b = temp; } return temp;} 时间复杂度为 O(n)，空间复杂度为 O(1)。 动态规划的解题思路： 如果一个问题，可以把所有可能的答案穷举出来，并且穷举出来后，发现存在重叠子问题，就可以考虑使用动态规划。如最长递增子序列、最小编辑距离、背包问题、找零钱问题等等，都是动态规划的经典应用场景。 动态规划的核心思想就是拆分子问题，记住过往，减少重复计算。 并且动态规划一般都是自底向上的。动态规划的思路： 穷举分析 确定边界 找出规律，确定最优子结构 写出状态转移方程 回溯算法回溯法（back tracking）是一种选优搜索法，又称为试探法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。 八皇后问题 在 8X8 格的国际象棋上摆放八个皇后（棋子），使其不能互相攻击，即任意两个皇后都不能处于同一行、同一列或同一斜线上。亦即，每行、每列上只能有且只有一个皇后。 思路：在第一行放置一枚皇后；遍历剩下行的每个位置，如果可放置，更新放置数据，如果没有可放置的点，返回上一步；最后判断边界。 12345678910111213141516171819202122232425262728293031323334353637#define MAX_NUM 8 //皇后数量int queen[MAX_NUM][MAX_NUM] = { 0 };bool check(int x, int y) { //检查一个坐标是否可以放置 for (int i = 0; i &lt; y; i++) { if (queen[x][i] == 1) { //这一行是否可以存在 return false; } if (x - 1 - i &gt; 0 &amp;&amp; queen[x - 1 - i][y - 1 - i] == 1) { //检查左斜列 return false; } if (x + 1 + i &lt; MAX_NUM &amp;&amp; queen[x + 1 + i][y + 1 + i] == 1) { //检查右斜列 return false; } } queen[x][y] = 1; return true;}bool settleQueen(int x) { if (x == MAX_NUM) { //遍历完毕，找到答案 return true; } for (int i = 0; i &lt; MAX_NUM; i++) { for (int y = 0; y &lt; MAX_NUM; y++) queen[y][x] = 0; //清空当前列，省的回溯的时候被打扰 if (check(i,x)) { //如果这行找着了 queen[i][x] = 1; showQueen(); //直观测试结果 if (settleQueen(x + 1)) { //是时候往左了 return true; //一路往左 } } } return false; //如果不行，就退回来} 背包问题 123456789101112131415161718192021222324252627int weight[4] = { 1,2,3,4 };int value[4] = { 3,2,4,3 };int flag[4] = { 0 };int maxvalue = 0;int packweight = 0;int packvalue = 0;void backtrack(int node) { if (node == 4) { if (maxvalue &lt; packvalue) maxvalue = packvalue; } else { for (int i = 0; i &lt; 2; i++) { if (i == 0) { backtrack(node + 1); } else { if (packweight + weight[node] &lt;= 8) { // 小于背包总容量 packweight += weight[node]; packvalue += value[node]; backtrack(node + 1); // 继续进行深度搜索 packweight -= weight[node]; // 把当前物品从背包中删除 packvalue -= value[node]; // 因为计算下一条路径时需要把前一次的清空 } } } }} 回溯法属于深度优先搜索，由于是全局搜索，复杂度相对高。 3. 线性表数组与链表 数组： 数组设计之初是在形式上依赖内存分配而成的，所以必须在使用前预先分配好空间大小。这使得数组有以下特性： 用连续的内存空间来存储数据 数组支持随机访问，根据下标随机访问的时间复杂度为 O(1) 数组的插入、删除操作，平均时间复杂度为 O(n) 空间大小固定，一旦建立，不能再改变。扩容只能采用复制数组的方式 链表： 链表用不连续的内存空间来存储数据；并通过一个指针按顺序将这些空间串起来，形成一条链。链表的每一个结点除了存储数据以外，还保存了到下一个节点的指针。由于不必按顺序存储，链表在插入数据的时候可以达到 O(1) 的复杂度，但是查找一个节点或者访问特定编号的节点则需要 O(n) 的时间。 链表具有以下特性： 链表允许插入和移除任意位置上的节点，其时间复杂度为 O(1) 链表没有数组的随机访问特性，链表只支持顺序访问，其时间复杂度为 O(n) 数组的空间大小是固定的，而链表的空间大小可以动态增长。相比于数组，链表支持扩容，显然更为灵活，但是由于多了指针域，空间开销也更大 链表相比于数组，多了头指针、尾指针（非必要），合理使用可以大大提高访问效率 链表有多种类型： 单链表 双链表 循环链表 4. 树树是非线性数据结构的一种，由结点和边组成，且不存在着任何环。 节点的度：树中某个节点的子树的个数 树的度：树中各节点的度的最大值 分支节点：度不为零的节点 叶子节点：度为零的节点 树的高度（深度）：树中节点的最大层次 有序树：树中节点子树按次序从左向右安排，区分左右节点 森林：互不相交的树的集合 树的节点数：数的所有节点度数加1（根节点） 度为 m 的树中第 i 层最多有 $m^{i-1}$个节点 高度为 h 的 m 次树至多有 $\\dfrac{m^{h-1}}{m-1}$个节点 具有 n 个节点的 m 次树的最小高度为 $log_m( n(m-1) + 1 )$ 向上取整 二叉树 满足以下两个条件的树就是二叉树： 本身是有序树 树中包含的各个节点的度不超过 2 二叉树的性质： 第 i 层最多有 $2^{i-1}$ 个结点 如果二叉树的深度为 K，那么此二叉树最多有 $2^K-1$ 个结点 二叉树中，叶子结点数为 n0，度为 2 的结点数为 n2，则 n0 = n2+1 满二叉树：如果二叉树中除了叶子结点，每个结点的度都为 2，则此二叉树称为满二叉树 完全二叉树：如果二叉树中除去最后一层节点为满二叉树，且最后一层的结点依次从左到右分布，则此二叉树被称为完全二叉树。 二叉树的遍历 前序（先序）遍历：（1）访问当前节点；（2）访问当前节点的左子树；（3）访问当前节点的右子树。 中序遍历：（1）访问当前节点的左子树；（2）访问当前节点；（3）访问当前节点的右子树。 后序遍历：（1）访问当前节点的左子树；（2）访问当前节点的右子树；（3）访问当前节点。 二叉查找树 二叉查找树是二叉树中最常用的一种类型，也叫二叉搜索树。顾名思义，二叉查找树是为了实现快速查找而生的。不过，它不仅仅支持快速查找一个数据，还支持快速插入、删除一个数据。 二叉查找树要求，在树中的任意一个节点，其左子树中的每个节点的值，都要小于这个节点的值，而右子树节点的值都大于这个节点的值。即二叉查找树的中序遍历结果为升序序列。 第一，哈希表中的数据是无序存储的，如果要输出有序的数据，需要先进行排序。而对于二叉查找树来说，我们只需要中序遍历，就可以在 O(n) 的时间复杂度内，输出有序的数据序列。 第二，哈希表扩容耗时很多，而且当遇到散列冲突时，性能不稳定，尽管二叉查找树的性能不稳定，但是在工程中，我们最常用的平衡二叉查找树的性能非常稳定，时间复杂度稳定在 O(logn)。 第三，笼统地来说，尽管哈希表的查找等操作的时间复杂度是常量级的，但因为哈希冲突的存在，这个常量不一定比 logn 小，所以实际的查找速度可能不一定比 O(logn) 快。加上哈希函数的耗时，也不一定就比平衡二叉查找树的效率高。 第四，哈希表的构造比二叉查找树要复杂，需要考虑的东西很多。比如散列函数的设计、冲突解决办法、扩容、缩容等。平衡二叉查找树只需要考虑平衡性这一个问题，而且这个问题的解决方案比较成熟、固定。 最后，为了避免过多的散列冲突，哈希表装载因子不能太大，特别是基于开放寻址法解决冲突的哈希表，不然会浪费一定的存储空间。 堆 堆（Heap）是一个可以被看成近似完全二叉树的数组。 堆是一个完全二叉树 堆中每一个节点的值都必须大于等于（或小于等于）其子树中每个节点的值 堆可以分为大顶堆和小顶堆。 对于每个节点的值都大于等于子树中每个节点值的堆，叫作“大顶堆” 对于每个节点的值都小于等于子树中每个节点值的堆，叫作“小顶堆” 完全二叉树比较适合用数组来存储。用数组来存储完全二叉树是非常节省存储空间的。因为我们不需要存储左右子节点的指针，单纯地通过数组的下标，就可以找到一个节点的左右子节点和父节点。 堆常见的操作： 建堆：把一个乱序的数组变成堆结构的数组，时间复杂度为 O(n) 把一个数值放进已经是堆结构的数组中，并保持堆结构，时间复杂度为 O(logN) 从最大堆中取出最大值或从最小堆中取出最小值，并将剩余的数组保持堆结构，时间复杂度为 O(logN) 借由 HEAPFY 建堆和 HEAPPOP 堆数组进行排序，时间复杂度为 O(N log N)，空间复杂度为 O(1) 5. 图图是由顶点和边组成的另一种非线性数据结构。 强连通图：有向图中，若任意两个顶点 Vi 和 Vj，满足从 Vi 到 Vj 以及从 Vj 到 Vi 都连通，也就是都含有至少一条通路，则称此有向图为强连通图。 若有向图本身不是强连通图，但其包含的最大连通子图具有强连通图的性质，则称该子图为强连通分量。 生成树：对连通图进行遍历，过程中所经过的边和顶点的组合可看做是一棵普通树，通常称为生成树。 连通图中，由于任意两顶点之间可能含有多条通路，遍历连通图的方式有多种，往往一张连通图可能有多种不同的生成树与之对应。 深度优先搜索 深度优先搜索，是从图中的一个顶点出发，每次遍历当前访问顶点的临界点，一直到访问的顶点没有未被访问过的临界点为止。然后采用依次回退的方式，查看来的路上每一个顶点是否有其它未被访问的临界点。访问完成后，判断图中的顶点是否已经全部遍历完成，如果没有，以未访问的顶点为起始点，重复上述过程。 广度有限搜索 广度优先搜索类似于树的层次遍历。从图中的某一顶点出发，遍历每一个顶点时，依次遍历其所有的邻接点，然后再从这些邻接点出发，同样依次访问它们的邻接点。按照此过程，直到图中所有被访问过的顶点的邻接点都被访问到。最后还需要做的操作就是查看图中是否存在尚未被访问的顶点，若有，则以该顶点为起始点，重复上述遍历的过程。 最小生成树算法最小生成树是一个连通加权无向图中一棵权值之和最小的生成树。 Prim 算法 此算法可以称为“加点法”，基于贪心的策略，每次迭代选择权重最小的边对应的点，加入到最小生成树中。算法从某一个顶点 s 开始，逐渐覆盖整个连通网的所有顶点。 1）图的所有顶点的集合为 $V$；初始令集合 $u = {s}, v = V−u$。 2）在两个集合 $u, v$ 能够组成的边中，选择一条代价最小的边 $(u_0,v_0)$，加入到最小生成树中，并把 $v_0$ 并入到集合 $u$ 中。 3）重复步骤 2，直到最小生成树有 $n-1$ 条边或者 n 个顶点为止。 时间复杂度：$O(n^2)$ Kruskal 算法 Kruskal 算法是基于贪心的思想得到的。首先我们把所有的边按照权值先从小到大排列，接着按照顺序选取每条边，如果这条边的两个端点不属于同一集合，那么就将它们合并，直到所有的点都属于同一个集合为止。 1）将图 G 看做一个森林，每个顶点为一棵独立的树。2）将所有的边加入集合 S，即一开始 S = E。3）从 S 中选择权重最小的边 (u, v)，如果 (u, v) 不在同一棵树内，则连接 u, v 合并这两棵树，同时将 (u, v) 加入生成树的边集E’。4）重复 (3) 直到所有点属于同一棵树，边集 E’ 就是一棵最小生成树。 时间复杂度：$O(eloge)$ Prim 算法更适合与解决边的绸密度更高的连通网。Kruskal 算法更适合于求边稀疏的网的最小生成树。 最短路径算法Dijkstra 算法 Dijkstra 算法使用了广度优先搜索解决带权有向图的单源最短路径问题。 Dijkstra 算法采用的是一种贪心的策略。 1）首先，声明一个数组 dis 保存从源点 s 到其他各点的最短距离；集合 T 保存已经找到最短路径的顶点。初始时，源点的路径权重赋为 0（dis[0] = 0）。对于顶点 s 能够到达的边 (s, t)，将 dis[t] 设为该条边的权重值，同时把不可达的顶点的路径长度设置为无穷大。初始时，集合 T 只包含源点 s。 2）从 dis 数组中选择最小值，则该值就是源点 s 到对应顶点的最短路径，并将该点添加到集合 T 中。 3）查看新加入的顶点可以到达的其他顶点，并比较经过新加入的顶点到达其他顶点的路径长度是否比从源点直接到达短，如果是，则将该较短路径长度更新到 dis 数组。 4）从 dis 数组选择其他最小值，重复上述步骤，直到 T 中包含图中所有顶点。 Floyd-Warshall 算法 Floyd-Warshall算法（Floyd-Warshall algorithm）是解决任意两点间的最短路径的一种算法，可以正确处理带权有向图最短路径问题，同时也被用于计算有向图的传递闭包。 时间复杂度为 $O(N^3)$，空间复杂度为 $O(N^2)$。 Floyd 算法是一个经典的动态规划算法，目标是寻找从点 i 到点 j 的最短路径。其核心思想是，在两个顶点之间插入一个或一个以上的中转点，比较经过与不经过中转点的距离哪个更短。 通过 Floyd 算法计算图 G = (V, E) 中任意两个顶点之间的最短路径时，需要引入两个矩阵，矩阵 D 中的元素 $a[i][j]$ 表示顶点 i (第 i 个顶点) 到顶点 j (第 j 个顶点) 的距离。矩阵 P 中的元素 $b[i][j]$，表示顶点 i 到顶点 j 经过了 $b[i][j]$ 记录的值所表示的顶点。 假设图 G 中顶点个数为 N，则需要对矩阵 D 和矩阵 P 进行 N 次更新。初始时，矩阵 D 中顶点 $a[i][j]$ 的距离为顶点 i 到顶点 j 的权值；如果 i 和 j 不相邻，则 $a[i][j]=∞$，矩阵 P 的值为顶点 $b[i][j]$ 的 j 的值。接下来开始，对矩阵 D 进行 N 次更新。第 1 次更新时，如果”$a[i][j]$ 的距离” &gt; “$a[i][0]+a[0][j]$”($a[i][0]+a[0][j]$ 表示” i 与 j 之间经过第 1 个顶点的距离”)，则更新 $a[i][j]$ 为”$a[i][0]+a[0][j]$”,更新 $b[i][j]=b[i][0]$。 同理，第 k 次更新时，如果”$a[i][j]$ 的距离” &gt; “$a[i][k-1]+a[k-1][j]$”，则更新 $a[i][j]$ 为”$a[i][k-1]+a[k-1][j]$”, $b[i][j]=b[i][k-1]$。更新 N 次之后，操作完成。 SPFA 算法 SPFA 算法是求解单源最短路径问题的一种算法，是对 Dijkstra 算法的优化。采取的方法是动态逼近法：设立一个先进先出的队列用来保存待优化的结点，优化时每次取出队首结点 u，并且用 u 点当前的最短路径估计值对离开 u 点所指向的结点 v 进行松弛操作，如果 v 点的最短路径估计值有所调整，且 v 点不在当前的队列中，就将 v 点放入队尾。这样不断从队列中取出结点来进行松弛操作，直至队列空为止。 https://blog.csdn.net/qq_35644234/article/details/61614581 6. 哈希表哈希表（散列表）是一种使用 哈希函数 组织数据，以支持快速插入和搜索的数据结构。 有两种不同类型的哈希表：哈希集合 和 哈希映射。 哈希集合 是集合数据结构的实现之一，用于存储非重复值。 哈希映射 是映射数据结构的实现之一，用于存储(key, value)键值对。 哈希表通过散列函数把元素的键值映射为下标，然后将数据存储在数组中对应下标的位置。按照键值查询元素时，用同样的散列函数，将键值转化数组下标，从对应的数组下标的位置取数据。 散列函数，可以把它定义成 hash(key)，其中 key 表示元素的键值，hash(key) 的值表示经过散列函数计算得到的散列值。 散列函数设计的基本要求： 散列函数计算得到的散列值是一个非负整数； 如果 key1 = key2，那 hash(key1) == hash(key2)； 如果 key1 ≠ key2，那 hash(key1) ≠ hash(key2)。 散列冲突 即便像业界著名的MD5、SHA、CRC等哈希算法，也会出现不满足以上要求的情况，即发生 散列冲突。 常用的散列冲突解决方法有两类，开放寻址法（open addressing）和链表法（chaining）。 当哈希表中空闲位置不多的时候，散列冲突的概率就会大大提高。为了尽可能保证哈希表的操作效率，一般情况下，我们会尽可能保证哈希表中有一定比例的空闲槽位。我们用装载因子（load factor）来表示空位的多少。 装载因子的计算公式是：哈希表的装载因子 = 填入表中的元素个数 / 哈希表的长度 装载因子越大，说明空闲位置越少，冲突越多，哈希表的性能会下降。当装载因子过大时，就需要对哈希表扩容。新申请一个更大的哈希表，将数据搬移到这个新哈希表中。针对数组的扩容，数据搬移操作比较简单。但是，针对哈希表的扩容，数据搬移操作要复杂很多。因为哈希表的大小变了，数据的存储位置也变了，所以我们需要通过散列函数重新计算每个数据的存储位置。 插入一个数据，最好情况下，不需要扩容，最好时间复杂度是 O(1)。最坏情况下，哈希表装载因子过高，启动扩容，我们需要重新申请内存空间，重新计算哈希位置，并且搬移数据，所以时间复杂度是 O(n)。用摊还分析法，均摊情况下，时间复杂度接近最好情况，就是 O(1)。 装载因子阈值需要选择得当。如果太大，会导致冲突过多；如果太小，会导致内存浪费严重。 开放寻址法：开放寻址法的核心思想是，如果出现了散列冲突，我们就重新探测一个空闲位置，将其插入。当数据量比较小、装载因子小的时候，适合采用开放寻址法。 线性探测（Linear Probing）：当我们往哈希表中插入数据时，如果某个数据经过散列函数散列之后，存储位置已经被占用了，我们就从当前位置开始，依次往后查找，看是否有空闲位置，直到找到为止。线性探测法其实存在很大问题。当哈希表中插入的数据越来越多时，散列冲突发生的可能性就会越来越大，空闲位置会越来越少，线性探测的时间就会越来越久。 链表法：在哈希表中，每个“桶（bucket）”或者“槽（slot）”会对应一条链表，所有散列值相同的元素我们都放到相同槽位对应的链表中。链表法比起开放寻址法，对大装载因子的容忍度更高。开放寻址法只能适用装载因子小于 1 的情况。接近 1 时，就可能会有大量的散列冲突，导致大量的探测、再散列等，性能会下降很多。但是对于链表法来说，只要散列函数的值随机均匀，即便装载因子变成 10，也就是链表的长度变长了而已，虽然查找效率有所下降，但是比起顺序查找还是快很多。 哈希表的应用场景： 安全加密：如：MD5、SHA 唯一标识：UUID 数据校验：数字签名 负载均衡：会话粘滞（session sticky）负载均衡算法。可以通过哈希算法，对客户端 IP 地址或者会话 ID 计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的值就是应该被路由到的服务器编号 数据分片 分布式存储：一致性哈希算法、虚拟哈希槽 7. 查找查找定义：根据给定的某个值，在查找表中确定一个其关键字等于给定值的数据元素（或记录）。 查找算法分类： 1）静态查找和动态查找 注：静态或者动态都是针对查找表而言的。动态表指查找表中有删除和插入操作的表。 2）无序查找和有序查找 无序查找：被查找数列有序无序均可； 有序查找：被查找数列必须为有序数列。 查找成功的平均查找长度为：ASL = Pi * Ci。 Pi：查找表中第 i 个数据元素的概率。Ci：找到第 i 个数据元素时已经比较过的次数。 七大查找算法： 顺序查找顺序查找适合于存储结构为顺序存储或链接存储的线性表。 基本思想：顺序查找也称为线形查找，属于无序查找算法。从数据结构线形表的一端开始，顺序扫描，依次将扫描到的结点关键字与给定值 k 相比较，若相等则表示查找成功；若扫描结束仍没有找到关键字等于 k 的结点，表示查找失败。 复杂度分析： 查找成功时的平均查找长度为：（假设每个数据元素的概率相等） $ASL = (1/n) * (1+2+3+…+n) = (n+1)/2$;当查找不成功时，需要 n+1 次比较，时间复杂度为 O(n); 所以，顺序查找的时间复杂度为O(n)。 123456int SequenceSearch(int a[], int value, int n) { for (int i = 0; i &lt; n; i++) if (a[i] == value) return i; return -1;} 二分查找元素必须是有序的，如果是无序的则要先进行排序操作。 基本思想：也称为折半查找，属于有序查找算法。用给定值 k 先与中间结点的关键字比较，中间结点把线形表分成两个子表，若相等则查找成功；若不相等，再根据 k 与该中间结点关键字的比较结果确定下一步查找哪个子表，这样递归进行，直到查找到或查找结束发现表中没有这样的结点。 复杂度分析：最坏情况下，关键词比较次数为 $log_2(n+1)$，且期望时间复杂度为 $O(log_2n)$； 注：折半查找的前提条件是需要有序表顺序存储，对于静态查找表，一次排序后不再变化，折半查找能得到不错的效率。但对于需要频繁执行插入或删除操作的数据集来说，维护有序的排序会带来不小的工作量，那就不建议使用。——《大话数据结构》 12345678910111213141516171819202122232425262728// 二分查找（折半查找）int BinarySearch1(int a[], int value, int n) { int low, high, mid; low = 0; high = n-1; while(low &lt;= high) { mid = (low + high) / 2; if (a[mid] == value) return mid; if (a[mid] &gt; value) high = mid-1; if (a[mid] &lt; value) low = mid+1; } return -1;}// 二分查找，递归版本int BinarySearch2(int a[], int value, int low, int high) { int mid = low + (high - low) / 2; if (a[mid] == value) return mid; if (a[mid] &gt; value) return BinarySearch2(a, value, low, mid-1); if (a[mid] &lt; value) return BinarySearch2(a, value, mid+1, high); return -1;} 插值查找在介绍插值查找之前，首先考虑一个新问题，为什么上述算法一定要是折半，而不是折四分之一或者折更多呢？ 打个比方，在英文字典里面查“apple”，你下意识翻开字典是翻前面的书页还是后面的书页呢？如果再让你查“zoo”，你又怎么查？很显然，这里你绝对不会是从中间开始查起，而是有一定目的的往前或往后翻。又比如要在取值范围1 ~ 10000 之间 100 个元素从小到大均匀分布的数组中查找5， 我们自然会考虑从数组下标较小的开始查找。 经过以上分析，折半查找这种查找方式，不是自适应的（也就是说是傻瓜式的），我们可以将查找的点改进为如下： $mid = low + (key - a[low]) / (a[high] - a[low]) * (high - low)$ 也就是将上述的比例参数1/2改进为自适应的，根据关键字在整个有序表中所处的位置，让mid值的变化更靠近关键字key，这样也就间接地减少了比较次数。当然，差值查找也属于有序查找。 复杂度分析：查找成功或者失败的时间复杂度均为 $O(log_2(log_2n))$。 123456789int InsertionSearch(int a[], int value, int low, int high) { int mid = low + (value - a[low]) / (a[high] - a[low]) * (high-low); if (a[mid] == value) return mid; if (a[mid] &gt; value) return InsertionSearch(a, value, low, mid-1); if (a[mid] &lt; value) return InsertionSearch(a, value, mid+1, high);} 斐波那契查找基本思想：也是二分查找的一种提升算法，通过运用黄金比例的概念在数列中选择查找点进行查找，提高查找效率。同样地，斐波那契查找也属于一种有序查找算法。 斐波那契查找与折半查找很相似，他是根据斐波那契序列的特点对有序表进行分割的。他要求开始表中记录的个数为某个斐波那契数小 1，及 n = F(k) - 1; 开始将 k 值与第 F(k-1) 位置的记录进行比较 (即 mid = low + F(k-1) - 1)，比较结果也分为三种： 1）相等，mid位置的元素即为所求 2）>，low = mid + 1, k -= 2; 说明：low = mid + 1说明待查找的元素在 [mid + 1, high] 范围内，k -= 2 说明范围 [mid+1, high] 内的元素个数为 n - (F(k-1)) = Fk-1 - F(k-1) = Fk - F(k-1) - 1 = F(k-2) - 1个，所以可以递归的应用斐波那契查找。 3）&lt;，high = mid-1, k -= 1。 说明：low = mid - 1说明待查找的元素在 [low, mid - 1] 范围内，k -= 1 说明范围 [low, mid - 1] 内的元素个数为 F(k-1) - 1 个，所以可以递归的应用斐波那契查找。 复杂度分析：最坏情况下，时间复杂度为 $O(log_2n)$，且其期望复杂度也为 $O(log_2n)$。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859#include &quot;stdafx.h&quot;#include &lt;memory&gt;#include &lt;iostream&gt;using namespace std;const int max_size=20; // 斐波那契数组的长度/*构造一个斐波那契数组*/ void Fibonacci(int * F) { F[0] = 0; F[1] = 1; for (int i = 2; i &lt; max_size; ++i) F[i] = F[i-1] + F[i-2];}/*定义斐波那契查找法*/ int FibonacciSearch(int *a, int n, int key) { // a为要查找的数组,n为要查找的数组长度,key为要查找的关键字 int low = 0; int high = n-1; int F[max_size]; Fibonacci(F); // 构造一个斐波那契数组F int k = 0; while (n &gt; F[k] - 1) // 计算n位于斐波那契数列的位置 ++k; int * temp; // 将数组a扩展到F[k]-1的长度 temp = new int [F[k] - 1]; memcpy(temp, a, n * sizeof(int)); for (int i = n; i &lt; F[k] - 1; ++i) temp[i] = a[n-1]; while (low &lt;= high) { int mid = low + F[k-1] - 1; if (key &lt; temp[mid]) { high = mid - 1; k -= 1; } else if (key &gt; temp[mid]) { low = mid + 1; k -= 2; } else { if (mid &lt; n) return mid; // 若相等则说明mid即为查找到的位置 else return n - 1; // 若mid&gt;=n则说明是扩展的数值,返回n-1 } } delete [] temp; return -1;}int main() { int a[] = {0,16,24,35,47,59,62,73,88,99}; int key = 100; int index = FibonacciSearch(a, sizeof(a) / sizeof(int), key); cout &lt;&lt; key &lt;&lt; &quot; is located at:&quot; &lt;&lt; index; return 0;} 树表查找最简单的树表查找算法——二叉树查找算法 基本思想：二叉查找树是先对待查找的数据进行生成树，确保树的左分支的值小于右分支的值，然后在就行和每个节点的父节点比较大小，查找最适合的范围。 这个算法的查找效率很高，但是如果使用这种查找方法要首先创建树。 二叉查找树（BinarySearch Tree，也叫二叉搜索树，或称二叉排序树Binary Sort Tree）或者是一棵空树，或者是具有下列性质的二叉树： 1）若任意节点的左子树不空，则左子树上所有结点的值均小于它的根结点的值； 2）若任意节点的右子树不空，则右子树上所有结点的值均大于它的根结点的值； 3）任意节点的左、右子树也分别为二叉查找树。 二叉查找树性质：对二叉查找树进行中序遍历，即可得到有序的数列。 复杂度分析：它和二分查找一样，插入和查找的时间复杂度均为O(logn)，但是在最坏的情况下仍然会有O(n)的时间复杂度。原因在于插入和删除元素的时候，树没有保持平衡。我们追求的是在最坏的情况下仍然有较好的时间复杂度，这就是平衡查找树设计的初衷。 平衡查找树之 2-3 查找树（2-3 Tree） 2-3查找树定义：和二叉树不一样，2-3 树运行每个节点保存 1 个或者两个的值。对于普通的 2 节点 (2-node)，他保存 1 个 key 和左右两个自己点。对应 3 节点 (3-node)，保存两个 Key，2-3 查找树的定义如下： 1）要么为空，要么： 2）对于 2 节点，该节点保存一个 key 及对应 value，以及两个指向左右节点的节点，左节点也是一个 2-3 节点，所有的值都比 key 要小，右节点也是一个 2-3 节点，所有的值比 key 要大。 3）对于 3 节点，该节点保存两个 key 及对应 value，以及三个指向左中右的节点。左节点也是一个 2-3 节点，所有的值均比两个 key 中的最小的 key 还要小；中间节点也是一个 2-3 节点，中间节点的 key 值在两个跟节点 key 值之间；右节点也是一个 2-3 节点，节点的所有 key 值比两个 key 中的最大的 key 还要大。 2-3查找树的性质： 1）如果中序遍历 2-3 查找树，就可以得到排好序的序列； 2）在一个完全平衡的 2-3 查找树中，根节点到每一个为空节点的距离都相同。（这也是平衡树中“平衡”一词的概念，根节点到叶节点的最长距离对应于查找算法的最坏情况，而平衡树中根节点到叶节点的距离都一样，最坏情况也具有对数复杂度） 复杂度分析： 2-3 树的查找效率与树的高度是息息相关的。 在最坏的情况下，也就是所有的节点都是 2-node 节点，查找效率为 lgN 在最好的情况下，所有的节点都是 3-node 节点，查找效率为 log3N 约等于 0.631lgN 距离来说，对于 1 百万个节点的 2-3 树，树的高度为 12-20 之间，对于 10 亿个节点的 2-3 树，树的高度为 18-30 之间。 对于插入来说，只需要常数次操作即可完成，因为他只需要修改与该节点关联的节点即可，不需要检查其他节点，所以效率和查找类似。 平衡查找树之红黑树（Red-Black Tree） 2-3 查找树能保证在插入元素之后能保持树的平衡状态，最坏情况下即所有的子节点都是 2-node，树的高度为 lgn，从而保证了最坏情况下的时间复杂度。但是 2-3 树实现起来比较复杂，于是就有了一种简单实现 2-3 树的数据结构，即红黑树（Red-Black Tree）。 基本思想：红黑树的思想就是对 2-3 查找树进行编码，尤其是对 2-3 查找树中的 3-node 节点添加额外的信息。红黑树中将节点之间的链接分为两种不同类型，红色链接，他用来链接两个 2-node 节点来表示一个 3-node 节点。黑色链接用来链接普通的 2-3 节点。特别的，使用红色链接的两个 2-node来表示一个 3-node 节点，并且向左倾斜，即一个 2-node 是另一个 2-node 的左子节点。这种做法的好处是查找的时候不用做任何修改，和普通的二叉查找树相同。 红黑树的定义： 红黑树是一种具有红色和黑色链接的平衡查找树，同时满足： 红色节点向左倾斜 一个节点不可能有两个红色链接 整个树完全黑色平衡，即从根节点到所以叶子结点的路径上，黑色链接的个数都相同。 红黑树的性质：整个树完全黑色平衡，即从根节点到所以叶子结点的路径上，黑色链接的个数都相同（2-3树的第2）性质，从根节点到叶子节点的距离都相等）。 复杂度分析：最坏的情况就是，红黑树中除了最左侧路径全部是由 3-node 节点组成，即红黑相间的路径长度是全黑路径长度的 2 倍。 红黑树的平均高度大约为 logn。 B树和B+树（B Tree/B+ Tree） B树，概括来说是一个节点可以拥有多于 2 个子节点的二叉查找树。与自平衡二叉查找树不同，B树为系统最优化大块数据的读和写操作。B-tree 算法减少定位记录时所经历的中间过程，从而加快存取速度。普遍运用在数据库和文件系统。 B树定义： B树可以看作是对 2-3 查找树的一种扩展，即他允许每个节点有 M-1 个子节点。 根节点至少有两个子节点 每个节点有 M-1 个 key，并且以升序排列 位于 M-1 和 M key 的子节点的值位于 M-1 和 M key 对应的 Value 之间 其它节点至少有 M/2 个子节点 B+ 树定义： B+ 树是对 B树的一种变形树，它与 B树的差异在于： 有 k 个子结点的结点必然有 k 个关键码； 非叶结点仅具有索引作用，跟记录有关的信息均存放在叶结点中。 树的所有叶结点构成一个有序链表，可以按照关键码排序的次序遍历全部记录。 B树和B+树的区别在于，B+树的非叶子结点只包含导航信息，不包含实际的值，所有的叶子结点和相连的节点使用链表相连，便于区间查找和遍历。 B+ 树的优点在于： 由于 B+树在内部节点上不好含数据信息，因此在内存页中能够存放更多的 key。 数据存放的更加紧密，具有更好的空间局部性。因此访问叶子几点上关联的数据也具有更好的缓存命中率。 B+树的叶子结点都是相链的，因此对整棵树的便利只需要一次线性遍历叶子结点即可。而且由于数据顺序排列并且相连，所以便于区间查找和搜索。而B树则需要进行每一层的递归遍历。相邻的元素可能在内存中不相邻，所以缓存命中性没有B+树好。 但是B树也有优点，其优点在于，由于 B树的每一个节点都包含 key 和 value，因此经常访问的元素可能离根节点更近，因此访问也更迅速。 分块查找分块查找又称索引顺序查找，它是顺序查找的一种改进方法。 算法思想：将 n 个数据元素”按块有序”划分为 m 块（m ≤ n）。每一块中的结点不必有序，但块与块之间必须”按块有序”；即第 1 块中任一元素的关键字都必须小于第 2 块中任一元素的关键字；而第 2 块中任一元素又都必须小于第 3 块中的任一元素，…… 算法流程：step1 先选取各块中的最大关键字构成一个索引表；step2 查找分两个部分：先对索引表进行二分查找或顺序查找，以确定待查记录在哪一块中；然后，在已确定的块中用顺序法进行查找。 B/B+树常用于文件系统和数据库系统中，它通过对每个节点存储个数的扩展，使得对连续的数据能够进行较快的定位和访问，能够有效减少查找时间，提高存储的空间局部性从而减少IO操作。 哈希查找算法流程： 1）用给定的哈希函数构造哈希表； 2）根据选择的冲突处理方法解决地址冲突； 常见的解决冲突的方法：拉链法和线性探测法。 3）在哈希表的基础上执行哈希查找。 复杂度分析：单纯论查找复杂度：对于无冲突的 Hash 表而言，查找复杂度为 O(1)（注意，在查找之前我们需要构建相应的Hash表）。 Hash 是一种典型以空间换时间的算法，比如原来一个长度为 100 的数组，对其查找只需要遍历且匹配相应记录即可，从空间复杂度上来看，假如数组存储的是 byte 类型数据，那么该数组占用 100byte 空间。现在我们采用Hash算法，我们前面说的 Hash 必须有一个规则，约束键与存储位置的关系，那么就需要一个固定长度的 hash 表，此时，仍然是 100byte 的数组，假设我们需要的 100byte 用来记录键与位置的关系，那么总的空间为 200byte，而且用于记录规则的表大小会根据规则，大小可能是不定的。 8. 排序十大排序算法： 冒泡排序冒泡排序循环走访要排序的数列，一次比较两个相邻元素，如果它们的顺序错误就把它们交换过来，直到所有元素排序完成。每次排序都会找到当前数列最大（最小）的值，并放到最后。 123456789101112void bubbleSort(int *arr, int n) { int temp = 0; for (int i = n - 1; i &gt; 0; i--) { // 每次需要排序的长度 for (int j = 0; j &lt; i; j++) { // 从第一个元素到第i个元素 if (arr[j] &gt; arr[j + 1]) { temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; } } }} 1234567def bubble_sort(arr: List[int]) -&gt; List[int]: N = len(arr) for i in range(N - 1, 0, -1): for j in range(i): if arr[j] &gt; arr[j + 1]: arr[j], arr[j + 1] = arr[j + 1], arr[j] return arr 稳定性 在相邻元素相等时，它们并不会交换位置，所以，冒泡排序是稳定排序。 适用场景 冒泡排序思路简单，代码也简单，特别适合小数据的排序。但是，由于算法复杂度较高，在数据量大的时候不适合使用。 算法优化 在数据完全有序的时候展现出最优时间复杂度，为O(n)。其他情况下，几乎总是O( n2 )。因此，算法在数据基本有序的情况下，性能最好。 增加一个swap的标志，当前一轮没有进行交换时，说明数组已经有序，没有必要再进行下一轮的循环了，直接退出。 1234567891011121314151617void bubbleSort(int *arr, int n) { int temp = 0; boolean swap; for (int i = n - 1; i &gt; 0; i--) { // 每次需要排序的长度 swap = false; for (int j = 0; j &lt; i; j++) { // 从第一个元素到第i个元素 if (arr[j] &gt; arr[j + 1]) { temp = arr[j]; arr[j] = arr[j + 1]; arr[j + 1] = temp; swap = true; } } if (swap == false) break; }} 选择排序选择排序也是一种交换排序算法，可以认为选择排序是冒泡排序的一种改进。 在未排序序列中找到最小（大）元素，与排序序列起始位置元素交换，直到所有元素均排序完毕。 12345678910111213141516void selectionSort(int *arr, int n) { int temp, min = 0; for (int i = 0; i &lt; n; i++) { min = i; // 循环查找最小值 for (int j = i + 1; j &lt; n; j++) { if (arr[min] &gt; arr[j]) min = j; } if (min != i) { temp = arr[i]; arr[i] = arr[min]; arr[min] = temp; } }} 12345678910def select_sort(arr: List[int]) -&gt; List[int]: N = len(arr) for i in range(N): min_idx = i for j in range(i, N): if arr[j] &lt; arr[min_idx]: min_idx = j if min_idx != i: arr[i], arr[min_idx] = arr[min_idx], arr[i] return arr 稳定性 用数组实现的选择排序是不稳定的，用链表实现的选择排序是稳定的。一般提到排序算法时，大家往往会默认是数组实现，所以选择排序是不稳定的。 适用场景 选择排序实现也比较简单，并且由于在各种情况下复杂度波动小，因此一般是优于冒泡排序的。 插入排序插入排序通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。 把待排序的数组分成已排序和未排序两部分，初始的时候把第一个元素认为是已排好序的。 从第二个元素开始，在已排好序的子数组中寻找到该元素合适的位置并插入该位置。 重复上述过程直到最后一个元素被插入有序子数组中。 1234567891011void insertionSort(int *arr, int n){ for (int i = 1; i &lt; n; ++i){ int value = arr[i]; int position = i; while (position &gt; 0 &amp;&amp; arr[position-1] &gt; value) { arr[position] = arr[position - 1]; position--; } arr[position] = value; }} 12345678910def insert_sort(arr): N = len(arr) for i in range(N): v = arr[i] pos = i while pos &gt; 0 and arr[pos - 1] &gt; v: arr[pos] = arr[pos - 1] pos -= 1 arr[pos] = v return arr 稳定性 由于只需要找到不大于当前数的位置而并不需要交换，因此，直接插入排序是稳定的排序方法。 归并排序归并排序是建立在归并操作上的一种有效的排序算法。是采用分治法的一个非常典型的应用。 递归法 申请空间，使其大小为两个已经排序序列之和，该空间用来存放合并后的序列 设定两个指针，最初位置分别为两个已经排序序列的起始位置 比较两个指针所指向的元素，选择相对小的元素放入到合并空间，并移动指针到下一位置 重复步骤3直到某一指针到达序列尾 将另一序列剩下的所有元素直接复制到合并序列尾 迭代法 将序列每相邻两个数字进行归并操作，形成ceil(n/2)个序列，排序后每个序列包含两/一个元素 若此时序列数不是1个则将上述序列再次归并，形成ceil(n/4)个序列，每个序列包含四/三个元素 重复步骤2，直到所有元素排序完毕，即序列数为1 1234567891011121314151617181920212223242526272829303132void mergeSort(int *arr, int n) { int temp[n]; internalMergeSort(arr, temp, 0, n);}void internalMergeSort(int *arr, int *temp, int left, int right) { //当left==right的时，已经不需要再划分了 if (left &lt; right) { int middle = (left + right) / 2; internalMergeSort(arr, temp, left, middle); //左子数组 internalMergeSort(arr, temp, middle+1, right); //右子数组 mergeSortedArray(arr, temp, left, middle, right); //合并两个子数组 }}// 合并两个有序子序列void mergeSortedArray(int *arr, int *temp, int left, int middle, int right) { int i = left; int j = middle + 1; int k = 0; while (i &lt;= middle &amp;&amp; j &lt;= right) { temp[k++] = arr[i] &lt;= arr[j] ? arr[i++] : arr[j++]; } while (i &lt;= middle) { temp[k++] = arr[i++]; } while (j &lt;= right) { temp[k++] = arr[j++]; } //把数据复制回原数组 for (i=0; i&lt;k; ++i) { arr[left+i] = temp[i]; }} 123456789101112131415161718192021def merge(left, right): result = [] i, j = 0, 0 while i &lt; len(left) and j &lt; len(right): if left[i] &lt;= right[j]: result.append(left[i]) i += 1 else: result.append(right[j]) j += 1 result += left[i:] result += right[j:] return resultdef merge_sort(arr): if len(arr) &lt;= 1: return arr mid = len(arr) // 2 left = merge_sort(arr[:mid]) right = merge_sort(arr[mid:]) return merge(left, right) 稳定性 因为我们在遇到相等的数据的时候必然是按顺序“抄写”到辅助数组上的，所以，归并排序同样是稳定算法。 适用场景 归并排序在数据量比较大的时候也有较为出色的表现（效率上），但是其空间复杂度 O(n) 使得在数据量特别大的时候（例如，1千万数据）几乎不可接受。 快速排序快速排序是一个知名度极高的排序算法，其对于大数据的优秀排序性能和相同复杂度算法中相对简单的实现使它注定得到比其他算法更多的宠爱。 从数列中挑出一个元素，称为”基准”（pivot）， 重新排序数列，所有比基准值小的元素摆放在基准前面，所有比基准值大的元素摆在基准后面（相同的数可以到任何一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。 递归地（recursively）把小于基准值元素的子数列和大于基准值元素的子数列排序。 123456789101112131415161718192021222324252627void quickSort(int *arr, int n) { qsort(arr, 0, n);}void qsort(int *arr, int low, int high) { if (low &gt;= high) return; int pivot = partition(arr, low, high); //将数组分为两部分 qsort(arr, low, pivot - 1); //递归排序左子数组 qsort(arr, pivot + 1, high); //递归排序右子数组}int partition(int *arr, int low, int high) { int pivot = arr[low]; //基准 while (low &lt; high) { while (low &lt; high &amp;&amp; arr[high] &gt;= pivot) --high; arr[low] = arr[high]; //交换比基准大的记录到左端 while (low &lt; high &amp;&amp; arr[low] &lt;= pivot) ++low; arr[high] = arr[low]; //交换比基准小的记录到右端 } //扫描完成，基准到位 arr[low] = pivot; //返回的是基准的位置 return low;} 12345678910111213141516def quick_sort(arr, i, j): if i &gt;= j: return arr pivot = arr[i] low, high = i, j while i &lt; j: while i &lt; j and arr[j] &gt;= pivot: j -= 1 arr[i] = arr[j] while i &lt; j and arr[i] &lt;= pivot: i += 1 arr[j] = arr[i] arr[j] = pivot quick_sort(arr, low, i - 1) quick_sort(arr, i + 1, high) return arr 稳定性 快速排序并不是稳定的。这是因为我们无法保证相等的数据按顺序被扫描到和按顺序存放。 适用场景 快速排序在大多数情况下都是适用的，尤其在数据量大的时候性能优越性更加明显。但是在必要的时候，需要考虑下优化以提高其在最坏情况下的性能。 堆排序堆排序(Heapsort)是指利用堆积树（堆）这种数据结构所设计的一种排序算法，它是选择排序的一种。可以利用数组的特点快速定位指定索引的元素。堆排序就是把最大堆堆顶的最大数取出，将剩余的堆继续调整为最大堆，再次将堆顶的最大数取出，这个过程持续到剩余数只有一个时结束。 最大堆调整（Max-Heapify）：将堆的末端子节点作调整，使得子节点永远小于父节点 创建最大堆（Build-Max-Heap）：将堆所有数据重新排序，使其成为最大堆 堆排序（Heap-Sort）：移除位在第一个数据的根节点，并做最大堆调整的递归运算 继续进行下面的讨论前，需要注意的一个问题是：数组都是 Zero-Based，这就意味着我们的堆数据结构模型要发生改变 稳定性 堆排序存在大量的筛选和移动过程，属于不稳定的排序算法。 适用场景 堆排序在建立堆和调整堆的过程中会产生比较大的开销，在元素少的时候并不适用。但是，在元素比较多的情况下，还是不错的一个选择。尤其是在解决诸如“前n大的数”一类问题时，几乎是首选算法。 希尔排序希尔排序是第一个突破O(n2)的排序算法，它是简单插入排序的改进版。希尔排序的提出，主要基于以下两点： 插入排序算法在数组基本有序的情况下，可以近似达到O(n)复杂度，效率极高。 但插入排序每次只能将数据移动一位，在数组较大且基本无序的情况下性能会迅速恶化。 先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，具体算法描述： 选择一个增量序列t1，t2，…，tk，其中 ti &gt; tj，tk=1； 按增量序列个数 k，对序列进行 k 趟排序； 每趟排序，根据对应的增量 ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为 1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 Donald Shell增量 123456789101112void shellSort(int *arr, int n) { int temp; for (int delta = n; delta &gt;= 1; delta /= 2) { //对每个增量进行一次排序 for (int i = delta; i &lt; n; i++) { for (int j = i; j &gt;= delta &amp;&amp; arr[j] &lt; arr[j - delta]; j -= delta) { //注意每个地方增量和差值都是delta temp = arr[j - delta]; arr[j - delta] = arr[j]; arr[j] = temp; } } }} 希尔排序的增量数列可以任取，需要的唯一条件是最后一个一定为1（因为要保证按1有序）。但是，不同的数列选取会对算法的性能造成极大的影响。 第一种增量是最初 Donald Shell 提出的增量，即折半降低直到 1。据研究，使用希尔增量，其时间复杂度还是 $O(n^2)$。 第二种增量 Hibbard：{1, 3, …, 2k-1}。该增量序列的时间复杂度大约是 $O(n^{1.5})$。 稳定性 在一次插入中我们能确保不移动相同元素的顺序，但在多次的插入中，相同元素完全有可能在不同的插入轮次被移动，最后稳定性被破坏，因此，Shell排序不是一个稳定的算法。 适用场景 Shell 排序虽然快，但是毕竟是插入排序，其数量级并没有后起之秀——快速排序 O(nlogn) 快。在大量数据面前，Shell排序不是一个好的算法。但是，中小型规模的数据完全可以使用它。 计数排序计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 找出待排序的数组中最大和最小的元素； 统计数组中每个值为 i 的元素出现的次数，存入数组 C 的第 i 项； 对所有的计数累加（从 C 中的第一个元素开始，每一项和前一项相加）； 反向填充目标数组：将每个元素 i 放在新数组的第 C(i) 项，每放一个元素就将 C(i) 减去1。 12345678910111213141516171819202122232425262728293031void countSort(int *a, int n, int max, int min) { int b[n]; // 存储数组 int count[max - min + 1]; // 计数数组 for (int num = min; num &lt;= max; num++) { // 初始化各元素值为0，数组下标从0开始因此减min count[num - min] = 0; } for (int i = 0; i &lt; a.length; i++) { int num = a[i]; count[num - min]++; // 每出现一个值，计数数组对应元素的值+1 } for (int num = min + 1; num &lt;= max; num++) { // 加总数组元素的值为计数数组对应元素及左边所有元素的值的总和 count[num - min] += sum[num - min - 1]; } for (int i = 0; i &lt; a.length; i++) { int num = a[i]; // 源数组第i位的值 int index = count[num - min] - 1; // 加总数组中对应元素的下标 b[index] = num; // 将该值存入存储数组对应下标中 count[num - min]--; // 加总数组中，该值的总和减少1。 } // 将存储数组的值一一替换给源数组 for(int i=0; i &lt; n; i++) { a[i] = b[i]; }} 稳定性 最后给 b 数组赋值是倒着遍历的，而且放进去一个就将 C 数组对应的值（表示前面有多少元素小于或等于 A[i]）减去一。如果有相同的数 x1,x2，那么相对位置后面那个元素 x2 放在（比如下标为 4 的位置），相对位置前面那个元素 x1 下次进循环就会被放在 x2 前面的位置 3。从而保证了稳定性。 适用场景 排序目标要能够映射到整数域，其最大值最小值应当容易辨别。例如高中生考试的总分数，显然用0-750就OK啦；又比如一群人的年龄，用个0-150应该就可以了，再不济就用0-200喽。另外，计数排序需要占用大量空间，它比较适用于数据比较集中的情况。 桶排序桶排序又叫箱排序，是计数排序的升级版，它的工作原理是将数组分到有限数量的桶子里，然后对每个桶子再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排序），最后将各个桶中的数据有序的合并起来。 找出待排序数组中的最大值max、最小值min 我们使用 动态数组 ArrayList 作为桶，桶里放的元素也用 ArrayList 存储。桶的数量为 (max-min) / arr.length + 1 遍历数组 arr，计算每个元素 arr[i] 放的桶 每个桶各自排序 遍历桶数组，把排序好的元素放进输出数组 稳定性 可以看出，在分桶和从桶依次输出的过程是稳定的。但是，由于我们在对每个桶进行排序时使用了其他算法，所以，桶排序的稳定性依赖于这一步。如果我们使用了快排，显然，算法是不稳定的。 适用场景 桶排序可用于最大最小值相差较大的数据情况，但桶排序要求数据的分布必须均匀，否则可能导致数据都集中到一个桶中。比如[104,150,123,132,20000], 这种数据会导致前4个数都集中到同一个桶中。导致桶排序失效。 基数排序基数排序 (Radix Sort) 是桶排序的扩展，它的基本思想是：将整数按位数切割成不同的数字，然后按每个位数分别比较。排序过程：将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后, 数列就变成一个有序序列。 取得数组中的最大数，并取得位数； arr为原始数组，从最低位开始取每个位组成radix数组； 对 radix 进行计数排序（利用计数排序适用于小范围数的特点）； 稳定性 通过上面的排序过程可以看到，每一轮映射和收集操作，都保持从左到右的顺序进行，如果出现相同的元素，则保持他们在原始数组中的顺序。可见，基数排序是一种稳定的排序。 适用场景 基数排序要求较高，元素必须是整数，整数时长度 10W 以上，最大值 100W 以下效率较好，但是基数排序比其他排序好在可以适用字符串，或者其他需要根据多个条件进行排序的场景，例如日期，先排序日，再排序月，最后排序年 ，其它排序算法可是做不了的。 总结 排序算法 时间复杂度 最好/最坏 空间复杂度 稳定性 冒泡排序 $O(n^2)$ $O(n), O(n^2)$ O(1) 稳定 选择排序 $O(n^2)$ $O(n), O(n^2)$ O(1) 不稳定 插入排序 $O(n^2)$ $O(n), O(n^2)$ O(1) 稳定 归并排序 $O(nlog_2n)$ $O(nlog_2n), O(nlog_2n)$ O(n) 稳定 快速排序 $O(nlog_2n)$ $O(nlog_2n), O(n^2)$ $O(log_2n)$ 不稳定 堆排序 $O(nlog_2n)$ $O(nlog_2n), O(nlog_2n)$ O(1) 不稳定 希尔排序 $O(n^{1.5})$ O(1) 不稳定 计数排序 $O(n+k)$ O(n+k), O(n+k) O(n+k) 稳定 桶排序 $O(n+k)$ O(n+k), O(n+k) O(n+k) 稳定 基数排序 $O(n*k)$ O(n+k) 稳定 9. 字符串KMP 算法KMP算法是一种改进的字符串匹配算法，关键是利用匹配失败后的信息，尽量减少模式串与主串的匹配次数以达到快速匹配的目的。 next 数组（前缀数组） https://www.ruanyifeng.com/blog/2013/05/Knuth%E2%80%93Morris%E2%80%93Pratt_algorithm.html 参考资料： https://github.com/dunwu/algorithm-tutorial","link":"2022/10/20/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"预训练模型合集","text":"NLUBERT2018 | BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Previous work ELMo (Peters et al., 2018): Deep contextualized word representations. ​ They extract context-sensitive features from a left-to-right and a right-to-left language model. GPT (Radford et al., 2018): Improving language understanding with unsupervised learning. Cloze procedure (Taylor, 1953): A new tool for measuring readability. Transformer (Vaswani et al., 2017): Attention is all you need. Contributions demonstrate the importance of bidirectional pre-training for language representations. show that pre-trained representations reduce the need for many heavily-engineered task-specific architectures. BERT advances the state of the art for eleven NLP tasks. Main concepts There are two steps in this framework: pre-training and fine-tuning. During pre-training, the model is trained on unlabeled data over different pre-training tasks. For fine-tuning, the BERT model is first initialized with the pre-trained parameters, and all of the parameters are fine-tuned using labeled data from the downstream tasks. BERT’s model architecture is a multi-layer bidirectional Transformer encoder. we pre-train BERT using two unsupervised tasks: Masked LM and Next Sentence Prediction (NSP). Masked LM: simply mask 15% of the input tokens at random, and then predict those masked tokens. NSP: choose the sentences A and B for each pre-training example, and then predict whether B is the next sentence of A. ERNIE2019 | Ernie: Enhanced representation through knowledge integration Inspired by the masking strategy of BERT (Devlin et al., 2018), ERNIE is designed to learn language representation enhanced by knowledge masking strategies, which includes entity-level masking and phrase-level masking. XLNetNIPS 2019 | XLNet: Generalized Autoregressive Pretraining for Language Understanding AE vs AR Autoregressive (AR) language modeling and autoencoding (AE) have been the two most successful pretraining objectives. AR language modeling seeks to estimate the probability distribution of a text corpus with an autoregressive model. Since an AR language model is only trained to encode a uni-directional context (either forward or backward), it is not effective at modeling deep bidirectional contexts. AE based pretraining does not perform explicit density estimation but instead aims to reconstruct the original data from corrupted input. Contributions we propose the permutation language modeling objective that not only retains the benefits of AR models but also allows models to capture bidirectional contexts. XLNet achieves substantial improvement over previous pretraining objectives on various tasks. RoBERTa2019 | RoBERTa: A Robustly Optimized BERT Pretraining Approach We find that BERT was significantly undertrained, and can match or exceed the performance of every model published after it. Our modifications are simple, they include: training the model longer, with bigger batches, over more data removing the next sentence prediction objective training on longer sequences dynamically changing the masking pattern applied to the training data ZEN2019 | ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations ZEN incorporates the comprehensive information of both the character sequence and words or phrases it contains (N-gram Representations). MacBERTEMNLP 2020 | Revisiting Pre-trained Models for Chinese Natural Language Processing Contributions use whole word masking (wwm) as well as N-gram masking strategies for selecting candidate tokens for masking, with a percentage of 40%, 30%, 20%, 10% for word-level unigram to 4-gram. Instead of masking with [MASK] token, which never appears in the fine-tuning stage, we propose to use similar words for the masking purpose. we perform sentence-order prediction (SOP) task as introduced by ALBERT instead of NSP. NEZHA2019 | NEZHA: Neural Contextualized Representation for Chinese Language Understanding Based on BERT and trained on Chinese text. Some techniques: effective positional encoding scheme, Whole Word Masking, Mixed Precision Training, LAMB Optimizer. Functional relative positional encoding Each dimension of the positional encoding corresponds to a sinusoid, and the sinusoidal functions for different dimensions have different wavelengths. Mixed Precision Training The technique can speed up the training by 2-3 times and also reduce the space consumption of the model, as a result of which, a larger batch size could be utilized. LAMB Optimizer The LAMB optimizer is designed for the large batch-size synchronous distributed training of deep neuron networks. The optimizer speeds up the training of BERT by using a very large batch size (up to more than 30k) without incurring a loss of the performance and even obtains the state-of-the-art performance in many tasks. RoFormer2021 | RoFormer: Enhanced Transformer with Rotary Position Embedding We investigate various methods to encode positional information in transformer-based language models and propose a novel implementation named Rotary Position Embedding(RoPE). flexibility of being expand to any sequence lengths decaying inter-token dependency with increasing relative distances capability of equipping the linear self-attention with relative position information WoBERT2020 | 以词为基本单位的中文BERT（Word-based BERT） 在哈工大开源的 RoBERTa-wwm-ext 基础上进行继续预训练，预训练任务为 MLM。此外，我们还提供了 WoNEZHA，这是基于华为开源的 NEZHA 进行再预训练的，训练细节跟 WoBERT 基本一样。NEZHA的模型结构跟BERT相似，不同的是它使用了相对位置编码，而 BERT 用的是绝对位置编码，因此理论上 NEZHA 能处理的文本长度是无上限的。 ALBERTICLR 2020 | ALBERT: A Lite BERT for Self-Supervised Learning of Language Representations At some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter reduction techniques to lower memory consumption and increase the training speed of BERT. The first one is a factorized embedding parameterization. By decomposing the large vocabulary embedding matrix into two small matrices, we separate the size of the hidden layers from the size of vocabulary embedding. The second technique is cross-layer parameter sharing. An ALBERT configuration similar to BERT-large has 18x fewer parameters and can be trained about 1.7x faster. ELECTRAICLR 2020 | Electra: Pre-training text encoders as discriminators rather than generators We propose replaced token detection, a pre-training task in which the model learns to distinguish real input tokens from plausible but synthetically generated replacements. A key advantage of our discriminative task is that the model learns from all input tokens instead of just the small masked-out subset, making it more computationally efficient. AMBERT2021 | AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization AMBERT adopts two encoders with the shared parameters for fine-grained token embedding and coarse-grained token embedding. For English, AMBERT takes both the sequence of words (fine-grained tokens) and the sequence of phrases (coarse-grained tokens) as input after tokenization. For Chinese, AMBERT takes both the sequence of characters (fine-grained tokens) and the sequence of word (coarse-grained tokens) as input after tokenization. LBERT2021 | Lattice-BERT: Leveraging Multi-Granularity Representations in Chinese Pre-trained Language Models Lattice-BERT explicitly incorporates word representations along with characters, thus can model a sentence in a multi-granularity manner. We design a lattice position attention mechanism to exploit the lattice structures in self-attention layers. We further propose a masked segment prediction task to push the model to learn from rich but redundant information inherent in lattices, while avoiding learning unexpected tricks. 模型 作者 语言 规模 数据集 代码 BERT Google AI Language English base (110M)large (340M) BooksCorpus (800M words)English Wikipedia (2500M words) tensorflow ERNIE Baidu 中文 base (110M) Chinese WikepediaBaidu BaikeBaidu newsBaidu Tieba paddle XLNet Carnegie Mellon UniversityGoogle AI Brain Team English baselarge BooksCorpusEnglish Wikipediaplain text (13GB) Giga5 (16GB)ClueWeb 2012-B Common Crawl tensorflow RoBERTa Facebook AI English base BOOKCORPUSCC-NEWSOPENWEBTEXTSTORIES pytorch MacBERT HIT, iFLYTEK 中文 baselarge Chinese Wikipedia (0.4B words)extended training data (5.4B words) NEZHA Huawei Technologies 中文 baselarge Chinese WikipediaBaidu BaikeChinese News pytorchtensorflow ALBERT Google Research English base (12M)large (18M)xlarge (60M)xxlarge (235M) same as BERT tensorflow ELECTRA Stanford UniversityGoogle Brain English smallbaselarge BERT datasetClueWebCommonCrawlGigaword tensorflow LBERT Peking UniversityAlibaba Group 中文 base Chinese WikipediaZhihuweb news tensorflow NLGGPT-22019| Language Models are Unsupervised Multi-task Learners GPT-32020 | Language Models are Few-Shot Learners CPM2020 | CPM: A large-scale generative Chinese Pre-trained language model CPM, with 2.6 billion parameters and 100 GB Chinese training data, is the largest Chinese pre-trained language model, which could facilitate several downstream Chinese NLP tasks, such as conversation, essay generation, cloze test, and language understanding. Our current model is a left-to-right Transformer decoder, which is similar to the model architecture of GPT. PanGu-α2021 | PanGu-α: Large-Scale Autoregressive Pretrained Chinese Language Models with Auto-Parallel Computation PanGu-α is a large-scale autoregressive language model (ALM) pretrained on a large corpus of text, mostly in Chinese language. The architecture of PanGu-α is based on Transformer. Besides, we develop an additional query layer on top of Transformer layers to predict the next token. 模型 作者 语言 规模 数据集 代码 GPT-2 Open AI English 1.5 B WebText (40 GB) tensorflow GPT-3 Open AI English 175 B WebTextbooks corporaEnglish-language Wikipedia(570 GB) CPM Tsinghua University 中文 Small (109 M)Medium (224 M)Large (2.6 B) EncyclopediaWebpageStoryNewsDialogue(100 GB) pytorch PanGu-α Huawei 中文 200 B 1.1TB mindspore NLU-NLGUniLMNIPS 2019 | Unified Language Model Pre-training for Natural Language Understanding and Generation UniLM can be applied to both natural language understanding (NLU) and natural language generation (NLG) tasks. UNILM is a multi-layer Transformer network, jointly pre-trained on large amounts of text, optimized for three types of unsupervised language modeling objectives (Bidirectional encoding, Unidirectional decoding, Unidirectional decoding conditioned on bidirectional encoding). T52019 | Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Our model is roughly equivalent to the original Transformer with the exception of removing the Layer Norm bias, placing the layer normalization outside the residual path, and using a different position embedding scheme. BART2020 | BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension BART, a denoising autoencoder for pretraining sequence-to-sequence models. BART is trained by (1) corrupting text with an arbitrary noising function, and (2) learning a model to reconstruct the original text. BART uses a standard Tranformer-based neural machine translation architecture which can be seen as generalizing BERT (due to the bidirectional encoder), GPT (with the left-to-right autogressive decoder). CPM-2AI Open 2021 | CPM-2: Large-scale Cost-effective Pre-trained Language Models We introduce knowledge inheritance to accelerate the pretraining process by exploiting existing PLMs instead of training models from scratch. Compared with conventional fine-tuning, prompt tuning significantly reduces the number of task-specific parameters. CPM-2 is a standard Transformer-based model combined with a bidirectional encoder and a unidirectional decoder. Prompt tuning 模型 作者 语言 规模 数据集 代码 BART Facebook AI English base 160 GB pytorch UniLM Microsoft Research English large (340M) English Wikipedia BookCorpus T5 Google Research English base C4 (750 GB) tensorflow CPM-2 Tsinghua University &amp; BAAI 中文 11 B WuDaoCorpus(2.3 TB Chinese300GB English) pytorch Pre-trained Dialogue SystemsDialoGPTMennaBlenderCDial-GPTPLATO-2EVA2021 | EVA: An Open-Domain Chinese Dialogue System with Large-Scale Generative Pre-Training EVA, a Chinese dialogue system that contains the largest Chinese pre-trained dialogue model with 2.8B parameters. We collect the largest Chinese dialogue dataset named WDC-Dialogue. GLUE benchmarkThe General Language Understanding Evaluation (GLUE) benchmark is a collection of diverse natural language understanding tasks. CLUE benchmarkOrganization of Language Understanding Evaluation benchmark for Chinese: tasks &amp; datasets, baselines, pre-trained Chinese models, corpus and leaderboard. SQuAD benchmark","link":"2022/02/19/%E9%A2%84%E8%AE%AD%E7%BB%83%E6%A8%A1%E5%9E%8B%E5%90%88%E9%9B%86/"},{"title":"计算机网络基础","text":"1. OSI 七层协议 物理层 ​ 主要解决两台物理机之间的通信，通过二进制比特流的传输来实现，二进制数据表现为电流电压上的强弱，到达目的地再转化为二进制机器码。网卡、集线器工作在这一层。 数据链路层 ​ 接收来自物理层的位流形式的数据，并封装成帧，传送到上一层；同样，也将来自上层的数据帧，拆装为位流形式的数据转发到物理层。这一层在物理层提供的比特流的基础上，通过差错控制、流量控制方法，使有差错的物理线路变为无差错的数据链路。提供物理地址寻址功能。交换机工作在这一层。 网络层 ​ 将网络地址翻译成对应的物理地址，并决定如何将数据从发送方路由到接收方，通过路由选择算法为分组通过通信子网选择最佳路径。路由器工作在这一层。 传输层 ​ 传输层提供了进程间的逻辑通信，传输层向高层用户屏蔽了下面网络层的核心细节，使应用程序看起来像是在两个传输层实体之间有一条端到端的逻辑通信信道。 会话层 建立会话，身份验证，权限鉴定等。 表示层 ​ 对数据格式进行编译，对收到或发出的数据根据应用层的特征进行处理，如处理为文字、图片、音频、视频、文档等，还可以对压缩文件进行解压缩、对加密文件进行解密等。 应用层 ​ 提供应用层协议，如 HTTP 协议，FTP 协议等等，方便应用程序之间进行通信。 2. TCP/IP 五层模型 应用层：为应用程序提供交互服务，如域名系统DNS、HTTP协议、SMTP协议等。 传输层：负责向两台主机进程之间的通信提供数据传输服务，如 TCP、UDP 等。 网络层：选择合适的路由和交换结点，确保数据及时传送。主要包括 IP 协议。 数据链路层：将网络层的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。 物理层：实现相邻节点间比特流的透明传输，尽可能屏蔽传输介质和物理设备的差异。 3. HTTP 是什么？常见的状态码？ HTTP 是超文本传输协议，确立了一种计算机之间交流通信的规范以及相关的各种控制和错误处理方式。 常见的状态码： 100：客户端应继续其请求 101：服务器根据客户端的请求切换协议 200：OK，请求成功 201：成功请求并创建了新的资源 202：已经接受请求，但未处理完成 203：非授权信息 204：服务器成功处理，但未返回内容 301：请求的资源已被永久的移动到新的 URI 302：资源临时被移动 305：所请求的资源必须通过代理访问 400：客户端请求的语法错误，服务器无法理解 401：请求要求用户的身份认证 403：服务器理解请求客户端的请求，但是拒绝执行此请求 404：服务器无法根据客户端的请求找到资源（网页） 405：客户端请求中的方法被禁止 408：请求超时 410：客户端请求的资源已经不存在 500：服务器内部错误，无法完成请求 501：服务器不支持请求的功能，无法完成请求 502：从远程服务器接收到了一个无效的响应 503：由于超载或系统维护，服务器暂时的无法处理客户端的请求 4. GET 和 POST 的区别 GET 请求，用于请求数据，无副作用的，是幂等的，且可缓存，请求的数据附加在 URL 之后，以 “?” 分割 URL 和传输数据，多个参数用 “&amp;” 连接。URL 的编码格式采用的是 ASCII 编码，所有的非 ASCII 字符都要编码之后再传输。 POST 请求，用于提交数据，有副作用，非幂等，不可缓存，POST 请求会把请求的数据放置在 HTTP 请求包的包体中。 GET 请求和 POST 请求数据的大小在 HTTP 协议中虽然没有做限制，但是实际上浏览器会有默认值。总体来说，少量的数据使用 GET，大量的数据使用POST。 GET 请求的数据会暴露在 URL 中，所以安全性比较低，比如密码是不能暴露的，就不能使用 GET 请求。而 POST 请求中，请求参数信息是放在请求头的，所以安全性较高，可以使用。在实际中，涉及到登录操作的时候，尽量使用HTTPS请求，安全性更好。 5. TCP 与 UDP 的区别 TCP 作为面向流的协议，提供可靠的、面向连接的传输服务，并且提供点对点通信。 UDP 作为面向报文的协议，不提供可靠交付，并且不需要连接，不仅仅对点对点，也支持多播和广播。 6. 为什么 TCP 传输是可靠的？ TCP 有三次握手建立连接，四次挥手关闭连接的机制。 除此之外还有滑动窗口和拥塞控制算法。 最关键的是还保留超时重传的机制。 同时，对于每份报文也存在校验，保证每份报文可靠性。 7. 为什么 UDP 传输不可靠？ UDP 是面向数据报无连接的，数据报发出去就不保留数据备份了。 报头部加入校验和复用。 UDP 没有服务器和客户端的概念。 UDP 报文过长的话是交给 IP 切成小段，如果某段报废报文就废了。 8. 简述 TCP 的滑动窗口？ 如果发送者发送数据过快，接收者来不及接收，那么就会有分组丢失。为了避免分组丢失，控制发送者的发送速度，使得接收者来得及接收，这就是流量控制。流量控制根本目的是防止分组丢失，它是构成TCP可靠性的一方面。 滑动窗口是传输层进行流量控制的一种措施，接收方通过告诉发送方自己的窗口大小，从而控制发送方的发送速度，防止发送方发送速度过快而导致接收方被淹没。 当发送者收到了一个窗口为 0 的应答，发送者便停止发送，等待接收者的下一个应答。但是如果这个窗口不为 0 的应答在传输过程丢失，发送者一直等待下去，而接收者以为发送者已经收到该应答，等待接收新数据，这样双方就相互等待，从而产生死锁。 为了避免流量控制引发的死锁，TCP 使用了持续计时器。每当发送者收到一个 0 窗口的应答后就启动该计时器。时间一到便主动发送报文询问接收者的窗口大小。若接收者仍然返回 0 窗口，则重置该计时器继续等待；若窗口不为 0，此时重置发送窗口后开始发送，这样就避免了死锁的产生。 9. 简述 TCP 的拥塞控制？ 拥塞是指一个或者多个交换点的数据报超载，而 TCP 又有重传机制，导致过载。 拥塞控制是作用于网络的，它是防止过多的数据注入到网络中，避免出现网络负载过大的情况；常用的方法就是： （ 1 ）慢开始、拥塞避免（ 2 ）快重传、快恢复。 慢开始： ​ 发送方维持一个叫做拥塞窗口 cwnd（congestion window）的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。 ​ 慢开始算法的思路就是，不要一开始就发送大量的数据，先探测一下网络的拥塞程度，也就是说由小到大逐渐增加拥塞窗口的大小。 ​ 一个传输轮次所经历的时间，也就是往返时间 RTT，而且每经过一个传输轮次（transmission round），拥塞窗口 cwnd 就加倍。 ​ 为了防止拥塞窗口 cwnd 增长过大引起网络拥塞，还需要设置一个慢开始门限 ssthresh 状态变量。 ​ 当 cwnd &lt; ssthresh 时，使用慢开始算法。 ​ 当 cwnd &gt; ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。 ​ 当 cwnd = ssthresh 时，即可使用慢开始算法，也可使用拥塞避免算法。 拥塞避免 ​ 拥塞避免算法让拥塞窗口缓慢增长，即每经过一个往返时间 RTT 就把发送方的拥塞窗口 cwnd 加1，而不是加倍。这样拥塞窗口按线性规律缓慢增长。 无论是在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（其根据就是没有按时收到确认，虽然没有收到确认可能是其他原因的分组丢失，但是因为无法判定，所以都当做拥塞来处理），就把慢开始门限 ssthresh 设置为出现拥塞时的发送窗口大小的一半（但不能小于2）。然后把拥塞窗口 cwnd 重新设置为1，执行慢开始算法。这样做的目的就是要迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。 快重传 快重传要求接收方在收到一个失序的报文段后就立即发出重复确认而不要等到自己发送数据时捎带确认。快重传算法规定，发送方只要一连收到三个重复确认就应当立即重传对方尚未收到的报文段，而不必继续等待设置的重传计时器时间到期。 快恢复 当发送方连续收到三个重复确认时，就执行“乘法减小”算法，把 ssthresh 门限减半（为了预防网络发生拥塞），但是接下去并不执行慢开始算法。考虑到如果网络出现拥塞的话就不会收到好几个重复的确认，所以发送方现在认为网络可能没有出现拥塞。所以此时不执行慢开始算法，而是将 cwnd 设置为 ssthresh 减半后的值，然后执行拥塞避免算法，使 cwnd 缓慢增大。 10. TCP 三次握手 假设发送端为客户端，接收端为服务端。开始时客户端和服务端的状态都是 CLOSED。 第一次握手：客户端向服务端发起建立连接请求，客户端会随机生成一个起始序列号 x，客户端向服务端发送的字段中包含标志位 SYN=1，序列号 seq=x。客户端发送请求后，状态由 CLOSED 变为 SYN-SENT。 第二次握手：服务端在收到客户端发来的报文后，由标志位 SYN=1 知道客户端请求建立连接，服务端将标志位 SYN和 ACK 都置为1，ack=x+1，并随机产生一个值 seq=y，并将该数据包发送给客户端确认连接请求。服务端的状态由 LISTEN 变为 SYN-RCVD。 第三次握手：客户端收到确认后检查，如果正确则将标志位 ACK为1，ack=y+1，并将该数据包发送给服务端，客户端进入 ESTABLISHED 状态。服务端对收到的数据包检查如果正确则连接建立成功，进入 ESTABLISHED 状态。客户端和服务端进入 ESTABLISHED 状态，完成三次握手。随后客户端和服务端之间可以开始传输数据了。 11. 两次握手可以吗？ 不可以。“三次握手”的目的是“为了防止已失效的连接请求报文段突然又传送到了服务端，因而产生错误”。 例如以下情形：假设不采用“三次握手”，那么只要 server 发出确认，新的连接就建立了。由于现在 client 并没有发出建立连接的请求，因此不会理睬 server 的确认，也不会向 server 发送数据。但 server 却以为新的运输连接已经建立，并一直等待 client 发来数据。这样，server 的很多资源就白白浪费掉了。 12. TCP 四次挥手 第一次挥手：客户端发送关闭连接报文（FIN=1，seq=u），并停止再发送数据，主动关闭 TCP 连接，进入FIN-WAIT-1（终止等待1）状态，等待服务端的确认。 第二次挥手：服务端收到关闭连接报文后，发出确认报文（ACK=1，ack=u+1，seq=v），进入 CLOSE_WAIT（关闭等待）状态。此时 TCP 处于半关闭状态，客户端已经没有要发送到数据了，但服务端仍可以向客户端发送数据。 第三次挥手：服务端发送关闭连接报文，用来关闭服务端到客户端的数据传送，服务端进入 LAST_ACK 状态，等待客户端确认。 第四次挥手：客户端收到关闭报文后，进入TIME_WAIT 状态，此时 TCP 未释放掉，需要经过时间等待计时器设置的时间 2 * MSL（最大报文段生存时间）后，才进入 CLOSED 状态。客户端发送一个确认报文给服务端，服务端确认后，进入 CLOSED 状态，完成四次挥手。 12. 为什么要四次挥手？ 主要原因是当服务端收到客户端的关闭连接请求报文后，服务端可能还有数据没发送完，不会立即 close。 24. 第四次挥手为什么要等待 2MSL？ 1 个 MSL 保证四次挥手中主动关闭方最后的 ACK 报文能够最终到达对端 1 个 MSL 保证对端如果没有收到 ACK 那么进行重传的 FIN 报文能够到达 13. TCP 连接的特点 TCP 是面向连接的传输层协议 TCP 连接是一对一的 TCP 提供可靠交付的服务（每份报文存在校验） TCP 提供全双工通信 TCP 是面向字节流的 14. 简述 DNS 协议及解析过程 DNS 是 Domain Name System 的缩写，中文是“域名系统”。DNS 协议是基于 UDP 的应用层协议，它的功能是根据用户提供的域名，解析出该域名对应的 IP 地址，从而给客户端进行访问。 域名的构成： www.baidu.com 其中，”www” 是三级域名，”baidu” 是二级域名，”com” 是顶级域名 域名服务器划分： 本地域名服务器：当一个主机发出 DNS 查询请求时，这个查询请求报文就发送给本地域名服务器。每一个互联网服务提供者 ISP 都可以拥有一个本地域名服务器。 根域名服务器：根域名服务器是最高层次的域名服务器，知道所有顶级域名服务器的域名和 IP 地址。如果本地域名服务器无法对域名进行解析，就首先求助于根域名服务器。 顶级域名服务器：顶级域名服务器负责管理在该服务器注册的所有二级域名。当收到 DNS 查询请求时，就给出相应的回答（可能是最后的结果，也可能是下一步需要查询的域名服务器的 IP 地址）。 权威域名服务器：权限域名服务器是负责一个区的域名服务器，如 “baidu.com” 下的所有域名。当一个权限域名服务器还不能给出最后的查询回答时，就会告知发出查询请求的DNS客户，下一步应当找哪一个权威域名服务器。 域名解析一般采用递归查询的方式进行。解析过程： 客户机发出查询请求，首先在本地缓存（浏览器缓存、系统 hosts 缓存、路由器缓存等）中查找，若没有找到，就会将请求发送给 DNS 服务器。 如果在本机上无法完成域名的解析，那么系统只能请求本地域名解析服务系统进行，本地域名服务器一般都缓存了大部分的域名解析的结果，本地域名服务器可能是当地的运行商，如联通、电信或校园网等。 如果在本地域名服务器还不能完成解析，那么本地域名服务器首先向根域名服务器发起查询。根域名服务器收到请求后会查看区域文件记录，若查询到记录，则返回解析的 IP 地址；若无则将其管辖范围内顶级域名（如.com、.cn等）服务器 IP 告诉本地 DNS 服务器。 若根域名服务器未完成解析，则本地域名服务器向收到的顶级域名服务器发起查询。顶级域名服务器收到请求后会查看区域文件记录，若查询到记录，则返回解析的 IP 地址；若无则将其管辖范围内权威服务器 IP 告诉本地 DNS 服务器。 若顶级域名服务器未完成解析，则本地域名服务器向收到的权威域名服务器发起查询。权威域名服务器收到请求后会查看区域文件记录，若查询到记录，则返回解析的 IP 地址；若无则将下一个应当查询的权威服务器 IP 告诉本地 DNS 服务器。 本地域名服务器将最终的查询结果返回到客户机，完成域名解析。 15. HTTP 协议及特点 HTTP 协议是超文本传输协议。它是基于 TCP 协议的应用层传输协议，即客户端和服务端进行数据传输的一种规则。该协议本身HTTP 是一种无状态的协议。 HTTP 允许传输任意类型的数据。传输的类型由Content-Type加以标记。 HTTP 请求是无状态的，对于客户端每次发送的请求，服务器都认为是一个新的请求，上一次会话和下一次会话之间没有联系。 支持客户端/服务端模式 16. HTTP 请求报文格式 HTTP 请求由请求行、请求头、空行和请求数据四个部分组成。 请求行：请求行由请求方法字段、URL字段和 HTTP协议版本 字段3个字段组成，用空格分隔。 例如，GET /index.html HTTP/1.1。 请求方法：GET、POST、HEAD、PUT、DELETE、OPTIONS 等 GET：最常见的一种请求方式，要求服务器将 URL 定位的资源放在响应报文的数据部分，回送给客户端。使用 GET 方法时，请求参数和对应的值直接附加在 URL 后面，使用一个问号 “?” 将 URL 与请求参数分开。显然，这种方式不适合传送私密数据。另外，浏览器通常对传递参数长度进行限制，因此 GET 方法也不适合传送大量数据。 POST：POST 方法将请求参数封装在 HTTP 请求数据中，以名称/值的形式出现，对传送的数据大小没有限制，可以传输大量数据，而且也不会显示在 URL 中。 HEAD：HEAD 就像 GET，只不过服务端接收到 HEAD 请求后只返回响应头，而不会发送响应内容。当我们只需要查看某个页面的状态的时候，使用 HEAD 是非常高效的，因为在传输的过程中省去了页面内容。 PUT：用客户端向服务端传送到数据替换指定文档的内容 请求头：请求头部由关键字/值对组成，每行一对，关键字和值用英文冒号“:”分隔。典型的请求头有： User-Agent：产生请求的浏览器类型 Accept：客户端可识别的内容类型列表 Host：请求的主机名 空行：最后一个请求头之后是一个空行，用于分割请求头与请求体 请求数据：请求数据不在 GET 方法中使用，而是在 POST 方法中使用。POST 方法适用于需要客户填写表单的场合。与请求数据相关的最常使用的请求头是 Content-Type 和 Content-Length。 17. HTTP 响应报文格式 HTTP 响应也由四个部分组成，分别是状态行、消息报头、空行和响应正文。 状态行：由 HTTP 协议版本、状态码和状态描述三部分组成。 响应头：响应头字段主要有 connection、content-type、content-encoding、content-length、set-cookie、Last-Modified，、Cache-Control、Expires 等。 空行：用于分割响应头与响应体。 响应体：服务器返回给客户端的内容。 18. HTTP 长连接与短链接 短链接：HTTP 1.0 默认使用的是短连接。浏览器和服务器每进行一次 HTTP 传输，就建立一次连接，任务结束就中断连接。 长连接：HTTP/1.1 起，默认使用长连接。长连接，指的是复用TCP连接。多个HTTP请求可以复用同一个TCP连接，这就节省了TCP连接建立和断开的消耗。 19. HTTP 1.0 和 HTTP 1.1 和 HTTP 2.0 的区别 HTTP 1.0：规定了请求头和请求体，响应头和响应体，每一个请求都是一个单独的连接，做不到连接的复用。 HTTP 1.1： HTTP1.1 默认开启长连接，在一个 TCP 连接上可以传送多个 HTTP 请求和响应。改善了短链接的性能开销。 支持管道（pipeline）网络传输，只要第一个请求发出去了，不必等其回来，就可以发第二个请求出去，可以减少整体的响应时间。 HTTP 2.0： 新的二进制格式：HTTP 1.1 基于文本格式传输数据，HTTP 2.0 采用二进制格式传输数据，解析更高效。 提出多路复用。在一个连接里，允许同时发送多个请求或响应，并且能够并行传输而不被阻塞。 头部压缩：HTTP 1.1 的头部有大量信息，每次都要重复发送；HTTP 2.0 将 header 从数据中分离，并封装成头帧和数据帧，使用特定算法压缩头帧，有效减少头部信息大小；并且 HTTP 2.0 在客户端和服务端记录了之前发送的键值对，对于相同的数据，不需要重复发送。 服务端推送：HTTP 2.0 允许服务器向客户端推送资源，无需客户端发送请求到服务端获取。 20. HTTP vs HTTPS HTTP 是超文本传输协议，明文传输信息；HTTPS 是具有安全性的 SSL 加密传输协议 HTTP 使用端口号 80；HTTPS 使用端口号 443 HTTPS 协议需要 CA 机构签发的证书 HTTP 运行在 TCP 协议之上；HTTPS 运行在 SSL 协议之上，SSL 运行在 TCP 协议之上 21. HTTPs 连接过程 HTTPS 建立连接的过程中，一共发起两次请求，进行非对称和对称两次加密。 客户端发送一个 https 请求到服务端 服务端准备配置好的数字证书，包含公钥和私钥 服务端将证书传送给客户端，证书中包含了很多信息，比如证书的颁发机构，过期时间，网址，公钥等 客户端解析证书，由客户端的 TLS 完成，首先会验证证书是否有效，比如颁发机构，过期时间等。如果有异常，就会弹出警告信息，并结束通信。如果正常，则生成一个随机值（用于对称加密），然后用服务端的公钥对随机值进行非对称加密 客户端将加密后的随机值传送到服务端 服务端使用证书的私钥非对称解密得到客户端的随机值，用获取的随机值将传输的明文内容进行对称加密 服务端把对称加密后的数据传输到客户端 客户端通过随机值对称解密获取明文内容 22. 对称加密 vs 非对称加密 对称加密：通信双方使用相同的密钥进行加密。特点是加密速度快，但是缺点是密钥泄露会导致密文数据被破解。常见的对称加密有 AES 和 DES 算法。 非对称加密：需要生成两个密钥，公钥和私钥。公钥是公开的，任何人都可以获得，而私钥是私人保管的。公钥负责加密，私钥负责解密；或者私钥负责加密，公钥负责解密。这种加密算法安全性更高，但是计算量相比对称加密大很多，加密和解密都很慢。常见的非对称算法有 RSA 和 DSA。 22. 浏览器从输入 URL 到返回页面的过程 浏览器对 URL 检查 检查 URL 是否合法 判断 URL 是否完整，如果不完整，浏览器可能会对其补全 DNS 解析（以下某一步完成解析则返回解析后的 IP 地址） 检查浏览器缓存和本地 hosts 文件 查找本地 DNS 服务器缓存 递归查询 DNS 建立 TCP 连接 当浏览器获取到服务器的 IP 地址后，会用一个随机的端口（1024 &lt; 端口号 &lt; 65535）向服务器的 80 端口发起 TCP 连接请求（HTTP 默认约定 80 端口，HTTPS 为 443 端口），通过 TCP 三次握手，建立 TCP 连接。 服务器响应 服务器根据浏览器发送的 HTTP 请求发送响应报文 浏览器渲染 浏览器将收到的响应报文进行渲染，得到显示的网页 处理 HTML 并构建 DOM 树 处理 CSS 并构建 CSSOM 树 将 DOM 与 CSSOM 合并成一个渲染树 根据渲染树布局，计算每个结点的几何信息 将各个结点绘制到屏幕上 断开连接 现在的浏览器页面为了优化请求的耗时，默认都会开启持久连接（keep-alive），当 tab 标签页被关闭的时候，会发起四次挥手断开 TCP 的连接。 23. Cookie vs Session 由于 HTTP 协议是无状态的协议，需要用某种机制来标识用户身份，以跟踪用户的整个会话。常用的会话跟踪技术是 cookie 与 session。 cookie 是由服务器发给客户端的特殊信息，而这些信息以文本的方式存放在客户端，然后客户端每次向服务器发送请求的时候都会带上这些特殊的信息。 一个 cookie 的工作流程：当用户使用浏览器访问一个支持 cookie 的网站时，用户会提供包括用户名在内的个人信息并且提交至服务器；服务器会根据这些身份信息生成对应的 cookie，并通过响应报文回传给客户端（cookie 存放在 HTTP 响应头中）；当客户端浏览器接收到来自服务器的响应之后，浏览器会将 cookie 信息存放在一个统一的位置。然后，客户端再向服务器发送请求的时候，都会把相应的 cookie 存放在 HTTP 请求头发回至服务器；服务器就能够接收到来自客户端浏览器的带有 cookie 信息的请求，从而得到客户端特有的身份信息，然后动态生成与该客户端相对应的内容。 session 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束。 Cookie 与 Session 的区别？ 作用范围不同，Cookie 保存在客户端（浏览器），Session 保存在服务器端。 存取方式的不同，Cookie 只能保存 ASCII，Session 可以存任意数据类型，一般情况下我们可以在 Session 中保持一些常用变量信息，比如说 UserId 等。 有效期不同，Cookie 可设置为保持较长时间，Session 一般保持时间较短，客户端关闭或者 Session 超时都会失效。 隐私策略不同，Cookie 存储在客户端，比较容易遭到不法获取，Session 存储在服务端，安全性要好一些。 存储大小不同， 单个 Cookie 保存的数据不能超过 4K，Session 可存储数据远高于 Cookie。 如果浏览器禁用了 Cookie，如何保障整个机制的正常运转？ 第一种方案：每次请求中都携带一个 SessionID 的参数，记录对话状态 第二种方案：使用 Token 机制，服务端产生一个令牌，用来标识客户端。 分布式 Session 的问题？ 互联网公司为了可以支撑更大的流量，后端往往需要多台服务器共同来支撑前端用户请求，那如果用户在 A 服务器登录了，第二次请求跑到服务 B 就会出现登录失效问题。分布式 Session 一般会有以下几种解决方案： Nginx ip_hash 策略，服务端使用 Nginx 代理，每个请求按访问 IP 的 hash 分配，这样来自同一 IP 固定访问一个后台服务器，避免了在服务器 A 创建 Session，第二次分发到服务器 B 的现象。 Session 复制，任何一个服务器上的 Session 发生改变（增删改），该节点会把这个 Session 的所有内容序列化，然后广播给所有其它节点。 共享 Session，服务端无状态化，将用户的 Session 等信息使用缓存中间件来统一管理，保障分发到每一个服务器的响应结果都一致。 26. 什么是 DNS 劫持？ DNS 劫持是在用户通过域名系统访问目标的时候劫持域名解析的回包，目标域名会被恶意地指向其他恶意 IP 地址，造成用户无法正常使用服务，甚至造成弹广告、挂马等危害。 DNS劫持最直接的手段是通过入侵 DNS 服务器，获取攻击目标域名的解析记录控制权，之后修改攻击目标域名的解析结果，等到 TTL 时间生效之后，所有对该域名的访问由原 IP 地址指向修改后的恶意 IP。 27. 什么是 SYN 攻击？ SYN 攻击利用 TCP 协议缺陷，通过发送大量的半连接请求，占用半连接队列，耗费服务端的 CPU 和内存资源。 优化方式： 缩短 SYN Timeout 时间 记录 IP，若连续受到某个 IP 的重复 SYN 报文，从这个 IP 地址来的包会被一概丢弃。","link":"2022/10/20/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"title":"统计学习方法","text":"一、统计学习与监督学习概论1.1 统计学习什么是学习？ 如果一个系统能够通过执行某个过程改进它的性能，这就是学习。——赫尔伯特 · 西蒙 统计机器学习 统计学习就是计算机系统通过运用数据及统计方法提高系统性能的及其学习。机器学习往往指统计机器学习。 统计学习的前提 统计学习关于数据的基本假设是同类数据具有一定的统计规律性。 统计学习的目的 统计学习用于对数据的预测与分析，特别是对未知新数据的预测与分析。 统计学习的方法 统计学习的方法是基于数据构建概率统计模型，从而对数据进行预测与分析，由监督学习、无监督学习和强化学习等组成。 定义 统计学习方法可以概括如下：从给定的、有限的、用于学习的训练数据集合出发，假设数据是独立同分布产生的；并且假设要学习的模型属于某个函数的集合，称为假设空间；应用某个评价准则，从假设空间中选取一个最优模型，使它对已知的训练数据及未知的测试数据在给定的评价准则下有最优的预测；最优模型的选取由算法实现。 这样，统计学习方法方法包括模型的假设空间、模型选择的准则以及模型学习的算法，简称为模型（model）、策略（strategy）和算法（algorithm），称为统计学习方法的三要素。 1.2 统计学习的分类基本分类监督学习 监督学习（supervised learning）是从标注数据中学习预测模型的机器学习问题。其本质是学习输入到输出的映射的统计规律。 监督学习假设输入与输出的随机变量 X 和 Y 遵循联合概率分布 P(X, Y)。训练数据与测试数据被看作是依联合概率分布 P(X, Y) 独立同分布产生的。这是监督学习关于数据的基本假设。 无监督学习 无监督学习（unsupervised learning）是指从无标注数据中学习预测模型的机器学习问题。其本质是学习数据中的统计规律或潜在结构。 强化学习 强化学习（reinforcement learning）是指智能系统在与环境的连续互动中学习最优行为策略的机器学习问题。其本质是学习最优的序贯决策。 半监督学习与主动学习 半监督学习（semi-supervised learning）是指利用标注数据和未标注数据学习预测模型的机器学习问题。利用未标注数据中的信息，辅助标注数据进行监督学习，以较低的成本达到较好的学习效果。 主动学习（active learning）是指机器不断主动给出实例让教师进行标注，然后利用标注数据学习预测模型的机器学习问题。主动学习的目标是找出对学习最有帮助的实例让老师标注，以较小的标注代价达到较好的学习效果。 半监督学习和主动学习更接近监督学习。 按模型分类概率模型与非概率模型 统计学习方法可以分为概率模型（probabilistic model）和非概率模型（non-probabilistic model）或确定性模型（deterministic model）。 在监督学习中，概率模型取条件概率分布形式 P(y|x)，非概率模型取函数形式 y = f(x)，其中 x 是输入，y 是输出。在无监督学习中，概率模型取条件概率形式 P(z|x) 或 P(x|z)，非概率模型取函数形式 z = g(x)，其中 x 是输入，z 是输出。 概率模型包括： 决策树 朴素贝叶斯 隐马尔可夫模型 条件随机场 概率潜在语义分析 潜在狄利克雷分配 高斯混合模型 非概率模型包括： 感知机 支持向量机 k 近邻 AdaBoost k 均值 潜在语义分析（LSA） 神经网络 逻辑回归既可以看作是概率模型，也可以看作是非概率模型。 注意：条件概率分布 P(y|x) 与函数 y = f(x) 可以相互转化。因此，概率模型和非概率模型的区别不在于输入与输出之间的映射关系，而在于模型的内在结构：概率模型一定可以表示为联合概率分布的形式。 1.3 统计学习方法的三要素方法 = 模型 + 策略 + 算法 模型在监督学习过程中，模型就是所要学习的条件概率分布或决策函数。模型对假设空间包含所有可能的条件概率分布或决策函数。 假设空间可以定义为决策函数的集合：$\\mathcal F={f|Y=f(X)}$，其中，X 和 Y 是定义在输入空间 $\\mathcal X$ 和输出空间 $\\mathcal Y$ 上的变量。 假设空间也可以定义为条件概率的集合：$\\mathcal F={P|P(Y|X)}$ 策略有了模型的假设空间，接着需要考虑的是按照什么样的准则学习或选择最优模型。统计学习的目标在于从假设空间中选取最优模型。 损失函数与风险函数 损失函数度量模型一次预测的好坏；风险函数度量平均意义下模型预测的好坏。 损失函数是 f(X) 和 Y 的非负实值函数，记作 L(Y, f(X))。 0-1 损失函数： $L(Y,f(X))=\\begin{cases}1,&amp;Y\\ne f(X)\\0,&amp;Y=f(X) \\end{cases}$ 平方损失函数： $L(Y,f(X))=(Y-f(X))^2$ 绝对损失函数： $L(Y,f(X))=|Y-f(X)|$ 对数（似然）损失函数： $L(Y,P(Y|X))=-logP(Y|X)$ 损失函数值越小，模型就越好。 风险函数或期望损失就是平均意义下的损失： ​ $R{exp}(f)=E_P[L(Y,f(X))]=\\int{\\mathcal X \\times \\mathcal Y}L(y,f(x))P(x,y)dxdy$ 学习的目标就是选择期望风险最小的模型。由于联合分布 P(X, Y) 是未知的（如果知道联合分布 P(X, Y)，就可以直接求出条件概率分布 P(Y|X)，就不需要学习了），所以 $R_{exp}(f)$ 不能直接计算。因此，监督学习需要用经验风险估计期望风险。 模型关于训练数据的平均损失称为经验风险或经验损失： ​ $R{emp}(f)=\\frac{1}{N}\\sum\\limits{i=1}^NL(y_i,f(x_i))$ 期望风险是模型关于联合分布的期望损失；经验风险是模型关于训练样本集的平均损失。根据大数定律，当样本容量 N 趋于无穷大时，经验风险趋于期望风险。但是现实中训练样本数有限，甚至很小，因此用经验风险估计期望风险常常并不理想，要对经验风险进行一定的矫正。有两个基本策略：经验风险最小化和结构风险最小化。 经验风险最小化和结构风险最小化 经验风险最小化（empirical risk minimization，ERM）的策略认为，经验风险最小的模型就是最优的模型。最优模型就是求解以下最优化问题： ​ $\\min\\limits{f \\in \\mathcal F}R{emp}(f)$ 当样本容量足够大时，经验风险最小化能保证有很好的学习效果。比如，极大似然估计就是经验风险最小化的一个例子。 但是，当样本容量很小时，经验风险最小化学习的效果会产生“过拟合”现象。 结构风险最小化（structural risk minimization，SRM）是为了防止过拟合而提出的策略。结构风险最小化等价于正则化（regularization），是在经验风险上加上表示模型复杂度的正则化项或罚项。结构风险的定义是： ​ $R{srm}(f)=R{emp}(f) + \\lambda J(f)$ 其中 J(f) 为模型的复杂度，是定义在假设空间上的泛函。模型越复杂，复杂度 J(f) 越大；反义，模型越简单，复杂度 J(f) 就越小。也就是说，复杂度表示了对复杂模型的惩罚。$\\lambda \\ge 0$ 是系数，用来权衡经验风险和模型复杂度。 这样，监督学习问题就变成了经验风险最优化或结构风险最优化问题。 算法算法是指学习模型的具体计算方法。统计学习的算法归结为求解最优化问题的算法。如果最优化问题有显示的解析解，这个优化问题就比较简单。但通常解析解不存在，这就需要用数值计算的方法求解。 1.5 正则化与交叉验证正则化模型选择的典型方法是正则化（regularization）。正则化是结构风险最小化策略的实现，一般具有如下形式： ​ $\\min\\limits{f\\in\\mathcal F}\\frac{1}{N}\\sum\\limits{i=1}^NL(y_i,f(x_i))+\\lambda J(f)$ 其中，第 1 项是经验风险，第 2 项是正则化项。 比如，损失函数是平方损失，正则化项可以是参数向量的 L2 范数： ​ $L(\\omega)=R_{emp} + \\frac{\\lambda}{2}||\\omega||^2$ 这里，$||\\omega||$ 表示参数向量 $\\omega$ 的 L2 范数。 交叉验证另一种模型选择方法是交叉验证（cross validation）。如果给定的样本数据充足，进行模型选择的一种简单方法是随机地将数据集切分为三部分：训练集、验证集和测试集。训练集用来训练模型，验证集用来选择模型，而测试集用于最终对学习方法的评估。选择对验证集有最小预测误差的模型。 如果数据不充足，可以采用交叉验证方法。交叉验证的基本想法是重复地使用数据：把给定的数据切分，将切分的数据集组合为训练集于测试集，在此基础上反复地进行训练、测试以及模型选择。 简单交叉验证 随机地将数据分为两部分：训练集和测试集（例如，70% 作为训练集，30% 作为测试集），在测试集上评价模型的测试误差，选择测试误差最小的模型。 S 折交叉验证 首先随机地将数据切分为 S 个互不相交、大小相同的子集；然后利用 S - 1 个子集的数据训练模型，剩下的子集用于测试模型；将这一过程对可能的 S 种选择重复进行，最后选择平均测试误差最小的模型。 留一交叉验证 当 S = N 时，称为留一交叉验证。 1.7 生成模型与判别模型监督学习方法又可以分为生成方法和判别方法，所学到的模型分别称为生成模型和判别模型。 生成方法 生成方法由数据学习联合概率分布 P(X, Y)，然后求出条件概率分布 P(Y|X) 作为预测的模型： ​ $P(Y|X)=\\frac{P(X,Y)}{P(X)}$ 生成模型包括： 朴素贝叶斯 隐马尔可夫模型 生成方法的特点： 可以还原出连个概率分布 P(X, Y) 学习收敛速度更快 当存在隐变量时，仍可以用生成方法学习，而不能用判别方法 判别方法 判别方法由数据直接学习决策函数 f(X) 或者条件概率分布 P(Y|X)。 判别模型包括： k 近邻 感知机 决策树 逻辑回归 最大熵模型 支持向量机 提升方法 条件随机场 判别方法的特点： 直接学习条件概率或决策函数，直接面对预测，往往准确率更高 可以对数据进行各种程度上的抽象、定义和使用特征，可以简化学习问题 1.8 监督学习的应用分类问题监督学习从数据中学习一个分类模型或分类决策函数，称为分类器。评价分类器性能的指标一般是分类的准确率（accuracy）：分类器正确分类的样本数与总样本数之比。 真实类别 \\ 分类结果 正类 负类 正类 TP FN 负类 FP TN 精确率：$P=\\frac{TP}{TP+FP}$ (所有预测的正类中，预测正确的占比) 召回率：$R=\\frac{TP}{TP+FN}$ (所有正类样本中，预测正确的占比) $F_1$ 值：$F_1=\\frac{2PR}{P+R}$ 准确率：$ACC=\\frac{TP+TN}{TP+FP+TN+FN}$ (所有样本中，预测正确的占比) 灵敏度：$TPR=R$ (所有正类样本中，预测正确的占比) 特异度：$TNR=\\frac{TN}{FP+TN}$ (所有负类样本中，预测正确的占比) ROC 曲线与 AUC 值 横轴：$FPR=\\frac{FP}{FP+TN}$ (假正率，预测错误的正类在所有负类样本中的占比) 纵轴：$TPR=\\frac{TP}{TP+FN}$ (真正率，预测正确的正类在所有正类样本中的占比) AUC（Area under Curve）：ROC 曲线下的面积，介于 0 和 1 之间。其数值可以评价分类器的好坏，值越大（曲线却靠近 (0, 1) 点），代表分类器的效果越好。 ROC 曲线一定是需要在 y = x 之上的，否则就是一个不理想的分类器 标注问题标注（tagging）问题是分类问题的一个推广，又是更复杂的结构预测（structure prediction）问题的简单形式。标注问题的输入是一个观测序列，输出是一个标记序列或状态序列。目标在于学习一个模型，能够对观测序列给出标记序列作为预测。 标注常用的统计学习方法有：隐马尔可夫模型、条件随机场。 回归问题回归用于预测输入变量（自变量）和输出变量（因变量）之间的关系，学习从输入变量到输出变量之间映射的函数。按照输入变量的个数，分为一元回归和多元回归；按照输入变量和输出变量之间的关系类型，分为线性回归和非线性回归。 二、感知机感知机（perceptron）是二分类的线性分类模型，其输入为实例的特征向量，输出为实例的类别，属于判别模型。感知机学习旨在求出将训练数据线性化分的分离超平面，是神经网络与支持向量机的基础。 2.1 感知机模型感知机函数：$f(x)=sign(\\omega \\cdot x + b)$ 其中，$\\omega$ 和 $b$ 为感知机模型参数，$\\omega \\in R^n$ 叫权值（向量），$b\\in R$ 叫偏置（bias），$\\cdot$ 表示内积。输入空间是 $\\mathcal X \\subseteqq R^n$，输出空间是 $\\mathcal Y={+1,-1}$。sign 是符号函数，即 ​ $sign(x)=\\begin{cases}+1,&amp;x\\ge0\\-1,&amp;x&lt;0\\end{cases}$ 感知机是一种线性分类模型，线性方程的几何解释如下： 对应于特征空间 $R^n$ 中的一个超平面 S，ω 是超平面的法向量，b 是超平面的截距。这个超平面将特征空间划分为两个部分，对应正负两类。 2.2 学习策略假设训练数据集是线性可分的（能够通过线性方程 $\\omega \\cdot x + b$ 分为两类），感知机学习的目标是求得一个能够将正负点完全正确分开的超平面。 损失函数 如果选择误分类点的总数作为损失函数，该损失函数不是参数 ω 和 b 的连续可导函数，不易优化。另一个选择是误分类点到超平面 S 的总距离。输入空间中任一点 $x_0$ 到超平面 S 的距离定义为 ​ $\\frac{1}{||\\omega||}|\\omega \\cdot x_0+b|$ 其中，$||\\omega||$ 表示 $\\omega$ 的 L2 范数。 对于误分类的数据 $(x_i,y_i)$，有 $-y_i(\\omega \\cdot x_i+b)&gt;0$。因此，误分类点到超平面的距离是 ​ $-\\frac{1}{||\\omega||}y_i|\\omega \\cdot x_0+b|$ 这样，所有误分类点到超平面的总距离为 ​ $-\\frac{1}{||\\omega||}\\sum\\limits_{x_i\\in M}y_i|\\omega \\cdot x_0+b|$ 不考虑 $\\frac{1}{||\\omega||}$，得到感知机的损失函数 ​ $L(w,b)=-\\sum\\limits_{x_i \\in M}y_i(\\omega \\cdot x_i + b)$ 这就是感知机学习的经验风险函数。显然，损失函数是非负的，如果没有误分类点，损失函数为 0。而且，误分类点越少，损失函数值越小。 2.3 算法算法为求解损失函数极小化问题： ​ $\\min\\limits{w,b}L(w,b)=-\\sum\\limits{x_i\\in M}y_i(\\omega \\cdot x_i+b)$ 感知机学习算法是误分类驱动的，采用随机梯度下降法不断地极小化目标函数。极小化过程不是一次使素有误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。具体梯度为 ​ $\\nabla\\omega L(w,b)=-\\sum\\limits{x_i\\in M}y_ix_i$ ​ $\\nablabL(w,b)=-\\sum\\limits{x_i\\in M}y_i$ 随机选取一个误分类点，对参数 ω 和 b 进行更新： ​ $\\omega \\leftarrow \\omega+\\eta y_ix_i$ ​ $b \\leftarrow b+\\eta y_i$ 其中，η 是步长，又称学习率。这样，通过迭代降低损失函数，直到为 0。 可证明，线性可分的数据集可经过有限次迭代，得到完全正确划分的分离超平面及感知机模型。 感知机学习算法存在多解，这些解依赖于参数初值的选择，也依赖于选择误分类点进行参数更新的次序。 为了得到唯一的超平面，需要对分离超平面添加约束（参考线性支持向量机）。 当训练集线性不可分时，感知机学习算法不收敛，迭代结果会发生震荡。 三、k 近邻法k 近邻法（KNN）是一种基本的分类与回归方法。在分类任务中，k 近邻法的输入为实例的特征向量，输出为实例的类别，可以取多类。在预测时，根据其 k 个最近邻的类别，通过多数表决等方法进行预测。因此，k 近邻法不具有显式的学习过程。 k 近邻法的三个基本要素：k 值的选择、距离度量以及分类决策规则。 3.2 模型当距离度量、k 值及分类决策规则确定后，对于任一新的输入实例，模型输出的类别唯一确定。 距离度量特征空间中两个实例点的距离是两点相似程度的反映。可以使用欧氏距离度量，也可以使用更一般的 $L_p$ 距离： ​ $Lp(x_i,x_j)=(\\sum\\limits{l=1}^n|x_i^{(l)}-x_j^{(l)}|^p)^{\\frac{1}{p}}$ 当 p = 1 时，称为曼哈顿距离；当 p = 2 时，称为欧式距离；当 p = ∞ 时，它是各个坐标距离的最大值。 k 值的选择k 值的选择会对 k 近邻法的结果产生重大影响。 k 值较小时，学习的近似误差减小，只有与输入实例较近的实例才会对预测结果起作用。学习的估计误差会增大，预测结果对近邻的实例点非常敏感，容易受噪声影响。换句话说，k 值的减小意味着整个模型变得复杂，容易发生过拟合。 k 值较大时，可以减小学习的估计误差，但是近似误差会增大。这时输入实例较远的实例也会对预测起作用，使预测发生错误。k 值的增大意味着整个模型变得简单。 通常采用交叉验证法来选取最优的 k 值。 分类决策规则通常采用多数表决，即由输入实例的 k 个近邻的实例中多数类决定输出实例的类。 多数表决等价于经验风险最小化。 四、朴素贝叶斯法朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法。 对于给定的训练数据集，首先基于特征条件独立假设学习输入输出的联合概率分布；然后基于此模型，对给定的输入 x，利用贝叶斯定理求出后验概率最大的输出 y。 4.1 朴素贝叶斯法的学习与分类朴素贝叶斯法通过徐连数据集学习联合概率分布 P(X, Y)。具体地，学习先验概率分布： ​ $P(Y=c_k), k=1,2,…,K$ 以及条件概率分布： ​ $P(X=x|Y=c_k)=P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)}|Y=c_k), k=1,2,…,K$ 于是学习到连个概率分布 P(X, Y)。 朴素贝叶斯法对条件概率分布作了条件独立性的假设： ​ $\\begin{aligned}P(X=x|Y=ck) &amp;=P(X^{(1)}=x^{(1)},…,X^{(n)}=x^{(n)}|Y=c_k) \\ &amp; = \\prod\\limits{j=1}^nP(X^{(j)}=x^{(j)}|Y=c_k)\\end{aligned}$ 朴素贝叶斯法分类时，对给定的输入 x，通过学习到的模型计算后验概率分布 $P(Y=c_k|X=x)$，将后验概率最大的类作为 x 的类输出。后验概率计算根据贝叶斯定理进行： ​ $P(Y=c_k|X=x)=\\frac{P(X=x|Y=c_k)P(Y=c_k)}{\\sum\\limits_kP(X=x|Y=c_k)P(Y=c_k)}$ 带入条件独立性假设，有 ​ $P(Y=c_k|X=x)=\\frac{P(Y=c_k)\\prod\\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum\\limits_kP(Y=c_k)\\prod\\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)}, k=1,2,…,K$ 于是，朴素贝叶斯分类器可以表示为 ​ $y=f(x)=arg\\max\\limits_{c_k} \\frac{P(Y=c_k)\\prod\\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)}{\\sum\\limits_kP(Y=c_k)\\prod\\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)}, k=1,2,…,K$ 注意到，上式中分母对所有 $c_k$ 都是相同的，所以 ​ $y=arg\\max\\limits_{c_k}P(Y=c_k)\\prod\\limits_jP(X^{(j)}=x^{(j)}|Y=c_k)$ 朴素贝叶斯法将实例分到后验概率最大的类中，这等价于期望风险最小化。 4.2 朴素贝叶斯法的参数估计极大似然估计在朴素贝叶斯法中，学习意味着估计 $P(Y=c_k)$ 和 $P(X^{(j)}=x^{(j)}|Y=c_k)$。 先验概率 $P(Y=c_k)$ 的极大似然估计是 ​ $P(Y=ck)=\\frac{\\sum\\limits{i=1}^NI(y_i=c_k)}{N}, k=1,2,…,K$ 条件概率 $P(X^{(j)}=x^{(j)}|Y=c_k)$ 的极大似然估计是 ​ $P(X^{(j)}=a{jl}|Y=c_k)=\\frac{\\sum\\limits{i=1}^NI(xi^{(j)}=a{jl},y=ck)}{\\sum\\limits{i=1}^NI(y_i=c_k)}$ ​ $j=1,2,…,n; l=1,2,…,S_j; k=1,2,…,K$ 其中，x 的第 j 个特征 $x^{(j)}$ 的取值范围集合为 ${a{j1},…,a{jS_j}}$，$x_i^{(j)}$ 是第 i 个岩本的第 j 个特征；$I$ 为指示函数。 算法 计算先验概率及条件概率 对于给定的实例，计算后验概率 确定实例 x 的类：分到后验概率最大的类 贝叶斯估计用极大似然估计可能会出现要估计的概率值为 0 的情况。这回影响到后验概率的计算结果，产生分类偏差。解决这一问题的方法是使用贝叶斯估计。具体地，条件概率的贝叶斯估计是 式中 $\\lambda\\ge0$，当 λ = 0 时就是极大似然估计。 同样，先验概率的贝叶斯估计是 常取 λ = 1，这时称为拉普拉斯平滑。 频率学派与贝叶斯学派频率学派认为世界是确定的，事件在多次重复实验中趋于一个稳定的值 p，这个值就是该事件的概率。 频率学派直接对事件本身建模，认为模型参数是定值，希望通过类似解方程组的方式从数据中求得该未知数。这就是频率学派使用的参数估计方法——极大似然估计（MLE），这种方法往往在大数据量的情况下可以很好的还原模型的真实情况。 贝叶斯学派则认为世界是不确定的，因获取的信息不同而不同。贝叶斯学派对世界有一个预先的估计，然后通过获取的信息来不断调整之前的估计。 贝叶斯学派不试图对事件本身进行建模，认为模型参数源自某种潜在分布，希望从数据中推知该分布。这就是贝叶斯学派用来估计参数的常用方法——最大后验概率估计（MAP）。这种方法在先验假设比较靠谱的情况下效果显著，随着数据量的增加，先验假设对于模型参数的主导作用会逐渐削弱，相反真实的数据样例会大大占据有利地位。如果把先验假设去掉，或者假设先验满足均匀分布的话，那它和极大似然估计就如出一辙了。 极大似然估计与最大后验概率估计根据已知的一些数据样本，来推测产生该数据的模型的参数，即已知数据，推测模型和参数。根据两大派别的不同，对于模型的参数估计方法也有两类：极大似然估计与最大后验概率估计。 极大似然估计（MLE） 极大似然就是最大化该事件发生的可能性。根据已知样本，希望通过调整模型参数来使得模型能够最大程度地符合样本情况出现的概率。 例如，一个盒子里面有红黑共10个球，每次有放回的取出，取了10次，结果为7次黑球，3次红球。问拿出黑球的概率 p 是多少？ 假设7次黑球，3次红球的结果为事件 A，事件 A 发生的概率为 $P(A)=p^7*(1-p)^3$，极大似然估计就是让这个概率最大化，计算得 p = 0.7。 最大后验概率（MAP） 最大化在给定数据样本的情况下模型参数的后验概率。依然是根据已知样本，来通过调整模型参数使得模型能够产生该数据样本的概率最大，只不过对于模型参数有了一个先验假设，即模型参数可能满足某种分布，不再一味地依赖数据样例（万一数据量少或者数据不靠谱呢）。 例如，抛一枚硬币10次，有10次正面朝上，0次反面朝上。问正面朝上的概率 θ 。 在频率学派看来，用极大似然估计可以得到 θ = 1.0，但具有先验知识的我们直到，这显然是有很大偏差的。 如果利用最大后验概率来估计，先验认为符合概率分布 $P(\\theta|X)$，最大值介于 0.5~1之间。 显然，随着数据量的增加，参数分布会更倾向于向数据靠拢，先验假设的影响会越来越小。 五、决策树决策树是一种基本的分类与回归方法。主要优点是模型具有可读性，分类速度快。 5.1 决策树模型与学习分类决策树模型是一种描述对实例进行分类的树形结构。 可以将决策树看呈一个 if-then 规则的集合。由决策树的根结点到叶结点的每一条路径构建一条规则；路径上内部结点的特征对应着规则的条件，而叶结点的类对应着规则的结论。决策树的路径或其对应的 if-then 规则集合具有一个重要的性质：互斥并且完备。决策树还表示给定特征条件下类的条件概率分布。 决策树学习本质上是从训练数据集中归纳出一组分类规则。能正确分类的决策树可能有多个，也可能一个都没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。 决策树学习的损失函数通常是正则化的极大似然函数。决策树学习的策略是最小化目标函数。因为从所有可能的决策树种选取最优决策树是 NP 完全问题，所以现实中决策树学习算法通常采用启发式方法，近似求解这一最优化问题。这样得到的决策树是次最优的。 决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程： 开始，构建根结点，将所有训练数据都放在根结点。 选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集已经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些自己选择新的最优特征，继续对其进行分割，构建相应的结点。 如此递归下去，直到所有训练数据子集被基本正确分类，或者没有合适的特征为止。 最后每个子集都被分类到叶结点上，即都有了明确的类，这就生成了一颗决策树。 以上方法能对训练数据较好地分类，但可能发生过拟合现象。我们需要对已生成的树自下而上进行剪枝，使树变得更简单，从而提高泛化能力。具体地，就是去掉过于细分的叶结点，使其回退到父结点甚至更高的结点，然后将父结点或更高的结点改为新的叶结点。 可以看出，决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。 5.2 特征选择特征选择在于选取对训练数据具有分类能力的特征。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的，可以扔掉。通常特征选择的准则是信息增益或信息增益比。 熵 在信息论与概率统计中，熵（entropy）是表示随机变量不确定性的度量。设 X 是一个取有限个值的李赛随机变量，其概率分布为 $P(X=x_i)=p_i, i=1,2,…,n$，则随机变量 X 的熵定义为 ​ $H(X)=-\\sum\\limits_{i=1}^np_ilogp_i$ 由定义可知： 熵只依赖于 X 的分布，与 X 的取值无关，所以也可将 X 的熵记作 H(p) 熵越大，随机变量的不确定性就越大 $0\\le H(p)\\le logn$ 条件熵 设有随机变量的联合概率分布 ​ $P(X=xi,Y=y_i)=p{ij}$ 条件熵 H(Y|X) 表示在已知随机变量 X 的条件下随机变量 Y 的不确定性，定义为 X 给定条件下 Y 的条件概率分布的熵对 X 的数学期望 ​ $H(Y|X)=\\sum\\limits_{i=1}^np_iH(Y|X=x_i)$ 信息增益 信息增益表示得知特征 X 的信息而使得类 Y 的信息的不确定性减少的程度。当熵和条件熵的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵和经验条件熵。 特征 A 对训练数据集 D 的信息增益 g(D, A)，定义为集合 D 的经验熵 H(D) 与特征 A 给定条件下 D 的经验条件熵 H(D|A) 之差，即 ​ $g(D,A)=H(D)-H(D|A)$ 一般地，熵 H(Y) 与条件熵 H(Y|X) 之差称为互信息，决策树学习中的信息增益等价于训练数据集中类与特征的互信息。信息增益依赖于特征，不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。 根据信息增益准则的特征选择方法是：计算每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。 设训练数据集为 D，|D|表示其样本容量。设有 K 个类 $Ck$，k=1, 2, …, K，$|C_k|$ 为属于类 $C_k$ 的样本个数。设特征 A 有 n 个不同的取值 ${a_1,a_2,…,a_n}$，根据特征 A 的取值将 D 划分为 n 个子集 $D_1,D_2,…,D_n$，$|D_i|$ 为 $D_i$ 的样本个数。设子集 $D_i$ 中属于类 $C_k$ 的样本的集合为 $D{ik}$。 信息增益的算法 计算数据集 D 的经验熵 H(D) ​ $H(D)=-\\sum\\limits_{k=1}^K\\frac{|C_k|}{|D|}log_2\\frac{|C_k|}{|D|}$ 计算特征 A 对数据集 D 的经验条件熵 H(D|A) ​ $H(D|A)=\\sum\\limits{i=1}^n\\frac{|D_i|}{|D|}H(D_i)=-\\sum\\limits{i=1}^n\\frac{|Di|}{|D|}\\sum\\limits{k=1}^K\\frac{|D{ik}|}{|D_i|}log_2\\frac{|D{ik}|}{|D_i|}$ 计算信息增益 ​ $g(D,A)=H(D)-H(D|A)$ 信息增益比的算法 以信息增益作为划分训练数据集的特征，存在偏向于选择取值较多的特征的问题。使用信息增益比可以对这一问题进行矫正。 信息增益比 $g_R(D,A)$ 定义为其信息增益 $g(D,A)$ 与训练数据集 D 关于特征 A 的值的熵 $H_A(D)$ 之比，即 ​ $g_R(D,A)=\\frac{g(D,A)}{H_A(D)}$ 其中，$HA(D)=-\\sum\\limits{i=1}^n\\frac{|D_i|}{|D|}log_2\\frac{|D_i|}{|D|}$，n 是特征 A 取值的个数。 5.3 决策树的生成ID3 算法ID3 算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。ID3 相当于用极大似然法进行概率模型的选择。 ID3 算法只有树的生成，所以该算法生成的树容易产生过拟合。 C4.5 算法C4.5 在生成的过程中，用信息增益比来选择特征。 5.4 决策树的剪枝递归生成决策树往往对训练数据分类的很准确，但对位置的测试数据的分类却没有那么准确，即出现过你和现象。对决策树进行简化的过程称为剪枝，有助于提高模型的泛化能力。 决策树的剪枝往往通过极小化决策树整体的损失函数来实现。 设树 T 的结点个数为 |T|，t 是树 T 的叶节点，该叶节点有 $Nt$ 个样本点，其中 k 类的样本点有 $N{tk}$ 个，k = 1, 2, …, K，$H_t(T)$ 为叶节点 t 上的经验熵，$\\alpha\\ge0$ 为参数。 决策树学习的损失函数定义为 ​ $C\\alpha(T)=\\sum\\limits{t=1}^{|T|}N_tH_t(T)+\\alpha|T|$ 将损失函数右端的第 1 项记作 ​ $C(T)=\\sum\\limits_{t=1}^{|T|}N_tH_t(T)$ 这时有 ​ $C_\\alpha(T)=C(T)+\\alpha|T|$ 式中，C(T) 表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，|T|表示模型复杂度，参数 α 控制两者之间的影响。较大的 α 促使选择较简单的模型，较小的 α 促使选择较复杂的模型。α = 0 意味着只考虑模型与训练数据的拟合程度，不考虑模型对复杂度。 剪枝，就是当 α 确定时，选择损失函数最小的模型。当 α 确定时，子树越大，往往训练数据的拟合越好，但是模型的复杂度就越高；设一组叶节点回缩到其父节点之前与之后的整体树分别为 TB 与 TA，其对应的损失函数值分别是 $C\\alpha(T_B)$ 与 $C\\alpha(TA)$，如果 $C\\alpha(TA) \\le C\\alpha(TB)$，则进行剪枝，即将父节点变为新的叶节点。重复剪枝过程，直至不能继续，得到损失函数最小的子树 $T\\alpha$。 5.5 CART 算法分类与回归树（classification and regression tree，CART）模型既可以用于分类也可以用于回归。 CART 事在给定输入随机变量 X 条件下输出随机变量 Y 的条件概率分布的学习方法。CART 假设决策树是二叉树，内部节点特征的取值是“是”和“否”，左分支取值为“是”，右分支取值为“否”。这样的决策树等价于递归地二分每个特征，将输入空间划分为有限个单元，并在这写单元上确定预测的概率分布，也就是在输入给定的条件下输出条件概率分布。 CART 算法包括决策树生成与决策树剪枝两个步骤。 CART 生成递归地构建二叉树的过程，对回归树用平方误差最小化准则，对分类树用基尼指数最小化准则，进行特征选择，生成二叉树。 回归树的生成 在训练数据集所在的输入空间中，递归地将每个区域划分为两个子区域并决定每个子区域上的输出值，构建二叉决策树： （1）选择最优切分变量 j 与切分点 s，求解 遍历变量 j，对固定的切分变量 j 扫描切分点 s，选择使上式达到最小值的对 (j, s)。 （2）用选定的对 (j, s) 划分区域并决定相应的输出值： （3）继续对两个子区域调用步骤（1），（2）直至满足停止条件。 （4）将输入空间划分为 M 个区域 R1, R2, …, RM，生成决策树： ​ $f(x)=\\sum\\limits_{m=1}^M \\hat{c}_mI(x\\in R_m)$ 当输入空间的划分确定时，用平方误差 $\\sum\\limits_{x_i\\in R_m}(y_i-f(x_i))^2$ 来表示回归树对于训练数据的预测误差，用平方误差最小的准则求解每个单元上的最优输出值。 分类树的生成 分类树用基尼指数选择最优特征，同时决定该特征的最优二值切分点。 基尼指数 分类问题中，假设有 K 个类，样本点属于第 k 类的概率为 $p_k$，则概率分布的基尼指数定义为 ​ $Gini(p)=\\sum\\limits{k=1}^Kp_k(1-p_k)=1-\\sum\\limits{k=1}^Kp_k^2$ 对于二分类问题，$Gini(p)=2p(1-p)$ 对于给定样本集合 D，基尼指数为 ​ $Gini(D)=1-\\sum\\limits_{k=1}^K(\\frac{|C_k|}{|D|})^2$ 这里，$C_k$ 是 D 中属于第 k 类的样本子集，K 是类的个数。 如果样本集合 D 根据特征 A 被划分为 D1 和 D2 两个部分，则在特征 A 的条件下，集合 D 的基尼指数定义为 ​ $Gini(D, A)=\\frac{|D_1|}{|D|}Gini(D_1)+\\frac{|D_2|}{|D|}Gini(D_2)$ 基尼指数 Gini(D) 表述集合 D 的不确定性，基尼指数值越大，表示样本集合的不确定性就越大。 在选择特征的切分点时，选择基尼指数最小的特征及其对应的切分点作为最优特征于最优切分点。算法停止计算的条件是结点中的样本个数小于预定阈值或样本集的基尼指数小于预定阈值，或者没有更多特征。 CART 剪枝算法（1）设 k = 0，T = T0 （2）设 α = +∞ （3）自下而上地对各内部节点 t 计算 $C(T_t), |T_t|$ 以及 ​ $g(t)=\\frac{C(t)-C(T_t)}{|T_t|-1}$ ​ $\\alpha=min(\\alpha,g(t))$ 这里，Tt 表示以 t 为根节点的子树，$C(T_t)$ 是对训练数据的预测误差，$|T_t|$ 是 Tt 的叶节点个数。 （4）对 $g(t)=\\alpha$ 的内部节点 t 进行剪枝，并对叶节点 t 以多数表决法决定其类，得到树 T （5）设 k = k + 1，$\\alpha_k=\\alpha, T_K=T$ （6）如果 Tk 不是由根节点及两个叶节点构成的树，则回到步骤（2）；否则令 $T_k=T_n$ （7）采用交叉验证法在子树序列 T0, T1, …, Tn 中选取最优子树 $T_\\alpha$。 六、逻辑回归与最大熵模型逻辑回归（logistic regression）是经典分类算法。最大熵是概率模型学习的一个准则。逻辑回归模型与最大熵模型都属于对数线性模型。 6.1 逻辑回归模型逻辑分布设 X 是连续随机变量，X 服从逻辑分布是指 X 具有以下分布函数和密度函数： 二项逻辑回归模型二项逻辑回归随机变量 Y 取值为 1 或 0，条件规律分布如下： 逻辑回归比较两个条件概率值的大小，将实例 x 分到概率值较大的那一类。其中，w, b 是学习参数。 一个事件的几率（odds）是指该事件发生的概率与不发生的概率的比值。如果事件发生的概率是 p，那么该事件的几率是 $\\frac{p}{1-p}$，该事件的对数几率或 logit 函数是 ​ $logit(p)=log\\frac{p}{1-p}$ 则 $log\\frac{P(Y=1|x)}{1-P(Y=1|x)}=w\\cdot x$ 这就是说，逻辑回归模型中，输出 Y = 1 的对数几率是输入 x 的线性函数。 模型参数估计可以用用极大似然估计法估计模型参数，从而得到逻辑回归模型。 设 $P(Y=1|x)=\\pi(x), P(Y=0|x)=1-\\pi(x)$ 似然函数为 $\\prod\\limits_{i=1}^N[\\pi(x_i)]^{y_i}[1-\\pi(x_i)]^{1-y_i}$ 对数似然函数为 对 L(w) 求极大值，得到 w 的估计值。 这样，问题就变成了以对数似然函数为目标函数的最优化问题。通常采用梯度下降法及拟牛顿法。 6.2 最大熵模型最大熵原理最大熵原理认为，学习概率模型时，在所有可能的概率模型（分布）中，熵最大的模型是最好的模型。最大熵原理也可以表述为在满足约束条件的模型集合中选取熵最大的模型。 假设离散随机变量 X 的概率分布是 P(X)，则其熵是 $H(P)=-\\sum\\limits_xP(x)logP(x)$ 熵满足下列不等式：$0\\le H(P)\\le log|X|$，其中 |X| 是 X 的取值个数，当 X 服从均匀分布时，熵最大。 最大熵模型将最大熵原理应用到分类得到最大熵模型。 给定训练数据集，可以确定联合分布 P(X, Y) 的经验分布和边缘分布 P(X) 的经验分布： 其中，$\\nu(X=x,Y=y)$ 表示训练数据中样本 (x, y) 出现的频数，N 表示训练样本容量。 用特征函数 f(x, y) 描述输入 x 和输入 y 之间的某一个事实，其定义是： ​ $f(x, y)=\\begin{cases}1,&amp;x与y满足某一事实\\0,&amp;否则\\end{cases}$ 特征函数 f(x, y) 关于经验分布 $\\tilde P(X,Y)$ 的期望值： ​ $E{\\tilde P}(f)=\\sum\\limits{x,y}\\tilde P(x,y)f(x,y)$ 特征函数 f(x, y) 关于模型 P(Y|X) 与经验分布 $\\tilde P(X)$ 的期望值： ​ $EP(f)=\\sum\\limits{x,y}\\tilde P(x)P(y|x)f(x,y)$ 如果模型能够获取训练数据中的信息，那么就可以假设这连个期望值相等，即 $EP(f)=E\\tilde P(f)$ 将 $EP(f)=E\\tilde P(f)$ 作为模型学习的约束条件。 假设满足所有约束条件的模型集合为 定义在条件概率分布 P(Y|X) 上的条件熵为 ​ $H(P)=-\\sum\\limits_{x,y}\\tilde P(x)P(y|x)logP(y|x)$ 则模型集合 C 中条件熵 H(P) 最大的模型称为最大熵模型。 最大熵模型的学习最大熵模型的学习过程就是求解最大熵模型的过程，可以形式化为约束最优化问题： 等价的最小值问题： 求解最优化问题的解就是最大熵模型学习的解。 七、支持向量机支持向量机（support vector machines，SVM）的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大是它有别于感知机；支持向量机还包括核技巧，这使它称为实质上的非线性分类器。支持向量机的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。支持向量机的学习算法时求解凸二次规划的最优化算法。 线性可分支持向量机 线性支持向量机 非线性支持向量机 7.1 线性可分支持向量机考虑一个二分类问题。假设输入空间与特征空间为两个不同的空间，输入空间为欧氏空间或离散集合，特征空间为欧氏空间或希尔伯特空间。假设这两个空间的元素一一对应，并将输入空间中的输入映射为特征空间中的特征向量。 学习的目标是在特征空间中找到一个分离超平面，能将实例分到不同的类。当训练数据集线性可分时，存在无穷个分离超平面可将两类数据正确分开。感知机利用误分类最小的策略求得分离超平面，不过这时的解有无穷多个。线性可分支持向量机利用间隔最大化求最优分离超平面，这时，解是唯一的。 定义学习得到的分离超平面为 ​ $w\\cdot x+b=0$ 相应的分类决策函数 ​ $f(x)=sign(w\\cdot x+b)$ 称为线性可分支持向量机。 函数间隔为 $\\tilde \\gamma_i=y_i(w\\cdot x_i+b)$ 超平面为函数间隔最小值 $\\tilde \\gamma=\\min\\limits_{i=1,…,N}\\tilde\\gamma_i$ 函数间隔可以表示分类预测的正确性及确信度，但是选择分离超平面时，只有函数间隔还不够，因为成比例地改变 w 和 b，超平面并没有改变，但是函数间隔却变化了。因此，可以对分离超平面的法向量 w 加某些约束，如规范化，||w|| = 1，使得间隔是确定的。这时函数间隔成为几何间隔。 ​ $\\gamma_i=\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||}$ 其中，||w||为 w 的 L2 范数。这是点 A 在超平面正的一侧的情形。如果在负的一侧，即 $y_i=-1$，那么点与超平面的距离为 ​ $\\gamma_i=-(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})$ 一般地，当样本点 $(x_i,y_i)$ 被超平面 (w, b) 正确分类时，点 $x_i$ 与超平面 (w, b) 的距离是 ​ $\\gamma_i=y_i(\\frac{w}{||w||}\\cdot x_i+\\frac{b}{||w||})$ 也就是超平面的几何间隔。 间隔最大化对线性可分的训练数据集而言，线性可分分离超平面有无穷多个（等价于感知机），但是几何间隔最大的分离超平面是唯一的。间隔最大化是找到几何间隔最大的超平面，可以表示为约束最优化问题： 函数间隔的取值并不影响最优化问题的解，可以取 $\\gamma=1$。注意到，最大化 $\\frac{1}{||w||}$ 和最小化 $\\frac{1}{2}||w||^2$ 是等价的，于是得到下面的最优化问题： 这是一个凸二次规划问题。求解该约束最优化问题，用到了最大间隔法。 H1、H2是两个超平面，在 H1、H2 上的点就是支持向量。H1 与 H2 平行，并且没有点落在它们中间。H1 与 H2 之间的距离称为间隔，间隔等于 $\\frac{2}{||w||}$。H1、H2 称为间隔边界。 在决定分离超平面时只有支持向量起作用，其他实例点不起作用。支持向量的个数一般很少，所以支持向量机由很少的“重要的”训练样本确定。 7.2 线性支持向量机与软间隔最大化线性支持向量机线性可分支持向量机对线性不可分的训练数据是不适用的，因为约束条件并不能都成立。为了扩展到线性不可分问题，需要将硬间隔最大化修改为软间隔最大化。 线性不可分的数据集通常有一些特异点，去掉这些特异点后，剩下的大部分样本点组成的集合是线性可分的。线性不可分意味着某些样本点 $(x_i,y_i)$ 不能满足函数间隔大于等于 1 的约束条件。为了解决这个问题，可以对每个样本点引进一个松弛变量 $\\xi_i\\ge 0$，使函数间隔加上松弛变量大于等于 1。这样，约束条件变为 ​ $y_i(w\\cdot x_i+b)\\ge1-\\xi_i$ 目标函数变为 ​ $\\frac{1}{2}||w||^2+C\\sum\\limits_{i=1}^N\\xi_i$ 这里，C &gt; 0 为惩罚参数，C 值大时对误分类的惩罚增大。相应的学习问题称为软间隔最大化。 可以证明 w 的解是唯一的，但 b 的解可能不唯一，而是存在一个区间。 7.3 非线性支持向量机与核函数核技巧非线性分类问题是指通过利用非线性模型才能很好地进行分类的问题。非线性问题往往不好求解，采取的方法是进行一个非线性变换，将非线性问题变换为线性问题，通过解变换后的线性问题的方法求解原来的非线性问题。 核技巧应用到支持向量机，其基本想法就是通过一个非线性变换将输入空间对应于一个特征空间，使得在输入空间中的超曲面模型对应于特征空间中的超平面模型（支持向量机）。这样通过在特征空间中求解线性支持向量机就可以完成。 核函数 设 $\\mathcal{X}$ 是输入空间（欧氏空间 $R^n$ 的子集或离散集合），$\\mathcal{H}$ 为特征空间（希尔伯特空间），如果存在一个从 $\\mathcal{X}$ 到 $\\mathcal{H}$ 的映射 ​ $\\phi(x):\\mathcal{X}\\rightarrow\\mathcal{H}$ 使得对所有 $x,z\\in\\mathcal{X}$，函数 $K(x,z)$ 满足条件 ​ $K(x,z)=\\phi(x)\\cdot\\phi(z)$ 则称 K(x, z) 为核函数，$\\phi(x)$ 为映射函数。 核技巧在学习与预测中之定义核函数 K(x, z)，而不显式地定义映射函数 $\\phi$。通常，直接计算 K(x, z) 比较容易，而通过 $\\phi(x)$ 和 $\\phi(z)$ 计算 K(x, z) 并不容易。映射函数 $\\phi$ 的取法并不唯一。学习是隐式地在特种空间进行的，不需要显式地定义特征空间和映射函数。在实际应用中，往往依赖领域知识直接选择核函数，核函数的有效性需要通过实验验证。 常用核函数 多项式核函数（polynomial kernel function） ​ $K(x,z)=(x\\cdot z+1)^p$ 高斯核函数（Gaussian kernel function） ​ $K(x,z)=exp(-\\frac{||x-z||^2}{2\\sigma^2})$ 八、提升方法提升（boosting）方法在分类问题中，通过改变训练样本的权重，学习多个分类器，并将这些分类器进行线性组合，提高分类的性能。 8.1 AdaBoost 算法提升方法的思想是：对于一个复杂任务来说，将多个专家的判断进行适当的综合得出的判断，要比其中任何一个专家单独的判断好。 对于分类问题而言，给定一个训练样本集，求比较粗糙的分类规则（弱分类器）要比求精确的分类规则（强分类器）容易得多。提升方法通过反复学习得到一系列弱分类器（基本分类器），然后组合这些弱分类器，构成一个强分类器。大多数的提升方法都是改变训练数据的概率分布（权值分布），针对不同的训练数据分布学习一系列弱分类器。 如何改变训练数据的权值或概率分布？AdaBoost 的做法是，提高那些被前一轮弱分类器错误分类样本的权值，而降低那些被正确分类样本的权值。 如何将弱分类器组合成一个强分类器？AdaBoost 采取加权多数表决的方法，加大分类误差率小的弱分类器的权值，使其在表决中起较大的作用；减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。 在训练过程中，它不改变所给的训练数据，而是不断改变训练数据权值的分布，使得被误分类的数据再后一轮的分类中受到更大的关注。 8.2 GBDT提升树提升树是以分类树或回归树为基本分类器的提升方法，被认为是统计学习中性能最好的方法之一。提升树实际采用加法模型（基函数的线性组合）与前向分步算法。以决策树为基函数的提升方法称为提升树（boosting tree）。 GBDTGBDT（Gradient Boosting Decision Tree） 是基于 boosting 的思想，串行地构造多棵决策树来进行数据的预测，它是在损失函数所在的函数空间中做梯度下降，即把待求的决策树模型当作参数，每轮迭代都去拟合损失函数在当前模型下的负梯度，从而使得参数朝着最小化损失函数的方向更新。 GBDT 可以看作是 AdaBoost 的一个推广，AdaBoost 是通过错分数据点来识别问题，通过调整错分数据点的权重来改进模型，GBDT 是通过负梯度来识别问题，通过计算负梯度来改进模型，实际上，负梯度绝对值大的样例同样会在之后的训练中受到更大的关注。相比 AdaBoost, Gradient Boosting 可以使用更多类型的损失函数，因此可以解决更多的问题。 8.3 随机森林（Random Forest）随机森林算法背后的思想是群体智慧的体现，它通过随机的行采样（bagging）和列采样（feature bagging）构造不同的训练集，建立一个决策树森林，利用加权平均方式或多数表决的方式得到最后的预测结果。随机森林能够并行学习，对噪声和异常数据具有很好的过滤作用，因此有很广泛的应用。 随机森林的行采样（bagging）和列采样（feature bagging）都是为了减小模型之间的相关性使基学习器变得不同从而减小集成模型的方差，但这种随机性会导致随机森林的偏差有所增加（相比于单棵不随机树），因此随机森林的单棵树都会采用很深的决策树，并不进行剪枝操作，以减小每棵树的偏差，这使得每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从全部特征中选择部分特征来让每一棵决策树学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终再通过投票或平均得到结果。这也正是群体智慧的体现。 8.4 XGboostXGboost 是梯度提升树（GBDT）的一种高效系统实现，是对 GBDT 进一步的改进，包括对代价函数进行了二阶泰勒展开，在代价函数里加入了正则项，借鉴了随机森林的列采样方法，支持并行计算等。具体为： 传统 GBDT 在优化时只用到一阶导数信息，xgboost 则对代价函数进行了二阶泰勒展开，同时用到了一阶和二阶导数。另外，xgboost 工具支持自定义代价函数，只要函数可一阶和二阶求导。 xgboost 在代价函数里加入了正则项，用于控制模型的复杂度。正则项降低了模型的方差，使学习出来的模型更加简单，防止过拟合。 xgboost 借鉴了随机森林的做法，支持列抽样，不仅能降低过拟合，还能减少计算。 xgboost 工具支持并行。xgboost 也是一次迭代完才能进行下一次迭代，它的并行是在特征粒度上的。决策树的学习最耗时的一个步骤就是对特征的值进行排序（因为要确定最佳分割点），xgboost 在训练之前，预先对数据进行了排序，然后保存为 block 结构，后面的迭代中重复地使用这个结构，大大减小计算量。 8.5 LightGBMLightGBM 是一个实现 GBDT 算法的分布式高效框架。它通过 leaf-wise 分裂方法进行决策树的生成，通过基于直方图的算法寻找特征分割点，并支持并行学习，能够更高效的处理大数据，也得到了越来越广泛的应用。 要减少训练的复杂度，可以通过减少特征量和数据量来实现，即从行和列两个角度来减少数据，同时要尽可能少的影响最后的精度。在 LightGBM 中，就是这样做的，对应着 GOSS 和 EFB： Gradient-based One-Side Sampling (GOSS)：GBDT 虽然没有数据权重，但每个数据实例有不同的梯度，根据计算信息增益的定义，梯度大的实例对信息增益有更大的影响，因此在下采样时，我们应该尽量保留梯度大的样本（预先设定阈值，或者最高百分位间），随机去掉梯度小的样本。此措施在相同的采样率下比随机采样获得更准确的结果，尤其是在信息增益范围较大时。从而减少训练的样本数量。 Exclusive Feature Bundling (EFB)：通常在真实应用中，虽然特征量比较多，但是由于特征空间十分稀疏，许多特征几乎是互斥的（例如许多特征不会同时为非零值，像 one-hot），EFB 通过捆绑互斥的特征，并将捆绑问题归约到图着色问题，通过贪心算法求得近似解，以减少特征数量。 另外，它支持并行学习，包括特征并行和数据并行： 特征并行的主要思想是不同机器在不同的特征集合上分别寻找最优的分割点，然后在机器间同步最优的分割点。（mapreduce （分治）思想） 数据并行则是让不同的机器先在不同的记录集合上构造直方图，然后进行全局的合并，最后在合并的直方图上面寻找最优分割点。 8.6 bagging 与 boosting 的区别？Bagging 和 Boosting 都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，即将弱分类器组装成强分类器的方法。 首先介绍 Bootstraping（自助法），是一种有放回的抽样方法。 Bagging（套袋法），算法过程如下： 从原始样本集 D 中抽取训练集。每轮从原始样本集 D 中使用 Bootstraping 自助的方法抽取 n 个训练样本（有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行 k 轮抽取，得到 k 个训练集。（k个训练集之间是相互独立的） 每次使用一个训练集得到一个模型，k 个训练集共得到 k 个模型。 对于分类问题，将上步得到的 k 个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。 Boosting（提升法）的主要思想是将弱分类器组装成一个强分类器。关于 Boosting 的两个核心问题： 每一轮如何改变训练数据的权值或概率分布？ ​ 通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据更多的关注。 通过什么方式来组合弱分类器？ ​ 通过加法模型将弱分类器进行线性组合，比如 AdaBoost 通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。 两者区别： 样本选择： Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。 Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。 样本的权重： Bagging：使用均匀取样，每个样例的权重相等。 Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。 预测函数： Bagging：所有预测函数的权重相等。 Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。 并行计算： Bagging：各个预测函数可以并行生成。 Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。 Bagging + 决策树 = 随机森林（Random Forest） AdaBoost + 决策树 = 提升树（Boosting Tree） Gradient Boosting + 决策树 = GBDT 两者之间如何选择？ Bagging 通过平均方法（averaging）减小方差，如果模型具有很高的偏差，Bagging 并不会对模型有很大的影响。 对于稳定的模型来说，Bagging 并不会工作地很好，而 Boosting 可得会有帮助。如果在训练集上有 noisy 数据，Boosting 会很快地过拟合，降低模型的性能，而Bagging 不存在这样地问题。 8.7 模型融合 stacking 与 blending 的区别？votingvoting 为投票方法，是模型融合策略中最简单的一种方法，其融合过程不需要建立新的模型，只需要在单一模型的输出结果上完成融合。可以分为硬投票和软投票： 硬投票（Hard Voting）是指对每个模型给出的样本分类结果以少数服从多数的方式产生最终结果。 软投票（Soft Voting）是指将各个模型预测样本为某一类别的概率的平均值大小来决定所属类别。 averagingaveraging 为平均方法，也比较简单，思路是对多个模型的结果取平均（或加权平均）。bagging 和 boosting 方法都基于此。 StackingStacking 是一种嵌套组合型的模型融合方法，其基本思路就是在第一层训练多个不同的基学习器，然后把第一层训练的各个基学习器的输出作为输入来训练第二层的学习器，从而得到一个最终的输出。 BlendingBlending 模型与 Stacking 模型预测过程大致相似，通常情况下模型训练也是要经过两轮，不同之处在于，Blending 划分的训练集不需要交叉验证，而是通过独立划分出来的验证集输入到基模型中，得到第二层模型的训练数据。 十、隐马尔可夫模型隐马尔可夫模型（hidden Markov model，HMM）是可用于标注问题的统计学习模型，描述由隐藏的马尔科夫链随机生成观测序列的过程，属于生成模型。 10.1 基本概念隐马尔可夫模型是关于时序的概率模型，描述一个隐藏的马尔可夫链随机生成不可观测的状态随机序列，再由各个状态生成一个观测，从而产生观测随机序列的过程。 隐藏的马尔可夫链随机生成的状态的序列，称为状态序列；每个状态生成一个观测，由此产生的观测的随机序列，称为观测序列。 隐马尔可夫模型由初始概率分布、状态转移概率分布以及观测概率分布确定。 设 Q 是所有可能的状态的集合，V 是所有可能的观测的集合： ​ $Q={q_1,q_2,…,q_N}, V={v_1,v_2,…,v_M}$ I 是状态序列，O 是对应的观测序列： ​ $I=(i_1,i_2,…,i_T), O=(o_1,o_2,…,o_T)$ A 是状态转移概率矩阵： ​ $A=[a{ij}]{N\\times N}$ 其中，$a{ij}=P(i{t+1}=q_j|i_t=q_i)$ 是在时刻 t 处于状态 $q_i$ 的条件下在时刻 t + 1 转移到状态 $q_j$ 的概率。 B 是观测概率矩阵： ​ $B=[bj(k)]{N\\times M}$ 其中，$b_j(k)=P(o_t=v_k|i_t=q_j)$ 是在时刻 t 处于状态 $q_j$ 的条件下生成观测 $v_k$ 的概率。 $\\pi$ 是初始状态概率向量： ​ $\\pi=(\\pi_i)$ 其中，$\\pi_i=P(i_1=q_i)$ 是时刻 t = 1 处于状态 $q_i$ 的概率。 因此，隐马尔可夫模型 λ 可以表示为： ​ $\\lambda=(A,B,\\pi)$ $A,B,\\pi$ 称为隐马尔可夫模型的三要素。 隐马尔可夫模型的两个基本假设： 假设隐藏的马尔可夫链在任意时刻 t 的状态只依赖于其前一时刻的状态，与其他时刻的状态及观测无关 假设任意时刻的观测只依赖于该时刻的马尔可夫链的状态，与其他观测及状态无关 隐马尔可夫模型的 3 个基本问题 概率计算问题：给定模型 $\\lambda=(A,B,\\pi)$ 和观测序列 $O=(o_1,o_2,…,o_T)$，计算观测序列 O 出现的概率 P(O|λ)。 学习问题：已知观测序列 O，估计模型 λ 的参数，使得在该模型下观测序列概率 P(O|λ) 最大。 预测问题（解码问题）：已知模型 λ 和观测序列 O，求对给定观测序列条件概率 P(I|O) 最大的状态序列 I。 10.2 概率计算算法直接计算法给定模型 λ 和观测序列 O，计算观测序列 O 出现的概率 P。最直接的方法是按概率公式直接计算。通过列举所有可能的长度为 T 的状态序列 I，求各个状态序列 I 与观测序列 O 的联合概率 P(O, I|λ)，然后对所有可能的状态序列求和，得到 P(O|λ)。 但是，这样计算的时间复杂度为 $O(TN^T)$，算法不可行。 前向算法前向概率 给定隐马尔可夫模型 λ，定义到时刻 t 部分观测序列为 $o_1,o_2,…,o_t$ 且状态为 $q_i$ 的概率为前向概率，记作 ​ $\\alpha_t(i)=P(o_1,o_2,…,o_t,i_t=q_i|\\lambda)$ 可以递推地求得前向概率 $\\alpha_t(i)$ 及观测序列概率 $P(O|\\lambda)$。 （1）初值 ​ $\\alpha_1(i)=\\pi_ib_i(o_1), i=1,2,…,N$ （2）递推 对 t = 1, 2, …, T - 1， ​ $\\alpha{t+1}(i)=[\\sum\\limits{j=1}^N\\alphat(j)a{ji}]bi(o{t+1})， i=1,2,…,N$ （3）终止 ​ $P(O|\\lambda)=\\sum\\limits_{i=1}^N\\alpha_T(i)$ 例子： 后向算法后向概率 给定隐马尔可夫模型 λ，定义在时刻 t 状态为 $qi$ 的条件下，从 t + 1 到 T 的部分观测序列为 $o{t+1}, o_{t+2},…,o_T$ 的概率为后向概率，记作 ​ $\\betat(i)=P(o{t+1},o_{t+2},…,o_T|i_t=q_i,\\lambda)$ 可以用递推地方法求得后向概率 $\\beta_t(i)$ 及观测序列概率 $P(O|\\lambda)$。 10.3 学习问题已知观测序列 $O=(o_1,o_2,…,o_T)$，估计模型 $\\lambda=(A,B,\\pi)$ 参数，使得在该模型下观测序列概率 $P(O|\\lambda)$ 最大。用极大似然估计的方法估计参数。Baum-Welch 算法，也就是 EM 算法可以高效地对隐马尔可夫模型进行训练，它是一种无监督学习算法。 10.4 预测问题已知模型 $\\lambda=(A,B,\\pi)$ 和观测序列 $O=(o_1,o_2,…,o_T)$，求对给定观测序列条件概率 $P(O|\\lambda)$ 最大的状态序列 $I=(i_1,i_2,…,i_T)$。维特比算法应用动态规划高效地求解最优路径，即概率最大的状态序列。 十一、条件随机场条件随机场（conditional random field，CRF）是给定一组输入随机变量条件下另一组输出随机变量的条件概率分布模型，其特点是假设输出随机变量构成马尔可夫随机场。线性链条件随机场应用于标注问题，由输入序列对输出序列预测的判别模型，形式为对数线性模型，其学习方法通常是极大似然估计或正则化的极大似然估计。 11.1 概率无向图模型概率无向图模型，又称为马尔可夫随机场（Markov random field），是一个可以由无向图表示的联合概率分布。 设有联合概率分布 P(Y)，由无向图 G = (V, E) 表示，在图 G 中，结点表示随机变量，边表示随机变量之间的依赖关系。如果联合概率分布 P(Y) 满足成对、局部或全局马尔可夫性，就称此联合概率分布为概率无向图模型，或马尔可夫随机场。 因子分解将概率无向图模型的联合概率分布表示为其最大团上的随机变量的函数的乘积形式的操作，称为概率无向图模型的因子分解。 11.2 条件随机场条件随机场是给定随机变量 X 的条件下，随机变量 Y 的马尔科夫随机场。 设 X 与 Y 是随机变量，P(Y|X) 是在给定 X 的条件下 Y 的条件概率分布。若随机变量 Y 构成一个由无向图 G=(V, E) 表示的马尔可夫随机场，则称条件概率分布 P(Y|X) 为条件随机场。 若 X，Y 均为线性链表示的随机变量序列，则称 P(Y|X) 为线性链条件随机场。 监督学习方法总结 十四、聚类无监督学习是从无标注的数据中学习数据的统计规律或内在结构的机器学习，主要包括聚类、降维、概率估计。 无监督学习可用于数据分析或者监督学习的前处理。 聚类（clustering）是将样本集合中相似的样本分配到相同的类，不相似的样本分配到不同的类。 相似度或距离聚类的核心概念是相似度或距离。在进行聚类时，选择合适的距离或相似度非常重要。 闵可夫斯基距离 对于样本 $xi=(x{1i},x{2i},…,x{mi})^T,xj=(x{1j},x{2j},…,x{mj})^T$，样本 $x_i$ 与样本 $x_j$ 的闵可夫斯基距离定义为 ​ $d{ij}=(\\sum\\limits{k=1}^m|x{ki}-x{kj}|^p)^{\\frac{1}{p}}, p\\ge1$ 当 p = 2 时，称为欧氏距离，即 ​ $d{ij}=(\\sum\\limits{k=1}^m|x{ki}-x{kj}|^2)^{\\frac{1}{2}}$ 当 p = 1 时，称为曼哈顿距离，即 ​ $d{ij}=\\sum\\limits{k=1}^m|x{ki}-x{kj}|$ 当 p = ∞ 时，称为切比雪夫距离，取各个坐标数值差的绝对值的最大值，即 ​ $d{ij}=\\max\\limits_k|x{ki}-x_{kj}|$ 马哈拉诺比斯距离 简称马氏距离，考虑各个分量（特征）之间的相关性并于各个分量的尺度无关。 给定一个样本集合 X，$X=[x{ij}]{m\\times n}$，其协方差矩阵记作 S。样本 $x_i$ 与样本 $x_j$ 的马哈拉诺比斯距离定义为 ​ $d_{ij}=[(x_i-x_j)^TS^{-1}(x_i-x_j)]^{\\frac{1}{2}}$ 当 S 为单位矩阵矩阵时，即样本数据的各个分量相互独立且各分量的方差为 1 时，马氏距离就是欧氏距离，所以马氏距离是欧氏距离的推广。 相关系数 样本 $x_i$ 与样本 $x_j$ 的相关系数定义为 其中 夹角余弦 样本 $x_i$ 与样本 $x_j$ 的夹角余弦定义为 14.1 K-means14.2 DBSCANDBSCAN（Density-Based Spatial Clustering of Applications with Noise）是一基于密度的聚类算法，DBSCAN 将簇定义为密度相连的点的最大集合，能够把具有足够高密度的区域划分为簇，并可在噪声的空间数据库中发现任意形状的聚类。 定义DBSCAN 是基于一组邻域来描述样本集的紧密程度的。对于样本集 $D={x_1, x_2, …, x_m}$，相关定义如下： $(\\epsilon,MinPts)$ 用来描述邻域的样本分布的紧密程度。其中，$\\epsilon$ 表示某一数据点的邻域距离阈值（半径），$MinPts$ 表示数据点半径为 $\\epsilon$ 的邻域中数据点个数的最小值。 $\\epsilon-邻域$：样本集 D 中与 $x_j$ 的距离不大于 $\\epsilon$ 的子样本集。 核心对象：如果 $x_j$ 的 $\\epsilon -邻域$ 至少包含 MinPts 个样本，则 $x_j$ 是核心对象。 密度直达：如果 $x_i$ 位于 $x_j$ 的 $\\epsilon-邻域$ 中，且 $x_j$ 是核心对象，则称 $x_i$ 由 $x_j$ 密度直达，反之不一定成立，除非 $x_i$ 也是核心对象。 密度可达：对于 $xi$ 和 $x_j$ ,如果存在样本样本序列 $p_1,p2,…,p_T$ ,满足 $p_1=x_i,p_T=x_j$ , 且 $p{t+1}$ 由 $pt$ 密度直达，则称 $x_j$ 由 $x_i$ 密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本 $p_1,p2,…,p{T-1}$ 均为核心对象。 密度相连：对于 xi 和 xj ,如果存在核心对象样本 xk ，使xi 和 xj 均由 xk 密度可达，则称 xi 和 xj 密度相连。密度相连关系满足对称性。 由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别，或者说一个簇。如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的 ϵ -邻域里；如果有多个核心对象，则簇里的任意一个核心对象的 ϵ -邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。这些核心对象的 ϵ -邻域里所有的样本的集合组成的一个 DBSCAN 聚类簇。 另外，由三个问题需要考虑： 异常点问题：一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。 距离度量问题：即如何计算某样本和核心对象样本的距离。在DBSCAN中，一般采用某一种距离度量来衡量样本距离，比如欧式距离、曼哈顿距离等。 数据点优先级分配问题：例如某些样本可能到两个核心对象的距离都小于 ϵ ，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？一般来说，此时 DBSCAN 采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说 DBSCAN 的算法不是完全稳定的算法。 算法 初始化核心对象集合 Ω=∅，初始化类别 k=0 遍历 D 的元素，如果是核心对象，则将其加入到核心对象集合 Ω 中 如果核心对象集合 Ω 中元素都已经被访问，则算法结束，否则转入步骤 4 在核心对象集合 Ω 中，随机选择一个未访问的核心对象 o ，首先将 o 标记为已访问，然后将 o 标记类别 k ，最后将 o 的 ϵ -邻域中未访问的数据，存放到种子集合 Seeds 中。 如果种子集合 Seeds=∅ ，则当前聚类簇 $C_k$ 生成完毕, 且 k=k+1 ，跳转到3。否则，从种子集合 Seeds 中挑选一个种子点 seed ，首先将其标记为已访问、标记类别 k ，然后判断 seed 是否为核心对象，如果是将 seed 中未访问的种子点加入到种子集合中，跳转到5。 算法优点： 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。 算法缺点： 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。 如果样本集较大时，聚类收敛时间较长。 调参相对于传统的 K-Means 之类的聚类算法稍复杂，主要需要对距离阈值 ϵ，邻域样本数阈值 MinPts 联合调参，不同的参数组合对最后的聚类效果有较大影响。 14.3 分层聚类方法分层聚类法是聚类分析中的一种算法，就是对给定数据对象的集合进行层次分解，根据分层分解采用的分解策略，分层聚类法又可以分为凝聚的（agglomerative，即自底向上）和分裂的（divisive，即自顶向下）分层聚类。 凝聚的分层聚类采用自底向上的策略，首先将每一个对象作为一个类，然后根据某种度量(如2个当前类中心点的距离)将这些类合并为较大的类，直到所有的对象都在一个类中，或者是满足某个终止条件时为止，绝大多数分层聚类算法属于这一类，它们只是在类间相似度的定义上有所不同。 分裂的分层聚类采用与凝聚的分层聚类相反的策略——自顶向下，它首先将所有的对象置于一个类中，然后根据某种度量逐渐细分为较小的类，直到每一个对象自成一个类，或者达到某个终止条件(如达到希望的类个数，或者2个最近的类之间的距离超过了某个阈值)。 十五、奇异值分解（SVD）任意一个 m × n 矩阵，都可以表示为三个矩阵的乘积形式，分别是 m 阶正交矩阵、降序排列的非负的 m × n 对角矩阵和 n 阶正交矩阵。矩阵的奇异值分解一定存在，但不唯一。奇异值分解可以看作是矩阵数据压缩的一种方法。 首先回顾下特征值和特征向量的定义： $Ax=\\lambda x$ 其中，A 是 n×n 矩阵，x 是 n 维向量，则 λ 是 矩阵 A 的一个特征值，x 是矩阵 A 的特征值 λ 对应的特征向量。 求出特征值就可以将矩阵 A 特征分解： $A=W\\sum W^{-1}$ 其中，W 是 n 个特征向量组成的 n×n 的矩阵， 是由 n 个特征值为主对角线的 n×n 的对角矩阵。 将 n 个向量标准化（$||w_i||_2=1$）后，此时的 n 个特征向量为标准正交基，满足 $W^TW=I$，即 $W^T=W^{-1}$。于是，特征分解表达式可以写为： $A=W\\sum W^T$ 如果 A 不是方阵，还可以对矩阵进行分解吗？奇异值分解（SVD）就实现了这一点。 SVDSVD 也是对矩阵进行分解，但是和特征分解不同，SVD 并不要求要分解的矩阵为方阵。假设我们的矩阵 A 是一个 m×n 的矩阵，那么我们定义矩阵 A 的SVD为： $A=U\\sum V^T$ 其中，U 是一个 m×m 的矩阵；Σ 是一个 m×n 的矩阵，主对角线上的元素为奇异值，其他元素都为 0；V 是一个 n×n 的矩阵。U 和 V 都是酉矩阵，即满足 $U^TU=I, V^TV=I$。 SVD 计算对于一个 m×n 的矩阵： 对 n×n 的方阵 $A^TA$ 进行特征分解，得到 n 个特征值和对应的特征向量，特征向量组成 n×n 的矩阵 V。V 中的每个特征向量叫作 A 的右奇异向量。 对 m×m 的方阵 $AA^T$ 以同样的方法计算得到 m×m 的矩阵 U，U 中的每个特征向量叫作 A 的左奇异向量。 求每个奇异值。 证明： 奇异值跟特征分解中的特征值类似，在奇异值矩阵中也是按照从大到小排列，而且奇异值的减少特别的快，在很多情况下，前 10% 甚至 1% 的奇异值的和就占了全部的奇异值之和的 99% 以上的比例。也就是说，我们也可以用最大的 k 个的奇异值和对应的左右奇异向量来近似描述矩阵。 其中，对于大矩阵，k 比 n 小得多。由于这个重要的性质，SVD 可以用于 PCA 降维，来做数据压缩和去噪。也可以用于推荐算法，将用户和喜好对应的矩阵做特征分解，进而得到隐含的用户需求来做推荐。 左奇异矩阵可以用于行数的压缩。 右奇异矩阵可以用于列数即特征维度的压缩，也就是 PCA 降维。 十六、主成分分析（PCA）主成分分析（Principal components analysis，PCA）是最重要的降维方法之一。在数据压缩消除冗余和数据噪音消除等领域都有广泛的应用。降维是将高维度的数据保留下最重要的一些特征，去除噪声和不重要的特征，从而实现提升数据处理速度的目的。在实际的生产和应用中，降维在一定的信息损失范围内，可以为我们节省大量的时间和成本。 PCA 将 n 维特征映射到 k 维上，这 k 维特征是全新的正交特征，也被称为主成分。PCA 的工作就是从原始的空间中顺序地找一组相互正交的坐标轴，其中，第一个新坐标轴选择是原始数据中方差最大的方向，第二个新坐标轴选取是与第一个坐标轴正交的平面中使得方差最大的，第三个轴是与第 1, 2 个轴正交的平面中方差最大的。依次类推，可以得到 n 个这样的坐标轴。我们发现，大部分方差都包含在前面 k 个坐标轴中，后面的坐标轴所含的方差几乎为 0。于是，我们可以忽略余下的坐标轴，只保留前 k 个含有绝大部分方差的坐标轴，从而实现对数据特征的降维处理。 方差最大的主成分的计算通过计算数据矩阵的协方差矩阵，然后计算协方差矩阵的特征值与特征向量，选择特征值最大（即方差最大）的 k 个特征所对应的特征向量组成的矩阵。可以使用特征分解或奇异值分解的方法。 算法的优缺点优点： 仅仅需要以方差衡量信息量，不受数据集以外的因素影响 各主成分之间正交，可消除原始数据成分间的相互影响的因素 计算方法简单，主要运算是特征值分解，易于实现 缺点： 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强 方差小的非主成分也可能含有对样本差异的重要信息，因此降维丢弃可能对后续数据处理有影响 十七、线性判别分析（LDA）PCA 降维是无监督的，没有利用数据的标签信息。有时候我们需要直到降维后的一些与标签 y 关系最密切的最佳特征。 LDA 是一种监督学习的降维技术，也就是说它的数据集的每个样本是有类别输出的。 LDA 的基本思想：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点中心尽可能远离。更简单的概括为一句话，就是“投影后类内方差最小，类间方差最大”。 LDA 除了可以用于降维以外，还可以用于分类。一个常见的 LDA 分类基本思想是假设各个类别的样本数据符合高斯分布（正态分布），这样利用 LDA 进行投影后，可以利用极大似然估计计算各个类别投影数据的均值和方差，进而得到该类别高斯分布的概率密度函数。当一个新的样本到来后，我们可以将它投影，然后将投影后的样本特征分别带入各个类别的高斯分布概率密度函数，计算它属于这个类别的概率，最大的概率对应的类别即为预测类别。 优缺点优点： 在降维过程中可以使用类别的先验知识经验，而 PCA 这样的无监督学习则无法使用类别先验知识 LDA 在样本分类信息依赖均值而不是方差的时候，比 PCA 之类的算法较优 缺点： LDA 不适合对非高斯分布样本进行降维，PCA 也有这个问题 LDA 降维最多降到类别数 k-1 的维数，如果我们降维的维度大于 k-1，则不能使用 LDA。当然目前有一些LDA的进化版算法可以绕过这个问题 LDA 在样本分类信息依赖方差而不是均值的时候，降维效果不好 LDA 可能过度拟合数据 十八、潜在语义分析（LSA）潜在语义分析（LSA）是一种用于知识获取和展示的计算理论和方法，出发点就是文本中的词与词之间存在某种联系，即存在某种潜在的语义结构。这种潜在的语义结构隐含在文本中词语的上下文使用模式中。因此采用统计计算的方法，对大量的文本中进行分析来寻找这种潜在的语义结构，它不需要确定的语义编码，仅依赖于上下文中事物的联系，并用语义结构来表示词和文本，达到消除词之间的相关性，简化文本向量的目的。 LSA 通过对原文本库的项／文本矩阵的奇异值分解，并取前 K 个最大的奇异值及对应的奇异向量构成一个新矩阵来近似表示原文本库的项／文本矩阵，由于新矩阵削减了项和文本之间语义关系的模糊度，从而有利于信息检索。 优缺点优点： LSA 利用潜在的语义结构表示词条和文本，将词条和文本映射到同一个 k 维的语义空间内，均表示为 k 个因子的形式，它反映的不再是简单的词条出现频率和分布关系，而是强化的语义关系。 由于词条和文本被映射到同一 k 维的语义空间，所以在 LSA 模型中不仅能够进行传统的词条与词条、文本与文本之间的相似关系分析，而且能够分析词条与文本之间的相似关系，与传统的向量空间模型相比，具有更好的灵活性。 对于原始的词条一文本矩阵，通过 LSA 提取出 k 维语义空间。这样用低维词条、文本向量代替原始的空间向量，可以有效地处理大规模的文本库。 LSA 不需要人工干预，不需要先验知识。 缺点： LSA 在进行信息提取时，忽略词语的语法信息（甚至是忽略词语在句子中出现顺序），认为语法结构在文本的语义表达中处于次要的地位。 LSA 处理的对象是可见变量（文本集中出现的词语、文本），它不能通过计算得到词语的暗喻含义，以及类比推论含义。","link":"2022/10/20/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0%E6%96%B9%E6%B3%95-%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"}],"tags":[{"name":"BERT","slug":"BERT","link":"tags/BERT/"},{"name":"Mysql","slug":"Mysql","link":"tags/Mysql/"},{"name":"GLUE","slug":"GLUE","link":"tags/GLUE/"},{"name":"ToD","slug":"ToD","link":"tags/ToD/"},{"name":"git","slug":"git","link":"tags/git/"},{"name":"Neo4j","slug":"Neo4j","link":"tags/Neo4j/"},{"name":"PaddleOCR","slug":"PaddleOCR","link":"tags/PaddleOCR/"},{"name":"XLNet","slug":"XLNet","link":"tags/XLNet/"},{"name":"关系数据库","slug":"关系数据库","link":"tags/%E5%85%B3%E7%B3%BB%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"ffmpeg","slug":"ffmpeg","link":"tags/ffmpeg/"},{"name":"会议","slug":"会议","link":"tags/%E4%BC%9A%E8%AE%AE/"},{"name":"hexo","slug":"hexo","link":"tags/hexo/"},{"name":"对话系统","slug":"对话系统","link":"tags/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"},{"name":"GNN","slug":"GNN","link":"tags/GNN/"},{"name":"机器学习","slug":"机器学习","link":"tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"算法","slug":"算法","link":"tags/%E7%AE%97%E6%B3%95/"},{"name":"PLM","slug":"PLM","link":"tags/PLM/"},{"name":"计算机网络","slug":"计算机网络","link":"tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C/"}],"categories":[{"name":"PLM","slug":"PLM","link":"categories/PLM/"},{"name":"Linux","slug":"Linux","link":"categories/Linux/"},{"name":"NLP","slug":"NLP","link":"categories/NLP/"},{"name":"对话系统","slug":"对话系统","link":"categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"},{"name":"工具使用","slug":"工具使用","link":"categories/%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"name":"Neo4j","slug":"Neo4j","link":"categories/Neo4j/"},{"name":"OCR","slug":"OCR","link":"categories/OCR/"},{"name":"学习笔记","slug":"学习笔记","link":"categories/%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"name":"人工智能","slug":"人工智能","link":"categories/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"},{"name":"博客","slug":"博客","link":"categories/%E5%8D%9A%E5%AE%A2/"},{"name":"深度学习","slug":"深度学习","link":"categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"}],"pages":[]}