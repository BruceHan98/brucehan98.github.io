<!doctype html>
<html lang="zh"><head><meta charset="utf-8"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta><title>图神经网络——综述 - Bruce Han&#039;s Blog</title><link rel="manifest" href="../../../../manifest.json"><meta name="application-name" content="Bruce Han&#039;s Blog"><meta name="msapplication-TileImage" content="/img/favicon.svg"><meta name="apple-mobile-web-app-capable" content="yes"><meta name="apple-mobile-web-app-title" content="Bruce Han&#039;s Blog"><meta name="apple-mobile-web-app-status-bar-style" content="default"><meta name="description" content="本文对 “A comprehensive survey on graph neural networks, IEEE 2021”这篇综述论文进行了翻译，旨在初步学习图神经网络并加强对文章内容的理解，供日后参考。"><meta property="og:type" content="blog"><meta property="og:title" content="图神经网络——综述"><meta property="og:url" content="https://brucehan98@github.io/2021/12/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BB%BC%E8%BF%B0/"><meta property="og:site_name" content="Bruce Han&#039;s Blog"><meta property="og:description" content="本文对 “A comprehensive survey on graph neural networks, IEEE 2021”这篇综述论文进行了翻译，旨在初步学习图神经网络并加强对文章内容的理解，供日后参考。"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="https://brucehan98@github.io/covers/gnn.png"><meta property="article:published_time" content="2021-12-22T07:05:18.000Z"><meta property="article:modified_time" content="2021-12-25T06:40:20.654Z"><meta property="article:author" content="Bruce Han"><meta property="article:tag" content="GNN"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="../../../../covers/gnn.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://brucehan98@github.io/2021/12/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BB%BC%E8%BF%B0/"},"headline":"图神经网络——综述","image":["https://brucehan98@github.io/covers/gnn.png"],"datePublished":"2021-12-22T07:05:18.000Z","dateModified":"2021-12-25T06:40:20.654Z","author":{"@type":"Person","name":"Bruce Han"},"publisher":{"@type":"Organization","name":"Bruce Han's Blog","logo":{"@type":"ImageObject","url":"https://brucehan98@github.io/img/logo.svg"}},"description":"本文对 “A comprehensive survey on graph neural networks, IEEE 2021”这篇综述论文进行了翻译，旨在初步学习图神经网络并加强对文章内容的理解，供日后参考。"}</script><link rel="canonical" href="https://brucehan98@github.io/2021/12/22/%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E2%80%94%E2%80%94%E7%BB%BC%E8%BF%B0/"><link rel="icon" href="../../../../img/favicon.svg"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.15.2/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/github.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="../../../../css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><!--!--><script>var _hmt = _hmt || [];
        (function() {
            var hm = document.createElement("script");
            hm.src = "//hm.baidu.com/hm.js?8ff5bda969cd5128662b3fbc6a899b16";
            var s = document.getElementsByTagName("script")[0];
            s.parentNode.insertBefore(hm, s);
        })();</script><!--!--><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><!--!--><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><!--!--><!--!--><meta name="generator" content="Hexo 5.4.0"></head><body class="is-2-column"><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="../../../../index.html"><img src="../../../../img/logo.svg" alt="Bruce Han&#039;s Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="../../../../index.html">首页</a><a class="navbar-item" href="../../../../archives">归档</a><a class="navbar-item" href="../../../../categories">分类</a><a class="navbar-item" href="../../../../tags">标签</a><a class="navbar-item" href="../../../../about">关于</a></div><div class="navbar-end"><a class="navbar-item night" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a><a class="navbar-item" target="_blank" rel="noopener" title="GitHub" href="../../../../https:/github.com/BruceHan98"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-9-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="fill" src="../../../../covers/gnn.png" alt="图神经网络——综述"></span></div><article class="card-content article" role="article"><div class="article-meta is-size-6 is-uppercase level is-mobile"><div class="level-left"><span class="level-item"><time dateTime="2021-12-22T07:05:18.000Z" title="2021-12-22T07:05:18.000Z">2021-12-22</time>发表</span><span class="level-item"><time dateTime="2021-12-25T06:40:20.654Z" title="2021-12-25T06:40:20.654Z">2021-12-25</time>更新</span><span class="level-item"><a class="link-muted" href="../../../../categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">深度学习</a></span><span class="level-item">2 小时读完 (大约17401个字)</span><span class="level-item" id="busuanzi_container_page_pv"><span id="busuanzi_value_page_pv">0</span>次访问</span></div><br></div><h1 class="title is-3 is-size-4-mobile">图神经网络——综述</h1><div class="content"><blockquote>
<p>本文对 “A comprehensive survey on graph neural networks, IEEE 2021”这篇综述论文进行了翻译，旨在加深对文章内容的理解，并供日后参考。综述原文：<a target="_blank" rel="noopener" href="https://ieeexplore.ieee.org/abstract/document/9046288/">A comprehensive survey on graph neural networks, IEEE 2021</a>。</p>
</blockquote>
<p>&emsp;&emsp;近年来，深度学习在许多机器学习任务上（如图像分类、视频处理、语音识别、自然语言处理）取得了开创性的成果。通常，这些任务的数据（如图像、序列信息）都是表示在欧氏空间的（Euclidean space）。然而越来越多的应用数据是非欧氏空间的，例如图数据用图结构来表示，不同对象之间有复杂的关系和内在依赖，使得现有的机器学习算法变得不再有效。</p>
<p>&emsp;&emsp;最近，出现了许多针对图数据的深度学习方法。本文对数据挖掘（data mining）和机器学习（machine learning）领域的图神经网络（Graph Neural Networks, GNNs）进行全面梳理，并将其分为四类：循环图神经网络（recurrent GNNs，RecGNNs），卷积神经网络（convolutional GNNs，ConvGNNs），图自编码器（graph autoencoders，GAEs）和时空神经网络（spatial-temporal GNNs，STGNNs）。</p>
<h3 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h3><p>&emsp;&emsp;神经网络近年来的成果促进了模式识别和数据挖掘的研究。许多机器学习任务，例如目标检测、机器翻译和语音识别等，曾严重依赖特征工程人工地提取特征信息，最近被各种端到端的深度学习方法所取代。深度学习方法的成功得益于快速发展的计算资源（如 GPU）和大量训练数据，以及从欧式空间数据（如图像、文本和视频）提取隐含表示的有效性。以图像数据为例，图像可以表示为欧氏空间的规则网格数据。CNN 能够利用图像数据的移位不变性、局部连通性和组合性提取可全局共享的有意义的局部特征。</p>
<p>&emsp;&emsp;深度学习能够有效捕获欧氏空间数据的隐藏模式，然而越来越多的应用数据表示为图的形式。例如，在电子商务中，一个基于图的学习系统可以利用用户和产品之间的交互做出高度准确的推荐；在化学领域，分子被表示为图模型，鉴定分子的生物活性可用于药物发现；在引文网络中，文章通过引文系统相互连接，需要将文章分为不同的组。图数据的复杂性给现有的机器学习算法带来了巨大的挑战。由于图是不规则的，节点无序且多变，并且节点的邻居数量也可能不同，导致一些重要操作（如卷积）在图像中容易计算但在图领域中却很难计算。另外，现有的机器学习算法的一个重要假设是数据实例之间的相互独立性，这个假设在图数据中不再成立，因为图的每个实例（节点）通过多变的连接相互关联，并非独立。</p>
<h3 id="2-背景介绍"><a href="#2-背景介绍" class="headerlink" title="2. 背景介绍"></a>2. 背景介绍</h3><h4 id="图神经网络简史"><a href="#图神经网络简史" class="headerlink" title="图神经网络简史"></a>图神经网络简史</h4><p>&emsp;&emsp;Sperduti 和 Starita （1997）首次将神经网络应用到有向无环图（directed acyclic graphs）中 [1]，激发了早期的对 GNNs 的研究。大多数早期的 GNNs 是 RecGNNs，它们通过迭代的方式传播邻居信息直到达到一个稳定值，以此来学习目标节点的表示，然而这个过程的计算成本很高。</p>
<p>&emsp;&emsp;鉴于 CNNs 在计算机视觉领域的成功，ConvGNNs 得到了大量研究，主要分为两类：基于谱（spectral-based）的方法和基于空间（spatial-based）的方法。基于谱的 ConvGNNs 的第一个重要研究是由 Bruna 等人（2014）提出的基于谱图论（spectral graph theory）的图卷积 [2]。从此，基于谱的 ConvGNNs 得到了越来越多的改进、扩展和研究。实际上，基于空间的 ConvGNNs 的研究要早于基于谱的 ConvGNNs。2009年，Micheli 继承了 RecGNNs 中消息传递的思想，通过复合非递归层（composite nonrecursive layers）首次解决了图的相互依赖性 [3]。然而这篇重要的工作的在当时被忽略了，直到最近才出现了许多基于空间的 ConvGNNs。</p>
<p>&emsp;&emsp;除了 RecGNNs 和 ConvGNNs，最近几年出现了一些其他的 GNNs，包括 GAEs 和 STGNNs 等。</p>
<h4 id="图神经网络-vs-网络嵌入"><a href="#图神经网络-vs-网络嵌入" class="headerlink" title="图神经网络 vs 网络嵌入"></a>图神经网络 vs 网络嵌入</h4><p>&emsp;&emsp;GNNs 的研究与图嵌入或网络嵌入是十分相关的。网络嵌入（Network Embedding）目的是在保留网络拓扑结构和节点内容信息的同时将网络节点表示为低维向量表示，从而方便将现有的机器学习算法应用到下游的图分析任务中，例如分类、聚类和推荐等。另一方面，GNNs 是以端到端的方式解决图相关任务的深度学习模型，目的是提取图、节点等的高层表示。GNNs 和网络嵌入之间的主要区别是，GNNs 是针对不同任务设计的一组神经网络模型，而网络嵌入涵盖网络节点嵌入的不同方法，包括非深度学习方法，例如矩阵分解、随机游走等。</p>
<h4 id="图神经网络-vs-图核方法"><a href="#图神经网络-vs-图核方法" class="headerlink" title="图神经网络 vs 图核方法"></a>图神经网络 vs 图核方法</h4><p>&emsp;&emsp;图核（Graph kernels）是早期解决图分类问题主流方法，这种方法使用核函数来度量图之间的相似性。与 GNNs 类似，图核方法可以通过映射函数将图或节点嵌入向量空间。不同的是，图核的映射函数是确定的，而 GNNs 是可学习的。</p>
<h3 id="3-分类和框架"><a href="#3-分类和框架" class="headerlink" title="3. 分类和框架"></a>3. 分类和框架</h3><h4 id="A-图神经网络分类"><a href="#A-图神经网络分类" class="headerlink" title="A. 图神经网络分类"></a>A. 图神经网络分类</h4><ul>
<li>循环图神经网络（Recurrent Graph Neural Networks, RecGNNs）：大多数早期的 GNNs 是循环图神经网络（下称 RecGNNs），它通过循环神经结构学习节点表示。RecGNNs 认为图中的一个节点不断地与其邻居节点交换信息直到达到稳定平衡。其消息传递（message passing）的思想在基于空间的卷积图神经网络（spatial-based ConvGNNs）中也得到了应用。</li>
<li>卷积图神经网络（Convolutional Graph Neural Networks, ConvGNNs）：这是将卷积操作从网格数据到图数据的拓展。主要思想是通过整合节点本身及其邻居节点的特征来表示该节点。</li>
<li>图自编码器（Graph Autoencoders, GAEs）：GAEs 是无监督的学习框架，编码器将节点、图编码到隐含向量空间，然后通过解码器从编码的信息重构图数据。</li>
</ul>
<h4 id="B-框架"><a href="#B-框架" class="headerlink" title="B. 框架"></a>B. 框架</h4><p>​&emsp;&emsp;图神经网络的输入是<code>图结构（graph structure）</code>和<code>节点的内容信息（node content information）</code>，输出可用于不同的图分析任务。</p>
<ul>
<li>节点级别（Node level）：用于<code>节点回归（node regression）</code>和<code>节点分类（node classification）</code>任务。RecGNNs 和 ConvGNNs 分别通过信息传播和图卷积捕获高层节点表示，然后接多层感知机或 softmax 层作为输出层，从而可以端到端地处理节点级别的任务。</li>
<li>边级别（Edge level）：用于<code>边分类（edge classification）</code>和<code>链接预测（link prediction）</code>任务。通过 GNNs 得到两个节点的隐藏表示，使用相似度函数或者神经网络可以预测节点之间的边的标签（label）或连接强度（connection strength）。</li>
<li>图级别（Graph level）：用于<code>图分类（graph classification）</code>任务。为了得到图级别的紧凑表示（compact representation），GNNs 通常与池化（pooling）和读出（readout）操作相结合。</li>
</ul>
<p><strong>训练框架</strong></p>
<p>​&emsp;&emsp;许多 GNNs 能够通过端到端的学习框架以（半）监督或完全无监督的方式训练，这取决于学习任务和可获得的标签信息。</p>
<ul>
<li>节点分类的半监督学习：对于只有部分节点标签的图网络，ConvGNNs 能够学习一个鲁棒的模型用于高效地判断（identify）无标签节点的标签。</li>
<li>图分类的监督学习：图分类的目的是预测整个图的类别。该任务可以结合图卷积层、图池化层和读出层进行端到端学习。其中，图卷积层提取高层节点表示；图池化层进行下采样，将图粗化（coarsen）为子结构；读出层将每个图的节点表示细化为图表示；最后使用多层感知机或 softmax 层对图表示进行分类。</li>
<li>图嵌入的无监督学习：当图中没有类别标签时，可以通过端到端的框架以完全无监督的方式学习图嵌入。这些利用边信息的算法分为两种：第一种简单的方法是采用自编码器框架，其中编码器使用图卷积层将图嵌入到隐藏表示，解码器用隐藏表示重构图结构；另一种流行的方法是利用负采样将一部分节点作为负样本对（negative pairs），而图中存在连接的节点对作为正样本对。然后使用逻辑回归层来区分正负样本对，以此来学习图嵌入。</li>
</ul>
<h3 id="4-循环图神经网络"><a href="#4-循环图神经网络" class="headerlink" title="4. 循环图神经网络"></a>4. 循环图神经网络</h3><p>&emsp;&emsp;RecGNNs 是最早得到研究的 GNNs，考虑到计算能力，早期的研究主要集中在有向无环图（directed acyclic graphs）。Scarselli 等人（2009）[4] 将之前的递归模型扩展到处理一般类型的图的模型 GNN*（为区分于广泛意义上的 GNN，用 GNN*来表示该模型），如无环图、有环图、有向图、无向图。基于信息扩散机制（information diffusion mechanism），GNN* 通过循环交换邻居信息来更新节点状态直到达到一个稳定平衡。节点的隐藏状态通过下面的公式更新：</p>
<script type="math/tex; mode=display">
h^{(t)}_v = \sum_{u \in N(v)} f(x_v, x^e_{(v, u)}, x_u, h^{(t-1)}_u)</script><p>&emsp;&emsp;其中，$f(·)$ 是参数函数，$h^{(0)}_v$ 是随机初始化的。该求和函数使得 GNN* 适用于任何节点，即使节点的邻居数量不同且不知道邻居的顺序。为了保证收敛，递归函数 $f(·)$ 必须是收缩映射，将两个点映射到潜在空间后收缩两点之间的距离。如果 $f(·)$ 是神经网络，那么参数的雅可比矩阵必须要施加一个惩罚项。当满足收敛准则时，再将最后一层节点的隐藏状态前向传播到读出层。GNN* 通过计算交替节点状态传播核参数梯度来最小化训练目标，这种策略使 GNN* 能够解决有环图。在后续工作中，GraphESN（graph echo state network）（2010）[5] 扩展了 echo state 网络以提高 GNN* 的训练效率。</p>
<p>&emsp;&emsp;GGNN （Gated GNN）（2015）[6] 使用了门控循环单元（GRU）作为循环函数，将循环次数降低到一个固定的步骤数，其优点是不再需要约束参数来确保收敛。节点隐藏状态通过其先前的隐藏状态和相邻的隐藏状态更新：</p>
<script type="math/tex; mode=display">
h^{(t)}_v = GRU(h^{(t-1)}_v, \sum_{u \in N(v)} Wh^{(t-1)}_u)</script><p>&emsp;&emsp;其中，$h^{(0)}_v = x_v$ 。与 GNN* 和 GraphESN 不同，GGNN 使用时间反向传播（backpropagation through time, BPTT）算法来学习模型参数。GGNN 需要对所有节点多次迭代计算，并需要将所有结点的中间状态存储在内存中，这对于大型图来说是有问题的。</p>
<p>&emsp;&emsp;SSE（Stochastic steady-state embedding）（2018）[7] 是一种对大型图更具扩展性的学习算法，以随机和异步的方式重复更新节点隐藏状态。它对一批节点进行采样以进行状态更新，并对一批节点进行梯度计算。为了保持稳定性，SSE 的循环函数为历史状态和新状态的加权平均：</p>
<script type="math/tex; mode=display">
h^{(t)}_v = (1 - \alpha )h^{(t-1)}_v + \alpha W_1 \sigma (W_2[x_v, \sum _{u \in N(v)}[h^{(t-1)}_u, x_u]])</script><p>&emsp;&emsp;其中，$\alpha$ 是超参数，$h^{(0)}_v$ 是随机初始化的。虽然 SSE 在概念上很重要，但是它没有从理论上证明通过重复利用该函数，节点状态能够收敛到稳定值。</p>
<h3 id="5-卷积图神经网络"><a href="#5-卷积图神经网络" class="headerlink" title="5. 卷积图神经网络"></a>5. 卷积图神经网络</h3><p>&emsp;&emsp;由于卷积操作效率更高并且更易于与其他网络组合，近年来对 ConvGNNs 的研究迅速增长。ConvGNNs 分为两类：基于谱的（spectral-based）与基于空间的（spatial-based）。基于谱的方法从图信号处理（graph signal processing）的角度引入了滤波器，将图卷积运算解释为从图信号中去除噪声；基于空间的方法继承了 RecGNNs 的信息传播的思想，将图卷积定义为信息传播。由于 GCN（2017）[8] 弥合了基于谱的方法和基于空间的方法之间的鸿沟，基于空间的方法由于其高效、灵活和通用，最近得到了迅速发展。</p>
<h4 id="A-基于谱的-ConvGNNs"><a href="#A-基于谱的-ConvGNNs" class="headerlink" title="A. 基于谱的 ConvGNNs"></a>A. 基于谱的 ConvGNNs</h4><p>&emsp;&emsp;基于谱的方法基于很强的图信号处理的数学基础。无向图的数学表示为归一化的图拉普拉斯矩阵（normalized graph Laplacian matrix）是，定义为 $ \mathrm{L = I_n - D ^{(1/2)}AD^{-(1/2)}}$，其中 $\mathrm D$ 是节点度的对角矩阵，$\mathrm{D_ij =  \sum_j(A_{i,j})}$。归一化的图拉普拉斯矩阵具有实对称半正定的性质，基于此，可以分解为 $\mathrm{L = U\Lambda U^T}$，其中 $\mathrm U = [u_0, u1, …, u_{n-1}] \in R^{n \times n}$ 是按特征值排序的特征向量矩阵，$ \Lambda $ 是特征值的对角矩阵（谱），$\Lambda_{ij} = \lambda_i$。在数学上，归一化拉普拉斯矩阵的特征向量形成一个正交空间 $\mathrm {U^TU = I}$。在图信号处理中，一个图信号 $x \in R^n $ 是图的所有节点的一个特征向量，其中 $x_i$ 是第 i 个节点的特征值。一个信号 $\mathrm x$ 的图傅里叶变换定义为 $\mathscr F(x) = \mathbf {U^T} \mathbf x$，逆变换为 $\mathscr F^{-1}(\hat x) = \mathbf {U \hat x}$，其中 $\mathbf {\hat {x}}$ 表示图傅里叶变换后的信号。图傅里叶变换将输入的图信号映射到正交空间，基由归一化的图拉普拉斯算子的特征向量构成。变换后的信号 $\hat x$ 的元素是新空间中图信号的坐标，因此输入的信号可以表示为 $x = \sum_i \hat x_iu_i$，这正是图傅里叶逆变换。于是，输入信号 $\mathbf x$ 与滤波器 $\mathbf {g \in R^n}$ 定义为:</p>
<script type="math/tex; mode=display">
\mathbf {x *_G g = \mathscr F^{-1}(\mathscr F(x) \odot \mathscr F(g)) = U(U^T x \odot U^T g)}</script><p>&emsp;&emsp;其中，$\odot$ 表示元素乘积（elementwise product）。如果我们将滤波器表示为 $\mathbf g_\theta = diag(\mathbf {U^T g})$，那么谱图卷积（spectral graph convolution）就被简化为：</p>
<script type="math/tex; mode=display">
\mathbf {x *_G g_\theta = U g_\theta U^T x}</script><p>&emsp;&emsp;基于谱的 ConvGNNs 遵循以上定义，主要区别在于滤波器 $\mathbf g_\theta$ 的选择。Spectral CNN（2014）[9] 将滤波器 $\mathbf g_\theta = \Theta^{(k)}_{i,j}$ 当作可学习的参数并且可以处理多通道的图信号。其图卷积层定义为：</p>
<p><img src="https://s2.loli.net/2021/12/22/4u8WniDajIgm5wR.png" alt="image-20211222101128859" style="zoom: 50%;" /></p>
<p>&emsp;&emsp;其中，$k$ 是层索引，$\mathbf {H^{(k-1)} \in R^{n \times f_{k-1}} }$ 是输入的图信号，$\mathbf {H{(0)} = X}$，$f_{k-1}$ 是输入的通道数，$f_k$ 是输出的通道数，$\Theta^{(k)}_{i,j}$ 是可学习参数的对角矩阵。由于使用拉普拉斯矩阵特征分解（eigendecomposition），Spetral CNN 有三个限制：第一，图的任何扰动都会导致特征基的变化；第二，学习的滤波器依赖域，这意味着不能应用到具有不同结构的图中；第三，特征分解的计算复杂度为 $O(n^3)$。后续的工作中，ChebNet 和 GCN 通过一些近似和简化将复杂度降低到 $O(m)$。</p>
<p>&emsp;&emsp;ChebNet（CHebyshev spectral CNN）（2016）[10] 用对角矩阵中特征值的 Chebyshev 多项式来近似滤波器：$\mathbf {g_\theta = \sum ^K_{i=0} \theta_i T_i (\hat {\Lambda})}$ ，其中 $\mathbf {\hat {\Lambda} = 2 \Lambda / \lambda_{max} - I_n}$，$\hat {\Lambda} \in [-1, 1]$。 Chebyshev 多项式是递归定义的：$T_i(x) = 2xT_{i-1}(x) - T_{i-2}(x)$， $T_0(x) = 1$，$T_1(x) = x$。于是，图信号 $\mathbf x$ 的卷积与滤波器 $\mathbf g_\theta$ 的定义为：</p>
<p><img src="https://s2.loli.net/2021/12/22/wmnZABCshO8YaI7.png" alt="image-20211222103147531" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$\mathbf {\hat L = 2L/\lambda _{max} - I_n}$。由于可以通过归纳证明 $T_i(\hat {\mathbf L}) = \mathbf {U} T_i (\hat {\mathbf \Lambda}) \mathbf {U}^T$，ChebNet 可以表示为：</p>
<p><img src="https://s2.loli.net/2021/12/22/xUaPEtQvqFA4mlB.png" style="zoom:50%;" /></p>
<p>&emsp;&emsp;作为对 Spectral CNN 的改进，ChebNet 定义的滤波器在空间中局部化，使滤波器可以不考虑图的大小来提取局部特征。ChebNet 的光谱被线性映射到 [-1, 1]。CayleyNet（2019）[11] 进一步应用参数有理复函数（parametric rational complex functions）的 Cayley 多项式来捕获窄频带（narrow frequency bands）。Cayley 的谱图卷积定义为：</p>
<p><img src="https://s2.loli.net/2021/12/22/eLyqcfubVOPHtRB.png" alt="image-20211222104603410" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$\mathrm {Re(·)}$ 返回复数的实部，$c_0$ 是实数系数，$c_j$ 是复数系数，$i$ 是虚数，$h$ 是控制 Cayley 滤波器频谱的参数。在保持空间局部性的同时，CayleyNet 证明 ChebNet 可以看作是 CayleyNet 的特例。</p>
<p>&emsp;&emsp;GCN（Graph convolutional network）对  ChebNet 进行了一阶近似。假设 $K = 1, \lambda_{max} = 2$，于是 ChebNet 简化为：</p>
<p><img src="https://s2.loli.net/2021/12/22/akOM9fm2v8ncLHb.png" alt="image-20211222105458861" style="zoom:50%;" /></p>
<p>&emsp;&emsp;为了限制参数量且避免过拟合，GCN 进一步假设 $\theta = \theta_0 = -\theta_1$，从而图卷积的定义如下：</p>
<p><img src="https://s2.loli.net/2021/12/22/BJ1uxsmKcMf3nUD.png" alt="image-20211222105700948" style="zoom:50%;" /></p>
<p>&emsp;&emsp;为了能满足多通道输入和输出，GCN 将上式修改为：</p>
<p><img src="https://s2.loli.net/2021/12/22/RFCiGWV3Zyaxk4M.png" alt="image-20211222105838472" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$\mathbf {\bar A = I_n + D^{-(1/2)}AD^{-(1/2)}}$，$f(·)$ 是激活函数。$\bar {\mathbf A}$ 会导致 GCN 的数值不稳定，为了解决这个问题，GCN 应用了一个规范化的技巧，用 $\mathbf {\bar A = \tilde D^{-(1/2)} \tilde A \tilde D^{-(1/2)}}$ 代替$\mathbf {\bar A = I_n + D^{-(1/2)}AD^{-(1/2)}}$。其中，$\mathbf {\tilde A = A + I_n, \tilde D_{ii} = \sum_j \tilde A_{ij}}$。作为一种基于谱的方法，GCN 也可以解释为基于空间的方法。从基于空间的角度来看，GCN 可以看作是从节点的邻居聚合特征信息：</p>
<p><img src="https://s2.loli.net/2021/12/22/3lh2ZrwKsfoM4Ya.png" alt="image-20211222110756503" style="zoom:50%;" /></p>
<p>&emsp;&emsp;最近的一些工作通过探索其他对称矩阵对 GCN 进行了改进。AGCN（Adaptive GCN）（2018）[12] 通过图邻接矩阵学习隐藏结构关系，它将两个节点的特征作为输入学习距离函数来构造残差图邻接矩阵。DGCN（Dual GCN）（2018）[13] 引入了一种双图（dual-graph）卷积结构，具有两个并行的共享参数的图卷积层，使用标准化的邻接矩阵 $\mathbf {\bar A}$ 和正点互信息（positive pointwise mutual information, PPMI）矩阵，对图随机游走采样捕获节点共现信息。PPMI 矩阵定义为：</p>
<p><img src="https://s2.loli.net/2021/12/22/1DSWM5HfnOdlGyt.png" alt="image-20211222112558702" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$v_1, v_2 \in V, |D| = \sum _{v_1, v_2} count(v_1, v_2)$ 和 $count(·)$ 函数返回节点 $v$ 和/或节点 $u$ 在随机游走采样中共现/出现的频率。通过对双图卷积层的输出进行聚合，DGCN 可以对局部和全局结构信息进行编码，而不需要堆叠多个图卷积层。</p>
<h4 id="B-基于空间的-ConvGNNs"><a href="#B-基于空间的-ConvGNNs" class="headerlink" title="B. 基于空间的 ConvGNNs"></a>B. 基于空间的 ConvGNNs</h4><p>&emsp;&emsp;与传统的图像卷积的 CNN 类似，基于空间的方法通过节点的空间关系来定义图卷积。如果将像素表示为节点，图像的卷积可以看作是图卷积的特例。在图像的卷积上，滤波器对每个通道上的中心节点及其邻居节点的像素值进行加权平均。类似地，基于空间的图卷积对中心节点表示及其邻居节点表示进行卷积，以此来更新中心节点的表示，如下图所示。从另一角度看，基于空间的 ConvGNNs 与 RecGNNs 具有相同的信息传播/消息传播思想。空间图卷积运算本质上是沿着边传播节点信息。</p>
<p><img src="https://s2.loli.net/2021/12/22/zZCi4E623hMnomY.png" alt="image-20211222113810808" style="zoom:67%;" /></p>
<p>&emsp;&emsp;与 GNN* 同时提出的 NN4G（Neural network for graphs）（2009）[14] 是第一个基于空间的 ConvGNNs。与 RecGNNs 明显不同的是，NN4G 通过一个每层具有独立参数的组合神经结构学习图的相互依赖性，并通过增加架构结构来扩展节点的邻居数。NN4G 通过直接汇总节点的邻居信息进行图卷积，并且应用残差连接来记忆每一层的信息。NN4G 通过下面的公式更新下一层节点：</p>
<p><img src="https://s2.loli.net/2021/12/22/A2Hq5swZjdyBmKz.png" alt="image-20211222115027748" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$f(·)$ 是激活函数，$h^{(0)}_v = 0$。上式的矩阵形式为：</p>
<p><img src="https://s2.loli.net/2021/12/22/mWy7b5P6eoHCcdR.png" alt="image-20211222115136740" style="zoom:50%;" /></p>
<p>&emsp;&emsp;与 GCN 不同的是，NN4G 使用非规范化的邻接矩阵，这可能会导致隐藏节点状态大小差别非常大。CGMM（Contextual graph Markov model）（2018）[15] 提出了一个受 NN4G 启发的概率模型，在保持空间局部性的同时，CGMM 具有概率解释能力。DCNN（Diffusion CNN）(2016）[16] 将图卷积看作一个扩散过程，它假设信息以一定的转移概率从一个节点转移到其邻居节点上，从而信息分布在几轮之后达到平衡。DCNN 将扩散图卷积（diffusion graph convolutin, DGC）定义为：</p>
<p><img src="https://s2.loli.net/2021/12/22/I6hEnbrcAmPaVfp.png" alt="image-20211222132428391" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$f(·)$ 是激活函数，概率变换矩阵 $\mathbf {P \in R^{n \times n}}$ 通过 $\mathbf {P = D^{-1}A}$ 计算。注意，在 DCNN 中，隐藏表示矩阵 $\mathbf H^{(k)}$ 与输入特征矩阵 $\mathbf X$ 维度相同，并且不是其先前隐藏表示矩阵 $\mathbf H^{(k-1)}$ 的函数。DCNN 将 $\mathbf {H^{(1)}, H^{(2)}, …, H^{(K)}}$ 连接（concatenation）在一起作为模型的最终输出。由于扩散过程的平稳分布是概率变换矩阵幂级数的总和，DGC（2018）[17] 将每次扩散的输出相加而不是连接：</p>
<p><img src="https://s2.loli.net/2021/12/22/OwDU65gJ9CZWPjq.png" alt="image-20211222133138933" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$\mathbf {W^{(k)} \in R^{D \times F}}$ ，$f(·)$ 是激活函数。使用转移概率矩阵的幂意味着较远的邻居对中心节点贡献的信息很少，PGC-DGCNN（2018）[18] 基于最短路径提高较远邻居的贡献，定义了最短路径邻接矩阵 $\mathbf S^{(j)}$ 。如果从节点 $v$ 到节点 $u$ 的最短路径长度是 $j$ ，那么 $\mathbf S^{(j)}_{v, u} = 1$，否则为0。PGC-DGCNN 通过超参数 $r$ 控制感受野大小，引入了一种图卷积运算：</p>
<p><img src="https://s2.loli.net/2021/12/22/gmDiGx4Xd9lLbBC.png" alt="image-20211222133741550" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$\mathbf {H^{(0)} = X, \bar{A}^{(j)} = (\tilde{D}^{(j)})^{-(1/2)} \tilde{A}^{(j)} (\tilde {D}^{(j)})^{-(1/2)}, \tilde{A}^{(j)} = A^{(j)} + I}$。</p>
<p>&emsp;&emsp;MPNN（message-passing neural net work）（2017）[19] 提出了基于空间的 ConvGNNs 的一般框架。它将图卷积视为消息传递（message-passing）过程，在这个过程中，信息可以沿着边直接从一个节点传递到另一个节点。MPNN 进行 K 步迭代进行消息传递，消息传递函数定义为：</p>
<p><img src="https://s2.loli.net/2021/12/22/rt2SvEg1H63jo9F.png" alt="image-20211222134705962" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$\mathbf h^{(0)}_v = \mathbf x_v$，$U_k(·), M_k(·)$ 是可学习参数的函数。在计算出每个结点的隐藏表示之后，可以将 $\mathbf h^{(K)}_v$ 传递到输出层用于节点预测任务，或者传递到读出（readout）函数用于图预测任务。读出函数基于节点隐藏表示生成整个图的表示，通常被定义为：</p>
<p><img src="https://s2.loli.net/2021/12/22/bVyXNr5Jwnou9Op.png" alt="image-20211222135156362" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$R(·)$ 表示可学习参数的读出函数。然而，GIN（Graph isomorphism network）（2019）[20] 发现以前基于 MPNN 的方法无法通过生成的图嵌入区分不同的图结构。为了解决这一点，GIN 通过一个可学习的参数 $\epsilon^{(k)}$ 调整中心节点的权重。其图卷积定义为：</p>
<p><img src="https://s2.loli.net/2021/12/22/slKr9IoYB2VSmPN.png" alt="image-20211222135936784" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$\mathbf MLP(·)$ 代表多层感知机。由于节点的邻居数量可能从一个到一千个甚至更多，因此获取节点所有的邻居是低效的。GraphSage（2017）[21] 为每个节点采样固定数量的邻居。其图卷积定义为：</p>
<p><img src="https://s2.loli.net/2021/12/22/Yd5cUFgABpKEufT.png" alt="image-20211222140213174" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$\mathbf h^{(0)}_v = \mathbf x_v$，$f_k(·)$ 是聚合函数，$S_{N(v)}$ 是节点 $v$ 的邻居的一个随机采样。聚合函数应该与节点的顺序无关，例如求平均值、求和或最大值等。</p>
<p>​&emsp;&emsp;GAT（Graph attention network）（2017）[22] 中邻居节点对中心节点的贡献既不同于 GraphSage，也不同于 GCN。GAT 采用注意力机制学习两个相连节点之间的相对权重，其图卷积运算定义为：</p>
<p><img src="https://s2.loli.net/2021/12/22/2GDuFOWtb573y6A.png" alt="image-20211222140829903" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$\mathbf h^{(0)}_v = \mathbf x_v$。注意力权重 $\alpha^{(k)}_{vu}$ 代表节点 $v$ 与其邻居节点 $u$ 之间的连接强度：</p>
<p><img src="https://s2.loli.net/2021/12/22/RNvIkr7WKGs3BF6.png" alt="image-20211222141254741" style="zoom:50%;" /></p>
<p>&emsp;&emsp;其中，$g(·)$ 是 $\mathrm {LeakyReLU}$ 激活函数，$a$ 是可学习的向量。softmax 函数能保证节点 $v$ 的所有邻居节点的注意力权重总和为1。使用多头注意力（multihead attention）的 GAT 进一步增加了模型的表达能力。 与 GraphSage 相比，GAT 在节点分类任务上有明显的性能提升。GAT 的注意力头的贡献是相等的，而 GAAN （Gated attention network）（2018）[23] 引入了自注意力（self-attention）机制，为每个注意力头计算额外的注意力分数。除了在空间上应用图注意力外，GeniePath（2019）[24] 进一步提出了类似 LSTM 的门控机制来控制图卷积层之间的信息流。</p>
<p>&emsp;&emsp;MoNet（Mixture model network）（2017）[25] 引入节点的伪坐标来确定节点与其邻居节点之间的相对位置，以此为结点的邻居分配不同的权重。一旦两个节点之间的相对位置确定，权重函数就会将相对位置映射为这两个节点之间的相对权重。通过这种方式，图滤波器的参数可以在不同的位置共享。在 MoNet 框架下，一些现有的流形（manifolds）的方法，例如 GCNN（geodesic CNN）（2015）[26] 、ACNN（anisotropic CNN）（2016）[27] 和 spline CNN（2018）[28]，图方法，例如 GCN、DCNN 等，可以通过构造非参数权重函数将其归纳为 MoNet 的特例。MoNet 还提出了一种具有可学习参数的高斯核来自适应学习权重函数。</p>
<p>&emsp;&emsp;另一个不同的工作路线基于特定标准对结点的邻居进行排序，并将每个排序与可学习的权重相关联，从而实现不同位置的权重共享。PATCHY_SAN（2016）[29] 根据每个结点的图标签对其邻居进行排序，并选择前 $q$ 个邻居。图标签本质上是节点分数，可以通过节点度（degree）、中心度（centrality）和 Weisfeiler-Lehman（WL）颜色计算。由于每个节点有了固定数量的有序邻居，因此可以将图结构数据转换为网格数据。PATCHY-SAN 应用标准的一维卷积滤波器来聚合邻居特征信息，其中滤波器权重的顺序对应于节点邻居的顺序。PATCHY-SAN 的排序标准只考虑图结构，数据处理需要大量的计算。LGCN（Large-scale GCN）（2018）[30] 基于节点特征信息对节点的邻居进行排序。对于每个节点，LGCN 组合由其邻居组成的特征矩阵，并按列对该特征矩阵排序。排序后的特征矩阵的前 $q$ 行作为中心节点的输入数据。</p>
<p><strong>训练效率的提升</strong></p>
<p>&emsp;&emsp;训练 ConvGNNs，例如 GCN，通常需要把整个图数据和所有节点的中间状态存到内存中。当图包含的节点过多时，一起训练全部图数据会出现严重的内存溢出问题。为了节省内存，GraphSage 提出了批训练算法，以固定的样本大小通过 K 步递循环扩展每个节点的邻居节点。对于每个采样的树，GraphSage 通过从下到上分层聚合隐藏节点表示来计算根节点的隐藏表示。FastGCN（Fast learning with GCN）（2018）[31] 对每个图卷积层采样固定的节点数，而不是对每个节点采样固定数量的邻居。它将图卷积解释为概率度量下的节点嵌入函数的积分变换。蒙特卡洛近似（Monte Carlo approximation）和方差缩减（variance reduction）技术也被用于加速训练过程。由于 FastGCN 为每层独立采样节点，层之间的连接可能是稀疏的。Huang 等人（2018）[32] 提出了一种自适应分层采样方法，其中上层的节点采样作为下层的节点采样的前置条件。与 FastGCN 相比，该方法具有更高的精度，但采样方案更复杂。</p>
<p>&emsp;&emsp;另一个工作中，StoGCN（stochastic training of GCNs）（2018）[33] 使用历史节点表示作为控制变量，将图卷积的感受野大小大大降低。即使每个节点只有两个邻居，StoGCN 也能获得很好的性能。然而，StoGCN 仍然需要保存所有结点的中间状态，这对于大型图来说依然消耗内存。</p>
<p>​&emsp;&emsp;Cluster-GCN（2019）[34] 使用图聚类算法对子图进行采样，并对采样子图的节点进行图卷积。由于在采样子图内进行邻居搜索，Cluster-GCN 能够用更少的时间和内存使用更深的结构处理更大的图。下表是一些模型的时间和内存的比较。</p>
<p><img src="https://s2.loli.net/2021/12/23/wCn4v2sY3dxVg5B.png" alt="image-20211223132250449"></p>
<p>​&emsp;&emsp;表中，$n$ 是节点总数，$m$ 是边总数，$K$ 是网络层数，$s$ 是批大小（batch-size）。GCN 是全批量（full-batch）训练的基线方法，GraphSage 牺牲时间效率降低了内存使用，同时，随着 $K$ 和 $r$ 的增加，GraphSage 的时间和内存呈指数增长。Sto-GCN 的时间复杂度最高，并且内存瓶颈也没有解决，然而在 $r$ 很小时能获得较好的性能。Cluster-GCN 的时间复杂度与基线方法相同，在所有方法中空间复杂度最低。</p>
<p><strong>谱模型与空间模型的比较</strong></p>
<p>​&emsp;&emsp;谱模型有图信号处理的理论基础，通过设计新的图信号滤波器，可以构建新的 ConvGNNs。然而，在效率、通用性和灵活性上，空间模型优于谱模型。首先，谱模型的效率低于空间模型，谱模型要么需要进行特征向量计算，要么同时处理整个图。而空间模型通过信息传播直接图进行卷积，在处理大型图上更具可扩展性。而且空间模型可以对节点分批计算，而不是计算整个图。其次，依赖图傅里叶基的谱模型很难推广到新的图上，因为对图的任何扰动都会导致特征基的变化。另一方面，基于空间的模型在每个节点上计算局部图卷积，其权重在不同的位置和结构上很容易共享。第三，谱模型仅适用于对无向图，而空间模型处理多源图，例如输入边、有向图、符号图和超图等，更灵活，因为这些图可以很容易地合并到聚合函数中。</p>
<h4 id="C-图池化模块"><a href="#C-图池化模块" class="headerlink" title="C. 图池化模块"></a>C. 图池化模块</h4><p>​&emsp;&emsp;GNN 生成节点的特征向量后可用于下游任务。然而，直接使用所有特征是困难的，因此需要采用下采样（downsample）策略。池化（pooling）操作旨在通过对节点进行下采样来生成更小的表示来减小参数量，从而避免过拟合、置换不变性（permutation invariance）和计算复杂性问题。读出操作和池化操作非常类似，主要用于基于节点表示生成图表示。</p>
<p>​&emsp;&emsp;在早期的工作中，图粗化（graph coarsening）算法根据图的拓扑结构使用特征分解对图进行粗化。然而，这些方法面临时间复杂度问题。Graclus 算法（2007）[35] 是特征分解的一种替代方法，用于计算原始图的聚类。最近的一些工作将其作为池化操作对图粗化。</p>
<p>​&emsp;&emsp;目前，由于在池化窗口计算平均值、最大值、求和是很快的，因此常用来作为下采样最原始和最有效的方法：</p>
<p><img src="https://s2.loli.net/2021/12/23/ulxWSkOQdIXnrtf.png" alt="image-20211223145400608" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，$K$ 代表最后一个图卷积层数。Henaff 等人（2015）[36] 表明，在网络开始时执行简单的最大、平均池化对于降低图域中的维数和降低图傅里叶变换操作的成本是尤为重要的。此外，一些工作也使用注意力机制增强平均、求和池化操作。图嵌入将任何大小的图生成固定大小的嵌入，即使使用注意力机制，下采样的操作会使嵌入的效率降低。Vinyals 等人（2016）[37] 提出了 Set2Set 的方法来生成随输入大小增加的内存，然后它用一个 LSTM 在下采样之前整合顺序相关的信息到内存嵌入中，避免破坏信息。</p>
<p>​&emsp;&emsp;Defferard 等人在 ChebNet 中设计了一个有效的池化策略，按照某种意义重新排序图的节点来解决这个问题。首先通过 Graclus 算法将输入图粗化为多个级别，然后将输入图的节点和粗化后的重新排列为平衡二叉树。将平衡二叉树相似的节点排列在一起，从下到上任意聚合，将排列好的信号池化比原始池化要有效得多。</p>
<p>​&emsp;&emsp;Zhang 等人（2018）[38] 提出的 DGCNN 中使用了名为 SortPooling 的类似的池化策略。与 ChebNet 不同，DGCNN 根据节点在图中的结构角色对节点排序。来自空间图卷积的图的无序节点特征被视为连续的 WL 颜色，并以此来排序节点。除了对节点特征进行排序外，他还通过截断/扩展节点特征矩阵将图大小统一为 $q$。如果 $n &gt; q$，则删除最后的 $n - q$ 行；否则，添加 $q - n$ 个零行。</p>
<p>​&emsp;&emsp;上述的池化方法主要考虑到图的特征而忽略了图的结构信息。最近提出了一种可微池化（differentiable pooling，DiffPool）（2018）[39] 生成图的层级表示。与以前所有的粗化方法相比，DiffPool 不止对图中节点聚类，而是在 $k$ 层学习一个聚合分配矩阵 $\mathbf S$，在第 $k$ 层表示为 $\mathbf S{(k)} \in \mathbf R^{n_k \times n_{k+1}}$，其中， $n_k$ 是第 $k$ 层的节点数量。矩阵 $\mathbf S^{(k)}$ 中的概率值是基于节点特征和拓扑结构，利用下面的公式计算的：</p>
<p><img src="https://s2.loli.net/2021/12/23/LicoWz1JGPENAst.png" alt="image-20211223165316981" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其核心思想是综合考虑图的拓扑结构和特征信息来学习节点的表示，因此上式可以用任何标准的 ConvGNNs 来实现。但是，DiffPool 的缺点是它在池化后生成稠密图，计算复杂度为 $O(n^2)$。最近提出的 SAGPool（2019）[40]方法考虑了节点的特征和图拓扑结构，并以自注意力的方式学习池化。</p>
<p>​&emsp;&emsp;总的来说，池化是减小图大小的一个基本操作。如何提高池化的有效性和计算复杂性是一个仍待研究的开放问题。</p>
<h4 id="D-理论方面的讨论"><a href="#D-理论方面的讨论" class="headerlink" title="D. 理论方面的讨论"></a>D. 理论方面的讨论</h4><ul>
<li>感受野形状（Shape of Receptive Field）：节点的感受野是决定其最终节点表示的一组节点。当合成多个空间图卷积层时，节点的感受野每次都朝着他的远处邻居节点扩展。Micheli （2009）[41] 证明了存在有限数量的空间图卷积层使得每个节点 $v \in V$ 的感受野覆盖图中的所有节点。因此，ConvGNNs 能够通过叠加局部图卷积层来提取全局信息。</li>
<li>VC 维（VC Dimension）：VC 维是模型复杂度的一种度，关于 GNNs 的 VC 维分析的工作很少。给定模型的参数量 $p$ 和节点数量 $n$，Scarselli 等人（2018）[42] 推导出，如果使用 sigmoid 或 tangent 双曲线激活函数，GNN* 的 VC 维是 $O(p^4n^2)$；如果使用分段多项式激活函数，则 VC 维是 $O(p^2n)$。这一结果表明，如果使用 sigmoid 或 tangent 双曲线激活函数，GNN* 的模型复杂度随着 $p$ 和 $n$ 的增加而迅速增加。</li>
<li>图同构（Graph Isomorphism）：如果两个图拓扑相同，则称为同构。给定两个非同构图 $G_1$ 和 $G_2$，Xu 等人（2019）[43] 证明，如果 GNN 将 $G_1$ 和 $G_2$ 映射到不同的嵌入，则这两个图可以通过同构的 WL 检验确定为非同构的。他们表明，常见的 GNNs，如 GCN 和 GraphSage，不能区分不同的图结构。Xu 等人进一步证明，如果 GNN 的聚合函数和读出函数是内射的，则 GNN 能够像 WL 检测一样区分不同的图。</li>
<li><p>等变性与不变性（Equivariance and Invariance）：GNN 在处理节点级任务时必须是等边函数，在处理图级别任务时必须是不变函数。对于节点级任务，令 $f(\mathbf {A, X}) \in R^{n \times d}$ 是一个 GNN，$\mathbf Q$ 可以是节点顺序不同的任何置换矩阵。如果 GNN 满足 $f(\mathbf {QAQ^T, QX}) = \mathbf Q f(\mathbf {A, X})$ 那么该 GNN 是等变的。对于图级别的任务，令 $f(\mathbf {A, X}) \in R^d$。如果满足 $f(\mathbf {QAQ^T, QX}) = f(\mathbf {A, X})$，则 GNN 是不变的。为了实现等变性与不变性，GNN 的组成部分必须对节点顺序保持不变。Maron 等人（2019）[44] 从理论上研究了图数据的置换不变和等变线形层的性质。</p>
</li>
<li><p>通用近似（Universal Approximation）：众所周知，具有一个隐藏层的多层感知机前馈神经网络可以逼近任何 Borel 可观测函数（1989）[45]，而 GNNs 的通用近似能力很少被研究。Hammer 等人（2005）[46] 证明了级联关系（cascade correlation）可以近似结构化输出的函数。Scarselli 等人（2009）[4] 证明了 RecGNN 可以近似任何保持展开等价性的任何精度的函数。（如果两个节点的展开树相同，则他们展开等价。其中一个节点的展开树是通过在一定深度上迭代展开节点的邻居节点来构造的。）Xu 等人（2019）[20] 表明，消息传递框架下的 ConvGNNs 不是定义在多级上的连续函数的通用近似。Maron 等人(20190[44] 证明了不变图网络可以近似图的任意不变函数。</p>
</li>
</ul>
<h3 id="6-图自编码器"><a href="#6-图自编码器" class="headerlink" title="6. 图自编码器"></a>6. 图自编码器</h3><p>​&emsp;&emsp;GAEs 是将节点映射到潜在特征空间，并从潜在表示解码图信息的深层神经网络。GAEs 可用于学习网络嵌入或生成新的图。</p>
<h4 id="A-网络嵌入"><a href="#A-网络嵌入" class="headerlink" title="A. 网络嵌入"></a>A. 网络嵌入</h4><p>​&emsp;&emsp;网络嵌入是保留节点拓扑信息的低维向量表示。GAEs 使用编码器提取网络嵌入，使用解码器保留图的拓扑信息（如 PPMI 矩阵和邻接矩阵）来学习网络嵌入。早期的方法主要使用多层感知机构建 GAE 来学习网络嵌入。DNGRs（Deep nerual networks for graph representations）（2016）[47] 使用多层去噪编码器来编码，并通过多层感知机对 PPMI 矩阵解码。同时，SDNE（structural deep network embedding）（2016）[48] 使用多层自编码器联合保持节点的一阶近似度和二阶近似度。SDNE 分别在编码器和解码器的输出端使用两个损失函数。第一个损失函数通过最小化节点与其邻居的网络嵌入之间的距离，使学习到的网络嵌入保持节点的一阶相似性。该损失函数定义为：</p>
<p><img src="https://s2.loli.net/2021/12/23/nfVuGpxjSzM16mZ.png" alt="image-20211223205332059" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，$\mathbf x_v = \mathbf A_{v,:}$，$enc(·)$ 是由多层感知机构成的编码器。第二个损失函数通过最小化节点输入与其重构输入之间的距离，使学习到的网络嵌入保持节点的二阶近似。具体定义为：</p>
<p><img src="https://s2.loli.net/2021/12/24/kGSNya54PTq3Vdt.png" alt="image-20211224121620433" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，如果 $A_{v,u} = 0$，则$b_{v, u} = 1$；如果 $A_{v,u} = 1$，则$b_{v, u} = \beta &gt; 1$，$dec(·)$ 使由多层感知机构成的解码器。</p>
<p>​&emsp;&emsp;DNGR 和 SDNE 只考虑了节点之间连通性的结构信息，忽略了可能包含描述节点自身属性的特征信息。GAE*（避免歧义）（2016）[49] 利用 GCN 同时对节点的结构信息和特征信息进行编码。其编码器由两个图卷积组成：</p>
<p><img src="https://s2.loli.net/2021/12/24/fHvhbIp8Zwiu6XW.png" alt="image-20211224122446845" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，$\mathbf Z$ 代表图的网络嵌入矩阵，$f(·)$ 是 ReLU 激活函数，$Gconv(·)$ 是 GCN 的图卷积层。GAE* 的解码器目的是通过重构图的邻接矩阵来解码节点嵌入到关系信息，定义为：</p>
<p><img src="https://s2.loli.net/2021/12/24/sZ4r93yf85M7NTB.png" alt="image-20211224123055087" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，$\mathbf z_v$ 是节点 $v$ 的嵌入。GAE* 通过最小化原邻接矩阵 $\mathbf A$ 和重构的邻接矩阵 $\hat {\mathbf A}$ 之间的负交叉熵来训练。</p>
<p>​&emsp;&emsp;由于自编码器的容量有限，简单地重建图邻接矩阵可能会导致过拟合。VGAE（Variational GAE）[49] 学习数据的分布优化变分下界 $L$：</p>
<p><img src="https://s2.loli.net/2021/12/24/KOchZ16yeauSAEm.png" alt="image-20211224123619198" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，$KL(·)$ 是 Kullback-Leibler 散度，用于衡量两个分布之间的距离，$p(\mathbf Z)$ 是高斯先验，$p(\mathbf Z) = \prod^n_{i=1}p(\mathbf z_i) = \prod^n_{i=1}N(\mathbf z_i|0,\mathbf I)$，$p(A_{ij} = 1|\mathbf z_i, \mathbf z_j) = \mathrm {dec}(\mathbf z_i, \mathbf z_j) = \sigma (\mathbf z^T_i \mathbf z_j)$，$q(\mathbf {Z|X,A}) = \prod^n_{i=1}q(\mathbf z_i|\mathbf {X, A})$，其中 $q(\mathbf z_i|\mathbf {X,A}) = N(\mathbf z_i|\mu_i, diag(\sigma^2_i))$。$\mu_i$ 是编码器输出的第 i 行的平均向量，$\mathrm {log}\sigma_i$ 是与另一个向量的近似度。根据上式，VGAE 认为经验分布 $q(\mathbf {Z|X,A})$ 应尽可能接近先验分布 $p(\mathbf Z)$。为了进一步使经验分布 $q(\mathbf {Z|X,A})$ 近似于先验分布 $p(\mathbf Z)$，ARVGA（adversarially regularized VGAE）（2018）[50] 采用 GANs（generative adversarial networks）的方式进行训练。</p>
<p>​&emsp;&emsp;与 GAE* 类似，GraphSage（2017）[21] 使用两个图卷积层对节点特征进行编码。GraphSage 表示，负采样可以保留两个节点之间俺的关系信息，其损失定义为：</p>
<p><img src="https://s2.loli.net/2021/12/24/qKY321s8jG4ZBAx.png" alt="image-20211224125522306" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，节点 $u$ 是节点 $v$ 的邻居，节点 $v_n$ 是从负采样分布 $P_n(v)$ 采样的节点 $v$ 的远节点，$Q$ 是负样本的数量。该损失本质上相近的节点有相似的表示，远处的节点有不相似的表示。DGI（2019）[51] 通过最大化局部互信息使局部网络嵌入获取到全局结构信息，在实验结果上比 GraphSage 有明显的改进。</p>
<p>​&emsp;&emsp;上述方法本质上是通过解决链路预测问题来学习网络嵌入。然而，图的稀疏性导致正节点对的数量远小于负节点对的数量。为了解决网络嵌入中数据稀疏的问题，有一些工作通过随机排列或随机游走将图转换为序列来处理，于是那些适用于序列的深度学习方法可以直接用于处理图数据。DRNE（Deep recursive network embedding）（2018）[52] 认为节点的网络嵌入应该近似于其邻居的网络嵌入的聚合，它使用 LSTM 来聚合节点的邻居。其重构损失定义为：</p>
<p><img src="https://s2.loli.net/2021/12/24/7eaR46yg5WhuQ3x.png" alt="image-20211224130956254" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，$\mathbf z_v$ 是节点 $v$ 通过查字典得到的网络嵌入，LSTM 网络的输入为节点 $v$ 的邻居节点，并使用度来排序。如上式所示，DRNE 通过 LSTM 网络隐式地学习网络嵌入，而不是用来生成网络嵌入，这避免了 LSTM 网络对节点序列的排序不变性的问题。NetRAs（Network representations with adversarially regularized autoencoders）（2018）[53] 提出了一种更具一般性的损失函数：</p>
<p><img src="https://s2.loli.net/2021/12/24/7QGsIHWMCunqdfL.png" alt="image-20211224131649444" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，dist(·) 衡量节点及其重构节点嵌入之间的距离。NetRA 的编码器和解码器都是 LSTM，每个节点作为根节点随机游走作为输入。与 ARVGA 类似，NetRA 通过对抗训练将学习到的网络嵌入规则化到先验分布，实验结果证明了其有效性。</p>
<h4 id="B-图生成"><a href="#B-图生成" class="headerlink" title="B. 图生成"></a>B. 图生成</h4><p>​&emsp;&emsp;对于多个图，GAEs 能够通过将图编码为隐藏表示，并将隐藏表示解码为图结构来学习图的一般分布。大多数用于图生成的 GAEs 是为了解决分子图生成问题而设计的，在药物发现中具有很高的实用价值。</p>
<p>​&emsp;&emsp;序列（Sequential）方法通过逐步生成节点和边来生成图。Gómez-Bombarelli 等人（2018）[54]，Kusner 等人（2017）[55]，Dai 等人（2018）[56] 分别用深度 CNNs 和 RNNs 作为编码器和解码器，构建了字符串表示的分子图生成模型。虽然这些方式是特定领域的，但也可应用于一般图中。DeepGMG（Deep generative model of graphs）（2018）[57] 认为图的概率是所有可能的节点置换的总和：</p>
<p><img src="https://s2.loli.net/2021/12/24/Mic52tW3qhEgaKj.png" alt="image-20211224141152951" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，$\pi$ 代表节点顺序。它提取了图中所有节点和边的复杂联合概率。DeepGMG 通过一系列决策生成图，包括是否添加节点、添加哪个节点、是否添加边、连接到哪个节点。生成节点和边的决策过程取决于由 RecGNN 更新的节点状态和图状态。在另一个工作中，GraphRNN（2018）[58] 提出了一个图级别 RNN 和一个边级别 RNN 来建模节点和边的生成过程。图级别 RNN 每次向节点序列添加一个新的节点，边级别 RNN 则生成一个二进制序列，表示新节点与之前生成的节点之间的连接。</p>
<p>​&emsp;&emsp;全局（Global）方法一次输出整个图。GraphVAE（Graph variational autoencoder）（2018）[59] 将节点和边建模成独立的随机变量。假设编码器得到的后验分布为 $q(\mathbf z|G)$ ，解码器生成的分布为 $p_\theta(G|\mathbf z)$ ，GraphVAE 优化了变分下界：</p>
<p><img src="https://s2.loli.net/2021/12/24/D6EqFH5pGxLKZIl.png" alt="image-20211224142409328" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，$p(\mathbf z)$ 遵循高斯先验，$\phi, \theta$ 是可学习的参数。GraphVAE 使用 ConvGNN 作为编码器，使用简单的多层感知机作为解码器，输出生成的图及其邻接矩阵、节点属性和边属性。控制生成图的全局属性，如图的连通性（connectivity）、有效性（validity）和节点兼容性（node compatibility）是一项挑战。RGVAE（Regularized GraphVAE）（2018）[60] 进一步对 GraphVAE 施加有效性约束以调节解码器的输出分布。MolGAN（Molecular GAN）（2018）[61] 结合了 convGNNs（2018）[62]、GANs（2017）[63] 和强化学习目标来生成具有所需属性的图。在 MolGAN 中，生成器生成伪（fake）图，鉴别器鉴定伪样本，同时奖励（reward）网络根据评估器（evaluator）促进生成的图具有某些属性。NetGAN（2018）[64] 结合 LSTMs 与 Wasserstein GANs（2017）[65]，基于随机游走的方法生成图。NetGAN 通过 LSTM 训练生成器以生成合理的随机游走，并使用判别器鉴定随机游走的真伪。经过训练，可以通过生成器随机游走计算的规范化共现矩阵导出生成图。</p>
<p>​&emsp;&emsp;简言之，序列方法将图线性化为序列，由于环的存在可能会丢失一些结构信息。全局方法一次生成一个图形，这种方法不能扩展到大型图，因为 GAE 的输出空间高达 $O(n^2)$。</p>
<h3 id="7-时空图神经网络"><a href="#7-时空图神经网络" class="headerlink" title="7. 时空图神经网络"></a>7. 时空图神经网络</h3><p>​&emsp;&emsp;在许多实际应用中，图的结构和输入是动态的。STGNNs 是处理动态图很重要的方法。这类方法旨在对动态节点输入进行建模，同时假设相连的节点之间存在相互依赖关系。例如，交通网络由放置在路上的速度传感器组成，边的权重由传感器之间的距离确定，由于道路的交通情况可能取决于其临近道路的情况，因此在预测交通速度时有必要考虑空间相关性。STGNNs 可以同时捕获图的空间和时间依赖关系，用来预测未来节点的值或标签，或预测时空图的标签。STGNNs 包括基于 RNN 的和基于 CNN 的两种方法。</p>
<p>​&emsp;&emsp;大多数基于 RNN 的方法通过使用图卷积过滤传递给循环单元的输入和隐藏状态来捕获时空依赖性。假设一个简单的 RNN 表示为：</p>
<p><img src="https://s2.loli.net/2021/12/24/fQuRJZeMUSgyKsO.png" alt="image-20211224145427127" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，$\mathbf X^{(t)} \in \mathbf R^{n \times d}$ 是在时间步 t 时的节点特征矩阵。在加入图卷积后，上式变为：</p>
<p><img src="https://s2.loli.net/2021/12/24/mon9B8NPlZEw26F.png" alt="image-20211224145614016" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，Gconv(·) 是图卷积层。GCRN（2018）[66] 将 ChebNet 与 LSTM 结合；DCRNN（ Diffusion convolutional RNN）（2018）[67] 将扩散图卷积层合并到 GRU 中，并采用编码器-解码器的框架来预测未来 K 步节点的值。同时，一些工作中节点的时间信息和边的时间信息分别通过节点级别 RNNs 和边级别 RNNs 传递。为了整合空间信息，节点 RNN 将边 RNN 的输出作为输入。由于对不同的节点使用不同的 RNNs 会显著增加模型复杂度，因此它将节点和边拆分为语义组。同一语义组的节点或边共享相同的 RNN 模型，以此来降低计算成本。</p>
<p>​&emsp;&emsp;基于 RNN 的方法存在梯度爆炸和梯度消失以及计算耗时的问题。基于 GNN 的方法以非递归的方式处理时空图，具有并行计算、梯度稳定、内存占用少的优势。基于 CNN 的方法将一维卷积层和图卷积层交错使用来学习时空依赖关系。CGCN 集成了 一维卷积层与 ChebNet（或 GCN 层），通过门控一维卷积层、图卷积层和另一个门控一维卷积层搭建了时空块（spatial-temporal block）。ST-GCN（2018）[68] 使用一个一维卷积层和一个 PGC 层 [36] 搭建了时空块。</p>
<p>​&emsp;&emsp;前面的方法都使用预定义的图结构，认为预定义的图结构反映了节点之间的真正的依赖关系。然而，由于图数据在时空设置中有许多快照（snapshot），因此可以从数据中自动学习潜在的静态图结构。为实现这一点，Graph WaveNet（2019）[69] 提出了一种自适应邻接矩阵进行图卷积。自适应邻接矩阵定义为：</p>
<p><img src="https://s2.loli.net/2021/12/24/4eN65JxohvCaK31.png" alt="image-20211224155531091" style="zoom:50%;" /></p>
<p>​&emsp;&emsp;其中，SoftMax 方法沿行方向计算，$\mathbf {E1}$ 代表源节点嵌入，$\mathbf {E2}$ 代表目标节点嵌入。通过将 $\mathbf {E1}$ 与 $\mathbf {E2}$ 相乘，可以得到源节点和目标节点之间的依赖性权重。Graph Wavenet 采用复杂的基于 CNN 的时空神经网络，无需给定邻接矩阵就有良好的表现。</p>
<p>​&emsp;&emsp;学习潜在的静态空间依赖关系有助于发现网络中不同实体之间的可解释且稳定的相关性。然而，在某些情况下，学习潜在的动态空间依赖关系可能进一步提高模型精度。例如，在交通网络中，在两条道路的行驶时间可能取决于它们当前的交通状况。GaAN（2018）[70] 采用注意力机制，通过基于 RNN 的方法学习动态空间依赖性。给定两个相连节点输入，使用注意力函数更新两个节点之间的边权重。ASTGCN（2019）[71] 使用空间注意力函数和时间注意力函数，通过基于 CNN 的方法学习潜在的动态空间依赖和时间依赖，时间复杂度为 $O(n^2)$。</p>
<h3 id="8-应用"><a href="#8-应用" class="headerlink" title="8. 应用"></a>8. 应用</h3><p>​&emsp;&emsp;图结构数据无处不在，GNNs 有着广泛的应用。</p>
<h4 id="A-数据集"><a href="#A-数据集" class="headerlink" title="A. 数据集"></a>A. 数据集</h4><p>​&emsp;&emsp;这里主要列举了四个方面的数据集：引用网络、生化图、社交网络以及其他。</p>
<p><img src="https://s2.loli.net/2021/12/24/bJwPQC5F36UlyLN.png" alt="image-20211224160915322"></p>
<h4 id="B-评价方法"><a href="#B-评价方法" class="headerlink" title="B. 评价方法"></a>B. 评价方法</h4><p>​&emsp;&emsp;节点分类和图分类是评估 RecGNNs 和 ConvGNNs 性能的常见任务。</p>
<ul>
<li>节点分类：节点分类的大多数方法遵循 benchmark 数据集（如 Cora、Citeseer、Pubmed、PPI 和 Reddit）的标准的 train/valid/test 的划分方法。通常使用多次测试的平均准确率（accuracy）或 F1 分数。更多信息请参考 [72]。</li>
<li>图分类：通常采用十倍交叉验证进行模型评估。然而，在不同的工作中实验设置不统一。更多信息请参考 [73]。</li>
</ul>
<h4 id="C-实际应用"><a href="#C-实际应用" class="headerlink" title="C. 实际应用"></a>C. 实际应用</h4><ul>
<li><p>计算机视觉：场景图生成、点云分类、动作识别。</p>
<ul>
<li><p>识别对象之间的语义关系有助于理解视觉场景背后的含义。场景图生成模型旨在将图像解析为由对象及其语义关系组成的语义图。相反地，由于自然语言可以被解析为语义图，其中每个单词表是一个对象，因此根据给定文本描述合成图像也是一个很有前途的解决方案。</p>
</li>
<li><p>点云是由激光雷达设备扫描到的三维点集。将点云转换为 k-近邻图或超点图，可使用 ConvGNNs 来探索其拓扑结构。</p>
</li>
<li>识别视频中人的行为有助于让机器理解视频内容。一些解决方案检测视频帧中人体相关位置，由骨骼连接的人体关节形成图，然后根据人关节位置的时空序列应用 STGNNs 来学习人类行为模式。</li>
</ul>
<p>另外，GNNs 在计算机视觉中的应用在不断增加，包括人机交互、少样本图像分类、语义分割、视觉推理和问答等。</p>
</li>
<li><p>自然语言处理：常见的应用是文本分类，GNNs 利用文本或单词的相互关系来推断文本类别。尽管自然语言数据往往是序列形式的，但内部可能包含图结构，如句法依赖树。</p>
</li>
<li><p>交通：准确预测交通网络中的交通速度、交通量或道路密度对于智能交通系统至关重要。</p>
</li>
<li><p>推荐系统：基于图的推荐系统将用户和商品作为节点，利用节点之间、商品之间、节点和商品之间以及节点内容信息，生成高质量的推荐。</p>
</li>
<li><p>化学：在化学领域，GNNs 用于研究分子、化合物的图结构。在分子、化合物图中，原子被视为节点，化学键被视为边。节点分类、图分类和图生成是针对分子、化合物的三个重要任务，目的是学习分子指纹（molecular fingerprints）、预测分子特性、推断蛋白质结构以及合成化合物。</p>
</li>
<li><p>其他：GNNs 不限于以上应用。在程序验证、程序推理、社会影响预测、对抗性攻击预防、电子健康记录建模、大脑网络、事件检测和组合优化上都有人探索。</p>
</li>
</ul>
<h3 id="9-未来方向"><a href="#9-未来方向" class="headerlink" title="9. 未来方向"></a>9. 未来方向</h3><h4 id="A-模型深度"><a href="#A-模型深度" class="headerlink" title="A. 模型深度"></a>A. 模型深度</h4><p>​&emsp;&emsp;深度学习的成功在于深度的神经网络结构。然而，Li 等人（2018）[74] 表明，随着图卷积层数量的增加，ConvGNNs 的性能急剧下降。由于图卷积使相邻节点的表示更接近，在理论上，经过无限多的图卷积层，所有结点的表示都将收敛到一个值。于是，更深的网络结构是否是学习图数据的好的策略呢，这是一个问题。</p>
<h4 id="B-权衡可扩展性"><a href="#B-权衡可扩展性" class="headerlink" title="B. 权衡可扩展性"></a>B. 权衡可扩展性</h4><p>​&emsp;&emsp;GNNs 的可扩展性以破坏图的完整性为代价，如论是使用采样还是聚类，模型都会丢失部分图信息。通过采样，节点可能会丢失部分邻居；通过聚类，图可能缺少了不同的结构模式。如何权衡算法的可扩展性和图的完整性可能是未来的研究方向。</p>
<h4 id="C-异质性"><a href="#C-异质性" class="headerlink" title="C. 异质性"></a>C. 异质性</h4><p>​&emsp;&emsp;当前的大多数 GNNs 都采用同质图，因此很难将这些方法应用于异构图。异构图是指可能包含不同节点类型和边类型，或者不同形式的节点和边作为输入（如图和文本）的图。因此，应该开发新的方法来处理异构图。</p>
<h4 id="D-动态性"><a href="#D-动态性" class="headerlink" title="D. 动态性"></a>D. 动态性</h4><p>​&emsp;&emsp;图的输入可能随时间发生变化，需要新的卷积来适应图的动态性，虽然图的动态性可以部分地由 STGNNs 来解决，但很少有人考虑在动态空间关系的情况下如何执行图卷积。</p>
<h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><p>[1]  A. Sperduti and A. Starita, “Supervised neural networks for the classification of structures,” IEEE Trans. Neural Netw., vol. 8, no. 3, pp. 714–735, May 1997.</p>
<p>[2] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and locally connected networks on graphs,” in Proc. ICLR, 2014, pp. 1–14.</p>
<p>[3] T. Luong, H. Pham, and C. D. Manning, “Effective approaches to attention-based neural machine translation,” in Proc. Conf. Empirical Methods Natural Lang. Process., 2015, pp. 1412–1421.</p>
<p>[4] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, “The graph neural network model,” IEEE Trans. Neural Netw., vol. 20, no. 1, pp. 61–80, Jan. 2009.</p>
<p>[5]  C. Gallicchio and A. Micheli, “Graph echo state networks,” in Proc. Int. Joint Conf. Neural Netw. (IJCNN), Jul. 2010, pp. 1–8.</p>
<p>[6] Y. Li, D. Tarlow, M. Brockschmidt, and R. Zemel, “Gated graph sequence neural networks,” in Proc. ICLR, 2015, pp. 1–20.</p>
<p>[7] H. Dai, Z. Kozareva, B. Dai, A. Smola, and L. Song, “Learning steady-states of iterative algorithms over graphs,” in Proc. ICML, 2018, pp. 1114–1122.</p>
<p>[8] T. N. Kipf and M. Welling, “Semi-supervised classification with graph convolutional networks,” in Proc. ICLR, 2017, pp. 1–14.</p>
<p>[9] J. Bruna, W. Zaremba, A. Szlam, and Y. LeCun, “Spectral networks and locally connected networks on graphs,” in Proc. ICLR, 2014, pp. 1–14.</p>
<p>[10] M. Defferrard, X. Bresson, and P. Van der Gheynst, “Convolutional neural networks on graphs with fast localized spectral filtering,” in Proc. NIPS, 2016, pp. 3844–3852.</p>
<p>[11] R. Levie, F. Monti, X. Bresson, and M. M. Bronstein, “CayleyNets: Graph convolutional neural networks with complex rational spectral filters,” IEEE Trans. Signal Process., vol. 67, no. 1, pp. 97–109, Jan. 2019.</p>
<p>[12]  R. Li, S. Wang, F. Zhu, and J. Huang, “Adaptive graph convolutional neural networks,” in Proc. AAAI, 2018, pp. 3546–3553.</p>
<p>[13] C. Zhuang and Q. Ma, “Dual graph convolutional networks for graph-based semi-supervised classification,” in Proc. World Wide Web Conf. World Wide Web (WWW), 2018, pp. 499–508.</p>
<p>[14]  A. Micheli, “Neural network for graphs: A contextual constructive approach,” IEEE Trans. Neural Netw., vol. 20, no. 3, pp. 498–511, Mar. 2009.</p>
<p>[15] D. Bacciu, F. Errica, and A. Micheli, “Contextual graph Markov model: A deep and generative approach to graph processing,” in Proc. ICML, 2018, pp. 1–10.</p>
<p>[16]  J. Atwood and D. Towsley, “Diffusion-convolutional neural networks,” in Proc. NIPS, 2016, pp. 1993–2001.</p>
<p>[17] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional recurrent neural network: Data-driven traffic forecasting,” in Proc. ICLR, 2018, pp. 1–16.</p>
<p>[18] D. V. Tran, N. Navarin, and A. Sperduti, “On filter size in graph convolutional networks,” in Proc. IEEE Symp. Ser. Comput. Intell. (SSCI), Nov. 2018, pp. 1534–1541.</p>
<p>[19] J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E. Dahl, “Neural message passing for quantum chemistry,” in Proc. ICML, 2017, pp. 1263–1272.</p>
<p>[20] K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural networks,” in Proc. ICLR, 2019, pp. 1–17.</p>
<p>[21] W. Hamilton, Z. Ying, and J. Leskovec, “Inductive representation learning on large graphs,” in Proc. NIPS, 2017, pp. 1024–1034.</p>
<p>[22] P. Velickovic, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio, “Graph attention networks,” in Proc. ICLR, 2017, pp. 1–12.</p>
<p>[23] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D.-Y. Yeung, “GaAN: Gated attention networks for learning on large and spatiotemporal graphs,” in Proc. UAI, 2018, pp. 1–10.</p>
<p>[24]  Z. Liu et al., “GeniePath: Graph neural networks with adaptive receptive paths,” in Proc. AAAI Conf. Artif. Intell., Jul. 2019, pp. 4424–4431.</p>
<p>[25] F. Monti, D. Boscaini, J. Masci, E. Rodola, J. Svoboda, and M. M. Bronstein, “Geometric deep learning on graphs and manifolds using mixture model CNNs,” in Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), Jul. 2017, pp. 5115–5124.</p>
<p>[26]  J. Masci, D. Boscaini, M. M. Bronstein, and P. Vandergheynst, “Geodesic convolutional neural networks on Riemannian manifolds,” in Proc. IEEE Int. Conf. Comput. Vis. Workshop (ICCVW), Dec. 2015, pp. 37–45.</p>
<p>[27] D. Boscaini, J. Masci, E. Rodolà, and M. Bronstein, “Learning shape correspondence with anisotropic convolutional neural networks,” in Proc. NIPS, 2016, pp. 3189–3197.</p>
<p>[28] M. Fey, J. E. Lenssen, F. Weichert, and H. Müller, “SplineCNN: Fast geometric deep learning with continuous B-spline kernels,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., Jun. 2018, pp. 869–877.</p>
<p>[29] M. Niepert, M. Ahmed, and K. Kutzkov, “Learning convolutional neural networks for graphs,” in Proc. ICML, 2016, pp. 2014–2023.</p>
<p>[30] H. Gao, Z. Wang, and S. Ji, “Large-scale learnable graph convolutional networks,” in Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Aug. 2018, pp. 1416–1424.</p>
<p>[31] J. Chen, T. Ma, and C. Xiao, “FastGCN: Fast learning with graph convolutional networks via importance sampling,” in Proc. ICLR, 2018, pp. 1–15.</p>
<p>[32] W. Huang, T. Zhang, Y. Rong, and J. Huang, “Adaptive sampling towards fast graph representation learning,” in Proc. NeurIPS, 2018, pp. 4563–4572.</p>
<p>[33] J. Chen, J. Zhu, and L. Song, “Stochastic training of graph convolutional networks with variance reduction,” in Proc. ICML, 2018, pp. 941–949.</p>
<p>[34] W.-L. Chiang, X. Liu, S. Si, Y. Li, S. Bengio, and C.-J. Hsieh, “Cluster-GCN: An efficient algorithm for training deep and large graph convolutional networks,” in Proc. 25th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD), 2019, pp. 257–266.</p>
<p>[35] I. S. Dhillon, Y. Guan, and B. Kulis, “Weighted graph cuts without eigenvectors a multilevel approach,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 29, no. 11, pp. 1944–1957, Nov. 2007.</p>
<p>[36] M. Henaff, J. Bruna, and Y. LeCun, “Deep convolutional networks on graph-structured data,” 2015, arXiv:1506.05163. [Online]. Available: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1506.05163">http://arxiv.org/abs/1506.05163</a></p>
<p>[37] O. Vinyals, S. Bengio, and M. Kudlur, “Order matters: Sequence to sequence for sets,” in Proc. ICLR, 2016, pp. 1–11.</p>
<p>[38] M. Zhang, Z. Cui, M. Neumann, and Y. Chen, “An end-to-end deep learning architecture for graph classification,” in Proc. AAAI, 2018, pp. 1–8.</p>
<p>[39]   Z. Ying, J. You, C. Morris, X. Ren, W. Hamilton, and J. Leskovec, “Hierarchical graph representation learning with differentiable pooling,” in Proc. NeurIPS, 2018, pp. 4801–4811.</p>
<p>[40] J. Lee, I. Lee, and J. Kang, “Self-attention graph pooling,” in Proc. ICML, 2019, pp. 3734–3743.</p>
<p>[41] A. Micheli, “Neural network for graphs: A contextual constructive approach,” IEEE Trans. Neural Netw., vol. 20, no. 3, pp. 498–511, Mar. 2009.</p>
<p>[42]  F. Scarselli, A. C. Tsoi, and M. Hagenbuchner, “The Vapnik– Chervonenkis dimension of graph and recursive neural networks,” Neural Netw., vol. 108, pp. 248–259, Dec. 2018.</p>
<p>[43]  K. Xu, W. Hu, J. Leskovec, and S. Jegelka, “How powerful are graph neural networks,” in Proc. ICLR, 2019, pp. 1–17.</p>
<p>[44]  H. Maron, H. Ben-Hamu, N. Shamir, and Y. Lipman, “Invariant and equivariant graph networks,” in ICLR, 2019, pp. 1–14.</p>
<p>[45] K. Hornik, M. Stinchcombe, and H. White, “Multilayer feedforward networks are universal approximators,” Neural Netw., vol. 2, no. 5, pp. 359–366, Jan. 1989.</p>
<p>[46] B. Hammer, A. Micheli, and A. Sperduti, “Universal approximation capability of cascade correlation for structures,” Neural Comput., vol. 17, no. 5, pp. 1109–1159, May 2005.</p>
<p>[47] S. Cao, W. Lu, and Q. Xu, “Deep neural networks for learning graph representations,” in Proc. AAAI, 2016, pp. 1145–1152.</p>
<p>[48] D. Wang, P. Cui, and W. Zhu, “Structural deep network embedding,” in Proc. 22nd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining (KDD), 2016, pp. 1225–1234.</p>
<p>[49] T. N. Kipf and M. Welling, “Variational graph auto-encoders,” in Proc. NIPS Workshop Bayesian Deep Learn., 2016, pp. 1–3.</p>
<p>[50]  S. Pan, R. Hu, G. Long, J. Jiang, L. Yao, and C. Zhang, “Adversarially regularized graph autoencoder for graph embedding,” in Proc. IJCAI, Jul. 2018, pp. 2609–2615.</p>
<p>[51] P. Veliˇckovi´c, W. Fedus, W. L. Hamilton, P. Liò, Y. Bengio, and R. D. Hjelm, “Deep graph infomax,” in Proc. ICLR, 2019, pp. 1–17.</p>
<p>[52] K. Tu, P. Cui, X. Wang, P. S. Yu, and W. Zhu, “Deep recursive network embedding with regular equivalence,” in Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Aug. 2018, pp. 2357–2366.</p>
<p>[53] W. Yu et al., “Learning deep network representations with adversarially regularized autoencoders,” in Proc. 24th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, Aug. 2018, pp. 2663–2671.</p>
<p>[54]  R. Gómez-Bombarelli et al., “Automatic chemical design using a data-driven continuous representation of molecules,” ACS Central Sci., vol. 4, no. 2, pp. 268–276, Jan. 2018.</p>
<p>[55] M. J. Kusner, B. Paige, and J. M. Hernández-Lobato, “Grammar variational autoencoder,” in Proc. ICML, 2017, pp. 1945–1954.</p>
<p>[56] H. Dai, Y. Tian, B. Dai, S. Skiena, and L. Song, “Syntax-directed variational autoencoder for structured data,” in Proc. ICLR, 2018, pp. 1–17.</p>
<p>[57] Y. Li, O. Vinyals, C. Dyer, R. Pascanu, and P. Battaglia, “Learning deep generative models of graphs,” in Proc. ICML, 2018, pp. 1–21.</p>
<p>[58] J. You, R. Ying, X. Ren, W. L. Hamilton, and J. Leskovec, “GraphRNN: A deep generative model for graphs,” in Proc. ICML, 2018, pp. 1–12.</p>
<p>[59] M. Simonovsky and N. Komodakis, “Graphvae: Towards generation of small graphs using variational autoencoders,” in Proc. ICANN. Cham, Switzerland: Springer, 2018, pp. 412–422.</p>
<p>[60] T. Ma, J. Chen, and C. Xiao, “Constrained generation of semantically valid graphs via regularizing variational autoencoders,” in Proc. NeurIPS, 2018, pp. 7110–7121.</p>
<p>[61]  N. De Cao and T. Kipf, “MolGAN: An implicit generative model for small molecular graphs,” ICML Workshop Theor. Found. Appl. Deep Generative Models, 2018, pp. 1–11.</p>
<p>[62]  M. Schlichtkrull, T. N. Kipf, P. Bloem, R. van den Berg, I. Titov, and M. Welling, “Modeling relational data with graph convolutional networks,” in ESWC. Cham, Switzerland: Springer, 2018, pp. 593–607.</p>
<p>[63] I. Gulrajani, F. Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, “Improved training of Wasserstein GANs,” in Proc. NIPS, 2017, pp. 5767–5777.</p>
<p>[64] A. Bojchevski, O. Shchur, D. Zügner, and S. Günnemann, “NetGAN: Generating graphs via random walks,” in Proc. ICML, 2018, pp. 1–16.</p>
<p>[65] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein GAN,” 2017, arXiv:1701.07875. [Online]. Available: <a target="_blank" rel="noopener" href="http://arxiv.org/abs/1701.07875">http://arxiv.org/abs/1701.07875</a></p>
<p>[66] Y. Seo, M. Defferrard, P. Vandergheynst, and X. Bresson, “Structured sequence modeling with graph convolutional recurrent networks,” in Proc. NeurIPS. Springer, 2018, pp. 362–373.</p>
<p>[67] Y. Li, R. Yu, C. Shahabi, and Y. Liu, “Diffusion convolutional recurrent neural network: Data-driven traffic forecasting,” in Proc. ICLR, 2018, pp. 1–16.</p>
<p>[68] S. Yan, Y. Xiong, and D. Lin, “Spatial temporal graph convolutional networks for skeleton-based action recognition,” in Proc. AAAI, 2018, pp. 1–9.</p>
<p>[69] Z. Wu, S. Pan, G. Long, J. Jiang, and C. Zhang, “Graph WaveNet for deep spatial-temporal graph modeling,” in Proc. IJCAI, Aug. 2019, pp. 1–7.</p>
<p>[70] J. Zhang, X. Shi, J. Xie, H. Ma, I. King, and D.-Y. Yeung, “GaAN: Gated attention networks for learning on large and spatiotemporal graphs,” in Proc. UAI, 2018, pp. 1–10.</p>
<p>[71] S. Guo, Y. Lin, N. Feng, C. Song, and H. Wan, “Attention based spatial-temporal graph convolutional networks for traffic flow forecasting,” in Proc. AAAI, 2019, pp. 922–929.</p>
<p>[72] O. Shchur, M. Mumme, A. Bojchevski, and S. Günnemann, “Pitfalls of graph neural network evaluation,” in Proc. NeurIPS workshop, 2018, pp. 1–11.</p>
<p>[73]  F. Errica, M. Podda, D. Bacciu, and A. Micheli, “A fair comparison of graph neural networks for graph classification,” in Proc. ICLR, 2020, pp. 1–15. [Online]. Available: <a target="_blank" rel="noopener" href="https://openreview">https://openreview</a>. net/forum?id=HygDF6NFPB</p>
<p>[74]  Q. Li, Z. Han, and X.-M. Wu, “Deeper insights into graph convolutional networks for semi-supervised learning,” in Proc. AAAI, 2018, pp. 1–8.</p>
</div><div class="article-licensing box"><div class="licensing-title"><p>图神经网络——综述</p><p><a href="https://brucehan98@github.io/2021/12/22/图神经网络——综述/">https://brucehan98@github.io/2021/12/22/图神经网络——综述/</a></p></div><div class="licensing-meta level is-mobile"><div class="level-left"><div class="level-item is-narrow"><div><h6>作者</h6><p>Bruce Han</p></div></div><div class="level-item is-narrow"><div><h6>发布于</h6><p>2021-12-22</p></div></div><div class="level-item is-narrow"><div><h6>更新于</h6><p>2021-12-25</p></div></div><div class="level-item is-narrow"><div><h6>许可协议</h6><p><a class="icons" rel="noopener" target="_blank" title="Creative Commons" href="../../../../https:/creativecommons.org/"><i class="icon fab fa-creative-commons"></i></a><a class="icons" rel="noopener" target="_blank" title="Attribution" href="../../../../https:/creativecommons.org/licenses/by/4.0/"><i class="icon fab fa-creative-commons-by"></i></a><a class="icons" rel="noopener" target="_blank" title="Noncommercial" href="../../../../https:/creativecommons.org/licenses/by-nc/4.0/"><i class="icon fab fa-creative-commons-nc"></i></a></p></div></div></div></div></div><hr style="height:1px;margin:1rem 0"><div class="level is-mobile is-flex"><div class="article-tags is-size-7 is-uppercase"><i class="fas fa-tags has-text-grey"></i> <a class="link-muted" rel="tag" href="../../../../tags/GNN/">GNN </a></div></div><!--!--></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button donate" data-type="alipay"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="../../../../img/alipay.jpg" alt="支付宝"></span></a><a class="button donate" data-type="wechat"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="../../../../img/wechat.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="../../../../2022/01/11/PaddleOCR-C-CPU-%E6%8E%A8%E7%90%86%E9%83%A8%E7%BD%B2/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">PaddleOCR C++ CPU 推理部署</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="../../18/Centos7-Mysql-%E4%BD%BF%E7%94%A8/"><span class="level-item">Centos7 Mysql 使用</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="comment-container"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@1.7.2/dist/gitalk.min.js"></script><script>var gitalk = new Gitalk({
            id: "9c084559d8f32dbe3f3c10e964d50508",
            repo: "brucehan98.github.io",
            owner: "BruceHan98",
            clientID: "84666a45ad34d2937a18",
            clientSecret: "f2432742d8824e7bd1006ae69b85f0488f928759",
            admin: ["BruceHan98"],
            createIssueManually: false,
            distractionFreeMode: false,
            perPage: 20,
            pagerDirection: "last",
            
            
            enableHotKey: true,
            language: "zh-CN",
        })
        gitalk.render('comment-container')</script></div></div></div><div class="column column-left is-4-tablet is-4-desktop is-3-widescreen  order-1 is-sticky"><div class="card widget" id="toc" data-type="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="level is-mobile" href="#1-引言"><span class="level-left"><span class="level-item">1</span><span class="level-item">1. 引言</span></span></a></li><li><a class="level is-mobile" href="#2-背景介绍"><span class="level-left"><span class="level-item">2</span><span class="level-item">2. 背景介绍</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#图神经网络简史"><span class="level-left"><span class="level-item">2.1</span><span class="level-item">图神经网络简史</span></span></a></li><li><a class="level is-mobile" href="#图神经网络-vs-网络嵌入"><span class="level-left"><span class="level-item">2.2</span><span class="level-item">图神经网络 vs 网络嵌入</span></span></a></li><li><a class="level is-mobile" href="#图神经网络-vs-图核方法"><span class="level-left"><span class="level-item">2.3</span><span class="level-item">图神经网络 vs 图核方法</span></span></a></li></ul></li><li><a class="level is-mobile" href="#3-分类和框架"><span class="level-left"><span class="level-item">3</span><span class="level-item">3. 分类和框架</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#A-图神经网络分类"><span class="level-left"><span class="level-item">3.1</span><span class="level-item">A. 图神经网络分类</span></span></a></li><li><a class="level is-mobile" href="#B-框架"><span class="level-left"><span class="level-item">3.2</span><span class="level-item">B. 框架</span></span></a></li></ul></li><li><a class="level is-mobile" href="#4-循环图神经网络"><span class="level-left"><span class="level-item">4</span><span class="level-item">4. 循环图神经网络</span></span></a></li><li><a class="level is-mobile" href="#5-卷积图神经网络"><span class="level-left"><span class="level-item">5</span><span class="level-item">5. 卷积图神经网络</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#A-基于谱的-ConvGNNs"><span class="level-left"><span class="level-item">5.1</span><span class="level-item">A. 基于谱的 ConvGNNs</span></span></a></li><li><a class="level is-mobile" href="#B-基于空间的-ConvGNNs"><span class="level-left"><span class="level-item">5.2</span><span class="level-item">B. 基于空间的 ConvGNNs</span></span></a></li><li><a class="level is-mobile" href="#C-图池化模块"><span class="level-left"><span class="level-item">5.3</span><span class="level-item">C. 图池化模块</span></span></a></li><li><a class="level is-mobile" href="#D-理论方面的讨论"><span class="level-left"><span class="level-item">5.4</span><span class="level-item">D. 理论方面的讨论</span></span></a></li></ul></li><li><a class="level is-mobile" href="#6-图自编码器"><span class="level-left"><span class="level-item">6</span><span class="level-item">6. 图自编码器</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#A-网络嵌入"><span class="level-left"><span class="level-item">6.1</span><span class="level-item">A. 网络嵌入</span></span></a></li><li><a class="level is-mobile" href="#B-图生成"><span class="level-left"><span class="level-item">6.2</span><span class="level-item">B. 图生成</span></span></a></li></ul></li><li><a class="level is-mobile" href="#7-时空图神经网络"><span class="level-left"><span class="level-item">7</span><span class="level-item">7. 时空图神经网络</span></span></a></li><li><a class="level is-mobile" href="#8-应用"><span class="level-left"><span class="level-item">8</span><span class="level-item">8. 应用</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#A-数据集"><span class="level-left"><span class="level-item">8.1</span><span class="level-item">A. 数据集</span></span></a></li><li><a class="level is-mobile" href="#B-评价方法"><span class="level-left"><span class="level-item">8.2</span><span class="level-item">B. 评价方法</span></span></a></li><li><a class="level is-mobile" href="#C-实际应用"><span class="level-left"><span class="level-item">8.3</span><span class="level-item">C. 实际应用</span></span></a></li></ul></li><li><a class="level is-mobile" href="#9-未来方向"><span class="level-left"><span class="level-item">9</span><span class="level-item">9. 未来方向</span></span></a><ul class="menu-list"><li><a class="level is-mobile" href="#A-模型深度"><span class="level-left"><span class="level-item">9.1</span><span class="level-item">A. 模型深度</span></span></a></li><li><a class="level is-mobile" href="#B-权衡可扩展性"><span class="level-left"><span class="level-item">9.2</span><span class="level-item">B. 权衡可扩展性</span></span></a></li><li><a class="level is-mobile" href="#C-异质性"><span class="level-left"><span class="level-item">9.3</span><span class="level-item">C. 异质性</span></span></a></li><li><a class="level is-mobile" href="#D-动态性"><span class="level-left"><span class="level-item">9.4</span><span class="level-item">D. 动态性</span></span></a></li></ul></li><li><a class="level is-mobile" href="#参考文献"><span class="level-left"><span class="level-item">10</span><span class="level-item">参考文献</span></span></a></li></ul></div></div><style>#toc .menu-list > li > a.is-active + .menu-list { display: block; }#toc .menu-list > li > a + .menu-list { display: none; }</style><script src="../../../../js/toc.js" defer></script></div></div><!--!--></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="../../../../index.html"><img src="../../../../img/logo.svg" alt="Bruce Han&#039;s Blog" height="18"></a><p class="is-size-7"><span>&copy; 2022 Bruce Han</span>  Powered by <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a> &amp; <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank" rel="noopener">Icarus</a><br><span id="busuanzi_container_site_uv">共<span id="busuanzi_value_site_uv">0</span>个访客</span></p></div><div class="level-end"></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script>moment.locale("zh-CN");</script><script>var IcarusThemeSettings = {
            article: {
                highlight: {
                    clipboard: true,
                    fold: 'unfolded'
                }
            }
        };</script><script src="../../../../js/column.js"></script><script src="../../../../js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="../../../../js/back_to_top.js" defer></script><!--!--><!--!--><!--!--><!--!--><script src="https://cdn.jsdelivr.net/npm/cookieconsent@3.1.1/build/cookieconsent.min.js" defer></script><script>window.addEventListener("load", () => {
      window.cookieconsent.initialise({
        type: "info",
        theme: "edgeless",
        static: false,
        position: "bottom-left",
        content: {
          message: "此网站使用Cookie来改善您的体验。",
          dismiss: "知道了！",
          allow: "允许使用Cookie",
          deny: "拒绝",
          link: "了解更多",
          policy: "Cookie政策",
          href: "https://www.cookiesandyou.com/",
        },
        palette: {
          popup: {
            background: "#edeff5",
            text: "#838391"
          },
          button: {
            background: "#4b81e8"
          },
        },
      });
    });</script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><!--!--><!--!--><script type="text/x-mathjax-config">MathJax.Hub.Config({
            'HTML-CSS': {
                matchFontHeight: false
            },
            SVG: {
                matchFontHeight: false
            },
            CommonHTML: {
                matchFontHeight: false
            },
            tex2jax: {
                inlineMath: [
                    ['$','$'],
                    ['\\(','\\)']
                ]
            }
        });</script><script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.5/unpacked/MathJax.js?config=TeX-MML-AM_CHTML" defer></script><!--!--><!--!--><!--!--><script src="../../../../js/main.js" defer></script><script src="../../../../js/night.js" defer></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="../../../../js/insight.js" defer></script><script>document.addEventListener('DOMContentLoaded', function () {
            loadInsight({"contentUrl":"../../../../content.json"}, {"hint":"想要查找什么...","untitled":"(无标题)","posts":"文章","pages":"页面","categories":"分类","tags":"标签"});
        });</script></body><script type="text/javascript" src="/js/mathjax-config.js"></script></html>